---
title: 'STAT564 Homework 2'
author: "Shen Qu"
date: "10/31/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---

```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

### Problem 1
The weight and systolic blood pressure of 26 randomly selected a group of adults are given in Weight vs BP text file

 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE, results='hide', message = FALSE, collapse=TRUE}
library(tidyverse)
# Import Data
table_weight_bp <- read_table2("Weight vs BP.txt")
# Observe the data frame
head(table_weight_bp)
# build the model
model_wei_bp <- lm(SystolicBP ~ Weight, data=table_weight_bp)
model_wei_bp_origin <- lm(SystolicBP ~ Weight+0, data=table_weight_bp)
model_wei_bp%>% summary()
model_wei_bp_origin%>% summary()
confint(model_wei_bp, level = 0.99)
confint(model_wei_bp_origin, level = 0.99)
anova(model_wei_bp)
anova(model_wei_bp_origin)

# visualizng

## table_weight_bp %>% ggplot(aes(Weight,SystolicBP))+geom_point(alpha=0.5)+xlim(0,max(table_weight_bp$Weight))+ylim(0,max(table_weight_bp$SystolicBP))+geom_abline(mapping=aes(slope=0.4194, intercept=69.1044),linetype=2,col="red")+geom_abline(mapping=aes(slope=0.7916, intercept=0),linetype=3,col="blue")+labs(linetype="")

## plot(table_weight_bp$Weight, table_weight_bp$SystolicBP,col="red", pch = 16, xlab = "Weight", ylab = "SystolicBP") 
## abline(model_wei_bp, col = "blue", lwd = 3)
## abline(model_wei_bp_origin, col = "black", lwd = 3, lty = 3)
## legend(30, 780, legend=c("Observed", "Fit 1", "Fit 2 (no intercept)"), col=c("red", "blue", "black"), lty=c(0,1,3), pch = c(16, NA_integer_, NA_integer_), lwd = c(1,3,3), cex=0.8)
```
(a) Fit a simple linear regression model to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

The simple linear model is $\hat{systolic blood pressure}=69.1044+0.4194Weight$

(b) Test for significance of true intercept of this model at 5% significance and provide the conclusion along with p-value.

The p-value is $1.71\times e^{-05}$, which is much smaller than 0.05. We can know the true intercept is not zero on 95% confidence level.

(c) Report a 99% confidence interval for true intercept of this model.

The 99% confidence interval for true intercept of this model is (32.9955223 105.2132233).

(d) Fit a simple linear regression model through origin to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

The model through origin is $\hat{systolic blood pressure}=0.7916Weight$

(e) Compare quality of two models and recommend only one model. Provide valid reasons for your recommendation.

```{r, include = FALSE, collapse=TRUE}
library(ggfortify)
model_wei_bp %>% autoplot()
model_wei_bp_origin %>% autoplot()

texreg::screenreg(l=list(model_wei_bp, model_wei_bp_origin))
huxtable::huxreg(model_wei_bp, model_wei_bp_origin, statistics = NULL)
```

Firstly, the estimated intercept (69.104) is not zero on 95% confident level by p-value ($1.71\times e^{-05}$).Moreover, the 99% confidence interval (32.996~105.213) for true intercept doesn't contain zero. There are strong relationship between weight and blood pressure.

Secondly, the MSE value of model with intercept (75.36) is about half of the MSE with zero intercept (159), which shows the former is better fitted.

Thirdly, the R-suqare value of model with intercept is 0.5983, while the R-suqare value of zero-intercept model is 0.9929. We can explain that the much larger SSR in zero-intercept model contributs most of the additional R-suqare value. The sum square of total variation with intercept ($2693.6+1808.6=4502.2$) is only 1 percent of the SST with zero intercept ($551834+3968=555802$). 

Fourthly, the overlayed fitted models on the scatterplot show significant difference between two models. The line through origin doesn't fit the pattern better.

Overall, the model with intercept is recommended because the factors of P-value, CI, MSE, and pattern support it while R-square is low. To increase R-square or fit the pattern better, we should consider other omitted variables or advanced models. 


### Problem 2: 
Consider the simple linear regression model:$y_i=β_0+β_1 x_i+ε_i\quad    for  i=1,2,…,n$, which can be written for each observation as

$$
\begin{matrix}
  y_1=\beta_0+\beta_1x_1+\varepsilon_1 \\
  y_2=\beta_0+\beta_1x_2+\varepsilon_2 \\
  \vdots\quad\quad  \vdots\quad\quad   \vdots\quad\quad  \vdots  \\
  y_n=\beta_0+\beta_1x_n+\varepsilon_n 
 \end{matrix}
$$
 (a) Write this model in matrix form. Provide all the vectors and design matrix along with dimensions. Use the above format so that you can write only 4 rows for each vector and matrix. For example, Y vector is written as
 
$$
\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1} 
\mathbf{X}=\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}_{2\times1}
\mathbf{ε}=\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots  \\ \varepsilon_n \end{bmatrix}_{n\times1}
$$

 (b) Compute $\mathbf{X'X}$ (show all the main steps)

$$
\mathbf{X'X}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2}=
\begin{bmatrix} n & x_{1}+x_{2}+\cdots+x_{n} \\ x_{1}+x_{2}+\cdots+x_{n} & x_1^2+x_2^2+\cdots+x_n^2  \end{bmatrix}_{2\times2}=\begin{bmatrix} n & \sum_{i=1}^nx_i \\ \sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 \end{bmatrix}_{2\times2}
$$

 (c) Compute $\mathbf{(X'X)^{-1}}$ (show all the main steps)
Hint: The inverse of a nonsingular is obtained by 1) replacing every element of the matrix with its cofactor, 2) transposing the resulting matrix, 3) dividing by the determinant of the origin matrix.

$$
\mathbf{|X'X|}^{-1}=\frac1{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix} \sum_{i=1}^nx_i^2  & -\sum_{i=1}^nx_i \\ -\sum_{i=1}^nx_i & n \end{bmatrix}_{2\times2}
$$

$$=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n\sum_{i=1}^nx_i^2-(n\bar x_i)^2}=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n(\sum_{i=1}^nx_i^2-n\bar x_i^2)}=\frac1{nS_{xx}}{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}$$

(d) Compute X'Y (show all the main steps)

$$
\mathbf{X'Y}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1}=\begin{bmatrix} y_1 + y_2 +\cdots + y_n \\ x_{1}y_1 + x_{2}y_2 + \cdots + x_{n}y_n \end{bmatrix}_{2\times{1}}=\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}}
$$

(e) Compute (X'X)^(-1) X'Y (show all the main steps).
The final answer is the expression for the least squares estimator of β
$$
\mathbf{\hatβ}=\mathbf{(X'X)^{-1} X'Y}=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i \\ -\sum_{i=1}^nx_i & n \end{bmatrix}_{2\times2}\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}}=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i \\ -\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times1}$$

$$=\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2n\bar{y}-n\bar{x}\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \\ \frac{\sum_{i=1}^nx_{i}y_i-\frac{\sum_{i=1}^nx_i\sum_{i=1}^ny_i}{n}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \frac{\bar{y}\sum_{i=1}^nx_i^2-\bar{y}\frac{(\sum_{i=1}^nx_i)^2}n+\bar{y}\frac{(\sum_{i=1}^nx_i)^2}n-\bar{x}\sum_{i=1}^nx_{i}y_i}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}$$

$$=\begin{bmatrix} \bar{y}\frac{S_{xx}}{S_{xx}}+\frac{\bar{x}\bar{y}\sum_{i=1}^nx_i-\bar{x}\sum_{i=1}^nx_{i}y_i}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}\frac{\sum_{i=1}^nx_{i}y_i-\frac{\sum_{i=1}^nx_i\sum_{i=1}^ny_i}n}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}\frac{S_{xy}}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}{\hat\beta_1} \\ \hat\beta_1 \end{bmatrix}_{2\times1}$$

in the multiple linear regression model when k=1. It reduces to the same expression you learned for the slope and intercept for simple linear regression model.

(f) Also, we know that if $Var(ε_i )=σ^2\ and\ Cov(ε_i,ε_j )=0 for i≠j$, 

$$Var(\mathbf{\hatβ})=σ^2(\mathbf{X'X})^{-1}=\sigma^2\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2}{nS_{xx}} & \frac{-\sum_{i=1}^nx_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i)}{nS_{xx}} & \frac1{S_{xx}} \end{bmatrix}_{2\times2}=\sigma^2\begin{bmatrix} C_{11} &  C_{12} \\  C_{21} &  C_{22} \end{bmatrix}_{2\times2}$$

From this result, show that 

(i)  $Var(\hatβ_0)= σ^2 [1/n+ x^2/S_{xx}]$  

$$Var(\mathbf{\hat\beta_0})=σ^2C_{11}=\frac{\sigma^2\sum_{i=1}^nx_i^2}{nS_{xx}}=\sigma^2\frac{\sum_{i=1}^nx_i^2-n\bar x_i^2+n\bar x_i^2}{nS_{xx}}=\sigma^2(\frac{1}{n}+\frac{n\bar x_i^2}{nS_{xx}})=\sigma^2(\frac{1}{n}+\frac{\bar x_i^2}{S_{xx}})$$

(ii)  $Var(\hatβ_1)=σ^2/S_{xx}$

$$Var(\mathbf{\hat\beta_1})=σ^2C_{22}=\frac{\sigma^2}{S_{xx}}$$

(iii)  $Cov(β_0,β_1)=-(\bar x σ^2)/S_{xx}$

$$Cov(\mathbf{\hat\beta_0,\hat\beta_1})=σ^2C_{12}=\frac{\sigma^2(-\sum_{i=1}^nx_i)}{nS_{xx}}=\frac{-\sigma^2(n\bar x_i)}{nS_{xx}}=\frac{-\sigma^2\bar x_i}{S_{xx}}$$