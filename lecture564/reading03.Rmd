---
title: "2.4 Interval Estimation"
subtitle: "STAT 564 Week 3"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document: 
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## {.tabset .tabset-fade .tabset-pills}

`2018.10.8 Lab1`

`2018.10.10`

### Example

Consider the following data set, in which the variables of interest are x=conmmuting distance(miles) and y=commuting time (min).  


x|5|10|15|20|25 
---|---|---|---|---|--- 
y|8|16|22|23|31 
$\hat y=4.1+1.06x$|9.4|14.7|20|25.3|30.6 
$e^2=(y-\hat y)^2$|1.96|1.69|4|5.29|0.16 


$$\hat\sigma^2=MSE=\frac{SSE}{n-2}=\frac{\sum_1^n(y-\hat y)^2}{n-2}=\frac{13.1}{5-2}=4.367 $$

(a) Compute the values of ANOVA table and test the significance of simple linear regression model for these data.

$$n=5,\quad \bar y=20,\quad \sum y_i^2=2294,\quad \hat\sigma^2=4.367,\quad S{xy}=265,\quad S{xx}=250,\quad \hat\beta_1=1.06$$


Source of Variation|Sum of Squars(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|P-value
---|---|---|---|---|---
Model(Regresion)|$\hat\beta_1S{xy}=1.06\times265=280.9$|1|$\frac{280.9}1$|$\frac{280.9}{4.367}=64.328$|$P(F>64.328)\approx0.01$
Error(Residual)|SST-SSR=294-280.9=13.1|n-2=5-2=3|$\frac{13.1}3=4.3667$||
Total|$\sum(y_i-\bar y)^2=\sum{y_i^2}-n\bar y^2=2294-5\times20^2=294$|n-1=4|||

At 5% significance level, the fitted model is significant because P-value<0.05.

#### Ex 2 Contd Consider the following data set, in which the variables of interest are x and y.

 - [a] Test whether the true slope of the simple linear regression model is significantly different from zero.

$$H_0:\ \beta_1=0,\quad H_1: \beta_1\ne0$$

$$Test\ statistic: t_0=\frac{\hat\beta_1-0}{se(\hat\beta_1)}=\frac{\hat\beta_1}{se(\hat\beta_1)}=\frac{1.06}{0.1322}=8.02$$

$$se(\hat\beta_1)=\sqrt \frac{\hat\sigma^2}{S{xx}}=\sqrt{\frac{4.367}{250}}=0.1322$$

$$df=n-2=5-2=3$$

$$P-value=2\times P(t>|t_0|)2\times0.0025=0.005$$

 > P-value<0.05, therefore, ture slope is different from zero.

 - [b] Estimate the 95% confidence interval for the true slope of the simple linear regression model.

$$\because\ df=n-2=5-2=3,\quad t_{\frac\alpha2}=t_{0.025}=3.182$$


$$\hat{\beta_1}\pm t_{\alpha/2}se(\hat{\beta_1})=1.06\pm3.182\times0.1322=(0.639,1.481)$$

 - [c] Estimate the 95% confidence interval for the true intercept of the simple linear regression model.

$$se(\hat{\beta_0})=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=4.367(\frac15+\frac{15^2}{250})=2.1917$$

$$\hat{\beta_0}\pm t_{\alpha/2}se(\hat{\beta_0})=4.1\pm3.182\times2.1917=(-2.874,11.074)$$

 > Since zero is inside thsi interval, ture intercept ($\beta_0$) can be zero with 95% confidence.

### 2.4.1 Confidence Intervals on $\beta_0,\ \beta_1$, and $\sigma^2$

> $100(1-\alpha)%$ Confidence Interval for the true slope $(\beta_1)$:

$$\hat{\beta_1}\pm t_{\alpha/2}se(\hat{\beta_1})=\hat{\beta_1}\pm t_{\alpha/2} \sqrt{\frac{\hat\sigma^2}{S_{xx}}}=\hat{\beta_1}\pm t_{\alpha/2,n-2}\times \sqrt{\frac{MSE}{\sum (x_i-\bar{x})^2}}=\hat{\beta_1}\pm t_{\alpha/2,n-2}\times \left(\frac{\sqrt{n}\hat{\sigma}}{\sqrt{n-2} \sqrt{\sum (x_i-\bar{x})^2}}\right)$$

> $100(1-\alpha)%$ Confidence Interval for the true intercept $(\beta_0)$:

$$\hat{\beta_0}\pm t_{\alpha/2}se(\hat{\beta_0})=\hat{\beta_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=\hat{\beta_0}\pm t_{\alpha/2,n-2}\times \left(\sqrt{\dfrac{MSE}{n}}\right)=\hat{\beta_0}\pm t_{\alpha/2,n-2}\times \left(\sqrt{\dfrac{\hat{\sigma}^2}{n-2}}\right)$$

> $100(1-\alpha)%$ Confidence Interval for the true error variance $(\sigma^2)$:

$$\Bigg[\dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{\frac\alpha2}}, \dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{1-\frac\alpha2}}\Bigg ]$$

> If $T\sim t(df=\upsilon)$, then $T^2\sim F(1,\upsilon)$

Therefore, test statistic ($t_0$) for testing true slope ($\beta_1$) different from zero and F test statistic ($F_0$) in ANOVA for simple linear regression are related as

$$t_0^2=\left(\frac{\hat\beta_1}{se(\hat\beta_1)}\right)^2=\frac{MSR}{MSE}=F_0$$

The t test has more flexibility than F test because,  

> t test can be used when testing $H_0:\ \beta_1=c$ versus $H_1:\ \beta_1\ne c$ where c is NOT zero while F test can be used only when c=0

> t test can be used when it is a one-tailed test (such as $H_1:\ \beta_1>c$ or $H_1:\ \beta_1<c$ while F test can be used for only two-tailed test.

### 2.4.2 Interval Estimation of the Mean Response & 2.5 Prediction of New Observations

One of the goals in regression analysis is to predict the response (y) for a given value/s of predictor/s (x).  
Assume there are n pairs of observatin $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ to fit least squares simple linear regression model.    
Let’s say we observe k number of new values of response (y) at a new value of predictor (x) denoted by $x_0$ where $x_0$ between minimum and maximum observed values of predictor (x).  
Let $\bar y_0|x_0=\bar y_0$ be the true mean of k values of response at new value (x_0) of predictor.   
Then$\hat y_0|x_0=\hat y_0=\hat\beta_0+\hat\beta_1x_0$ is the estimated mean of k values of response at new value ($x_0$)) of predictor using the fitted least squares model.   
Therefore, $100(1−\alpha)%$ **confidence** interval for the **MEAN** response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

Therefore, $100(1−\alpha)%$ **prediction** interval for a **single value** of response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

> Where $t_{\alpha/2}$ is the critical value from t distribution with (n-2) degrees of freedom

#### Consider the random variable $\hat y_0-\bar y_0$ and find its expected value and variance.


We know $\hat\beta_1$ and $\hat\beta_1$ have normal distributions. Therefore,

$$\hat y=\hat\beta_0+\hat\beta_1x \sim N$$

Similarly,

$$\hat y_0|x_0=\hat y_0=\hat\beta_0+\hat\beta_1x_0 \sim N$$

$$\frac{\hat y_0|x_0-(\hat\beta_0+\hat\beta_1x_0)}{se(\hat y_0|x_0)}=\frac{\hat y_0-\bar y_0}{se(\hat y_0)}= \sim t (df=n-2)$$



The figure below is an output graph from SAS for the scatterplot of data in Computer Lab 1 with overlaid fitted model, 95% confidence interval for mean response, and 95% prediction interval for a single new observation.

Be careful when predicting response for a given new value of predictor.

-Do not extrapolate!

### 2.6 Coefficient of Determination

If the fitted model is statistically significant, we expect

-Definition

> Coefficient of Determination $=R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$

> Proportion of variation in response(y) explained by the fitted model with predictor(s) $0\ge R^2\ge1$

-The coefficient of variation and correlation coefficient are related

$$|Correlation Coefficient|=|r|=\sqrt R^2=\sqrt {Coefficient of Determination}$$

-Unfortunately, adding predictor(s) to the model does not decrease $R^2$. Therefore, it cannot be used to compare models with different number of predictors.

### 2.10 Regression through the origin

There are situations where we know that intercept of the simple linear regression model is zero. Then we fit the model with intercept,$\beta_0=0$ so the line goes through origin (0,0). 

$$y_i=\beta_1x_i+\epsilon_i\quad where\ \epsilon_i\sim iid N(0,\sigma^2)$$
Find the least squares estimator of slope ($\beta_1$) for the simple linear regression model through origin.

$$\hat\beta_1=\frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}$$

$$E(\hat\beta_1)==\beta_1$$

$$E(y_i)=\beta_0+\hat\beta_1x_i$$ is unbiased;
$$Var(\hat\beta_1)==\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$$

$$se(\hat{\beta_1})=\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$$

$$\hat{\beta_1}\pm t_\frac\alpha2se(\hat{\beta_1})$$

Where $t_\frac\alpha2$ is critical value from t distribution with df=n-1