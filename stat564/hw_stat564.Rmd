---
title: 'STAT564 Homework'
author: ""
date: "Fall 2018"
output: 
  html_document:
   toc: false
   toc_float: false

---


```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```


# {.tabset .tabset-fade .tabset-pills}

## HW0
### Problem 1
 Let $X_1,X_2,X_3\sim iid\ N(\mu,\sigma ^2), and\ Z_i=\frac{X_i-\mu}{\sigma}\ for\ i=1,2,3$  
 (a). Specify the probability distribution of $Z_i$ along with the value(s) of parameter(s).  

 > $X_1,X_2,X_3$ are three **independent and identically distributed random variable** that follows a normal distribution with same mean $\mu$ and variance $\sigma^2$  
According to the definition, $Z_i=\frac{X_i-\mu}n$ is called a **standard normal random variable**.  

 $$\therefore Z_{i}\sim iid\ N\left(0,1\right)$$   

 Let $y_{1}=\sum_{i=1}^{3}(\frac{X_{i}-\mu}{\sigma})$  
 (b). Comptute the $E(y_{1})$  
 $$\because X_i\ are\ independent\ variables\ and\ \frac{X_i-\mu}\sigma=Z_i\sim N(0,1)$$
 $$So\quad E(y_1)=E(\sum_{i=1}^3\frac{X_i-\mu}\sigma)=\sum_{i=1}^3E(Z_i)=0$$
 (c). Comptute the $Var(y_{1})$  
 $$And\quad Var(y_{1})=Var(\sum_{i=1}^3\frac{X_i-\mu}\sigma)=\sum_{i=1}^3Var(Z_i)=3$$
 (d). Specify the probability distribution of $y_{1}$ along with the value(s) of parameter(s).  

 > According to the above results, and $Z_i$ is a SND  

 $$\therefore y_1\sim N[E(Z_i),Var(Z_i)]=N(0,3)$$
 (e). Specify the probability distribution of $Z_{i}^2$ along with the value(s) of parameter(s).  

 > According to the definition of **Chi-square** distribution  
   When $Z_i$ is a SND,  $Z_{i}^2$ is a chi-square distribution with 1 degree of freedom

 $$\therefore Z_i^2\sim \chi_1^2,\quad E(\chi_1^2)=1,\quad Var(\chi_1^2)=2\times1=2$$

 Let $y_2=\sum_{i=1}^{3}(\frac{X_{i}-\mu}{\sigma})^2$  
 (f). Specify the probability distribution of $y_2$ along with the value(s) of parameter(s). 

 > According to the definition of Chi-square distribution,  when $X_i$ be indepentdent normally distributed random variables with $E(X_i)=\mu,\quad  Var(X_i)=\sigma^2,\quad \frac{X_{i}-\mu}{\sigma}=Z_i$  
then the sum of n squared standard normal random variables follows a $\chi^2$ distribution **with n degrees of freedom**  

 $$\therefore y_2=\sum_{i=1}^3(\frac{X_{i}-\mu}{\sigma})=\sum_{i=1}^3Z_i\sim\chi_3^2,\quad E(\chi_3^2)=3,\quad Var(\chi_3^2)=2\times3=6$$

### Problem 2

 Let $X_{1},X_{2},...X_{n}\sim iid\ N\left(0,\sigma ^{2}\right), and\ \bar{A}=\frac{\sum_{i=1}^{n}X_{i}}{n}$  
 (a). Specify the distribution of $\bar{A}$ along with the value(s) of parameter(s).  

 > According to the definition of sample mean, when $X_i$ are independent identical random variables of size n with **mean of the distribution** $\mu$ and variance $\sigma^2$ from a population, the **sample mean** $\bar{A}=\frac{\sum_{i=1}^{n}X_{i}}{n}$

 $$So\quad E(\bar{A})=E(\frac{\sum_{i=1}^{n}X_{i}}{n})=\frac1n\sum_{i=1}^nE(X_i)=0$$
 $$And\quad Var(\bar{A})=Var(\frac{\sum_{i=1}^{n}X_{i}}{n})=\frac1{n^2}\sum_{i=1}^n[Var(X_i)]=\frac{n\sigma^2}{n^2}=\frac{\sigma^2}n$$
 $$\therefore \bar{A}\sim\ N(0,\frac{\sigma^2}n)$$

 Let $W\sim\chi_{\nu_{1}}^2$ where $\nu_{1}$ is the degrees of freedom, $\bar{X}\ and\ W$ are indipendent.  
 Consider the new random variable $\frac{\bar{X}\sqrt n}{\sigma\sqrt{W/\nu_{1}}}$  
 (b). Specify the distribution of this new randome variable along with the value(s) of parameter(s).  

 > According to the **Central Limit Theorem**, If $X_i\quad i=1,2,...n$ are independent identically distributed random  variables with $E(X_i)=\mu$ and $Var(X_i)=\sigma^2<\infty$ and n is sufficiently large, then $\frac{(\bar{X}-\mu)\sqrt n}{\sigma}$ is a standard normal distribution.

 > According to the defination of **t distribution**, when $Z\sim N(0, 1),\ W\sim\chi_{\nu_1}^2$, and Z and W are independent, then

$$\frac{(\bar{X}-0)\sqrt n}{\sigma\sqrt{W/\nu_1}}=\frac{Z}{\sqrt{W/\nu_1}}\sim t_{\nu_1}$$  

 > where $t_{\nu_1}$ is the t distribution with $\nu_1$ degrees of freedom.  

$$E(t_{\nu_1})=0\quad when\ \nu_1>1,\quad Var(t_{\nu_1})=\begin{cases}\frac{\nu_1}{\nu_1-2}&when\ \nu_1>2 \\\infty &when\ 1<\nu_1<2\end{cases}$$

 Let $V\sim\chi_{\nu_2}^2$ where $\nu_{2}$ is the degrees of freedom, $V\ and\ W$ are indipendent.  
 Consider the new random variable $\frac{W/\nu_{1}}{V/\nu_{2}}$    
 (c). Specify the distribution of this new randome variable and provide value(s) of parameter(s).  
 
 > According to the definition of **F distribution**, when $W\sim\chi_{\nu_{1}}^2,\quad W\sim\chi_{\nu_{1}}^2,\quad W\ and\ V$ are independent, then the ratio of two independent chi square random variables, each divided by their respective degrees of freedom, follows an F distribution.

$$\frac{W/\nu_{1}}{V/\nu_{2}}\sim F_{\nu_{1},\nu_{2}}$$

 > where $F\nu_1,\nu_2$ is the F distribution with $\nu_1$ and $\nu_2$ degrees of freedom.     

$$E(F_{\nu_{1},\nu_{2}})=\frac{\nu_2}{\nu_2-2}\quad when\ \nu_2>2,\quad Var(F_{\nu_{1},\nu_{2}})=\frac{2\nu_2^2(\nu_1+\nu_2-2)}{\nu_1(\nu_2-2)^2(\nu_2-4)}\quad when\ \nu_2>4$$

### Problem 3

 Let $Y|x=a+bx+\epsilon$ where a, b are constants, $E(\epsilon)=0,\ and\ Var(\epsilon)=\sigma^2$  
 
a. Comptute the $E(Y|x_{0})$  

$$E(Y|x_{0})=E(a+bx_0+\epsilon)=E(a)+E(bx_0)+E(\epsilon)$$
$$\because a,b,x_0\ are\ constant\quad E(a)=a,\ E(bx_0)=bx_0,\ and\quad E(\epsilon)=0$$
$$\therefore E(Y|x_{0})=a+bx_0$$

a. Comptute the $Var(Y|x_{0})$  

$$Var(Y|x_{0})=Var(a+bx_0+\epsilon)=Var(a)+Var(bx_0)+Var(\epsilon)$$

$$\because a,b,x_0\ are\ constant\quad Var(a)=0,\ Var(bx_0)=0,\ and\quad Var(\epsilon)=\sigma^2$$
$$\therefore Var(Y|x_{0})=\sigma^2$$

### Proof $E(\hat{\beta_0})=\beta_0$
We have known: 


 <span>number</span> | <span>meaning</span> | <span>equation</span>
 - | :---------- | :----------
(1) |  population regression model | $y=\beta_1+\beta_0x+\epsilon$
(2) |  assumption_2 of simple linear regression | $E(\epsilon)=0$
(3) |  mean of this distribution | $E(y|x)=a+bx$
(4) |  sample regression model | $y_i=\beta_1+\beta_0x_i+\epsilon_i,\quad i=1,2,3...n$
(5) |  fitted regression model (fitted value)  | $\hat{y_i}=\hat{\beta_1}+\hat{\beta_0}x_i$
(6) | least-squares estimator (slope) | $\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=\sum_{i=1}^nc_iy_i$
(7) | unbiased estimator (Intercept) |  $\hat{\beta_0}=\bar y-\hat{\beta_1}\bar x=\frac1n{\sum_{i=1}^ny_i}-\bar x(\sum_{i=1}^n c_iy_i)=\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i$
(8) | sample mean  |  $\bar x=\frac1n\sum_{i=1}^n x_i$
(9) | sum of corrected squares of $x_i$ | $S_{xx}=\sum_{i=1}^nx_i(x_i-\bar x)=\sum_{i=1}^n(x_i-\bar x)^2$ 
(10) | sum of corrected cross-products of $x_i\& y_i$ | $S_{xy}=\sum_{i=1}^nx_i(x_i-\bar x)$ 
(11) | | $c_i=\frac{x_i-\bar x}{S_{xx}}$


 $\text{According to 4, 7}\quad E(\hat{\beta_0})=E[\sum_{i-1}^n(\frac1n-\bar xc_i)(\beta_0+\beta_1x_i+\epsilon_i)]=\frac1n\sum_{i=1}^nE(\beta_0+\beta_1x_i+\epsilon_i)-\bar x\sum_{i=1}^nc_iE(\beta_0+\beta_1x_i+\epsilon_i)]$ 

$\text{According to 2, 3}\quad=\frac1n\sum_{i=1}^n[\beta_0+\beta_1x_i+E(\epsilon_i)]-\bar x\sum_{i=1}^nc_i[\beta_0+\beta_1x_i+E(\epsilon_i)]=\beta_0+\beta_1\frac{\sum_{i=1}^nx_i}n-\beta_0\bar x\sum_{i=1}^nc_i-\beta_1\bar x\sum_{i=1}^nc_ix_i$
 
 $\text{According to 8, 11}\quad \sum_{i=1}^nc_i=\sum_{i=1}^n\frac{x_i-\bar x}{S_{xx}}=\frac1{S_{xx}}\sum_{i=1}^n(x_i-\bar x)=\frac1{S_{xx}}(n\bar x-n\bar x)=0$
 
 $\text{According to 9,11}\quad \sum_{i=1}^nc_ix_i=\frac1{S_{xx}}\sum_{i=1}^nx_i(x_i-\bar x)=\frac{S_{xx}}{S_{xx}}=1$
 
 $$\therefore\quad E(\hat{\beta_0})=\beta_0+\beta_1\bar x-0-\beta_1\bar x=\beta_0$$


## HW1

### Problem 1: 

Exercise 2.3 (Page 58): Table B.2 presents data collected during a solar energy project at Georgia Tech.
y: Total heat flux (kwatts)  
x1 : Insolation (watts/m2)   
x2 : Position of focal point in east direction (inches)   
x3 : Position of focal point in south direction (inches)   
x4 : Position of focal point in north direction (inches)   
x5 : Time of day

 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b2 <- read_xlsx("Table B.2.xlsx")

# Observe the data frame
head(table_b2)

# visualizng y and x4
table_b2 %>% ggplot(aes(x4,y))+geom_point()+geom_smooth(method="lm", level=0.99)
```

 > There seems to be a decreasing (negative) medium approximately linear relationship..

a. Fit a simple linear regression model relating total heat flux y (kilowatts) to the radial deflection of the deflected rays $x_4$ (milliradians).

```{r}
# build the model
model_y_x4 <- lm(y ~ x4, data=table_b2)
model_y_x4
```

 > The model is $y=607.1-21.4x_4$

b. Construct the analysis-of-variance table and test for significance of regression.

```{r}
anova(model_y_x4)
```

 > The table shows that p-value is less than 5.935e-09. There is strong evidence that the fitted model is statistically significant at 5% significance level because p-value is much smaller than 0.05.

c. Find a 99% CI on the slope.

 > The equation for the 99% CI for the estimated β1 is defined as:

$$\beta_1 \pm t_{\alpha / 2, n - 2} \left(\frac{\sqrt{MSE}}{\sqrt{\sum (x_i - \bar{x})^2}}\right)$$

```{r}
model_y_x4 %>% confint(level=0.99)
```

 > The 99% confidence interval for slope of the regression line is (-28.50995, -14.29497).
The confidence interval for β1 does not contain 0.Therefore, we are 95% confidence that the true slope is different from zero.


d. Calculate $R^2$.

```{r}
model_y_x4 %>% summary()
```

 > According to the table, Multiple R-squared is 0.7205, Adjusted R-squared is 0.7102. This is not so close to 100% and therefore, fitted model explain moderate amount of variation is the heat flux with radial deflection.

e. Find a 95% CI on the mean heat flux when the radial deflection is 16.5 milliradians.

> The confidence interval around the mean response, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.
 
```{r}
model_y_x4 %>% predict(newdata = data.frame(x4=16.5), interval = "confidence", level=0.95)
```

 > From the output, the fitted heat flux is 253.9627 kwatts when the radial deflection is 16.5 milliradians. The confidence interval of (249.1468, 258.7787) signifies the range in which the true population parameter lies at a 95% level of confidence.

### Problem 2: 

Exercise 2.4 (Page 58): Table B.3 presents data on the gasoline mileage performance of 32 different automobiles.  
y : Miles/gallon   
x1 : Displacement (cubic in.)   
x2 : Horsepower (ft-lb)   
x3 : Torqne (ft-lb)   
x4 : Compression ratio   
x5 : Rear axle ratio Source : Motor Trend , 1975  
x6 : Carburetor (barrels)  
x7 : No. of transmission speeds   
x8 : Overall length (in.)   
x9 : Width (in.)   
x10 : Weight (lb)   
x11 : Type of transmission (A automatic; M manual)  

```{r, echo=TRUE}

# Import Data
table_b3 <- read_xlsx("Table B.3.xlsx")

# Observe the data frame
head(table_b3)

# visualizng y and x1
table_b3 %>% ggplot(aes(x1,y))+geom_point()+geom_smooth(method="lm")
```

 > There seems to be a decreasing (negative) medium approximately linear relationship.

a. Fit a simple linear regression model relating gasoline mileage y (miles per gallon) to engine displacement x1 (cubic inches).


```{r}
# build the model
model_y_x1 <- lm(y ~ x1, data=table_b3)
model_y_x1
```

 > The model is $y=33.72744-0.04743x_1$.

b. Construct the analysis-of-variance table and test for significance of regression.


```{r}
anova(model_y_x1)
```

 > The table shows that p-value is 3.82e-11. There is strong evidence that the fitted model is statistically significant at 5% significance level because p-value is much smaller than 0.05.

c. What percent of the total variability in gasoline mileage is accounted for by the linear relationship with engine displacement?

```{r}
model_y_x1 %>% summary()
```

 > According to the Multiple R-squared value, there are 77.2% variability in gasoline mileage is accounted for by the linear relationship with engine displacement. This is not so close to 100% and fitted model explain moderate amount of variation.

d. Find a 95% CI on the mean gasoline mileage if the engine displacement is 275 in.^3

 > The confidence interval around the mean response, denoted $\mu_y$, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.

```{r}

model_y_x1 %>% predict(newdata = data.frame(x1=275), interval = "confidence", level=0.95)

```

 > From the output, the fitted gasoline mileage when the engine displacement is 275 in.^3 is 20.68466 gasoline mileage. The confidence interval of (19.57343, 21.79589) signifies the range in which the true population parameter lies at a 95% level of confidence.

e. Suppose that we wish to predict the gasoline mileage obtained from a car with a 275-in.3 engine. Give a point estimate of mileage. Find a 95% prediction interval on the mileage.

 > The prediction interval is rather similar to the confidence interval in calculation, but as mentioned earlier, there are significant differences. The prediction interval equation is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(1 + \frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum (x_i - \bar{x})^2} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(1+\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.

```{r}

model_y_x1 %>% predict(newdata = data.frame(x1=275), interval = "prediction", level=0.95)

```

 > When the engine displacement is 275 in.^3, the 95% prediction interval on the mean gasoline mileage is (14.32311, 27.04622).

f. Compare the two intervals obtained in parts d and e. Explain the difference between them. Which one is wider, and why?

 > The fitted value is the same, but the prediction interval is wider than confidence interval due to the additional term in the standard error of prediction. The prediction interval is wider than the confidence interval because prediction interval is for a single observation of response while confidence interval is for the mean response. The prediction interval depends on both the error from the fitted model and the error associated with future observations. It is difficult to predict a single observation than mean response and hence there is more variation in the new observation.
 
### Problem 3: 

Exercise 2.12 (Page 60): The number of pounds of steam used per month at a plant is thought to be related to the average monthly ambient temperature. The past year’s usages and temperatures follow.

```{r, echo=TRUE}

# Import Data
table_12 <- read_xlsx("Problem 2.12.xlsx")

# Observe the data frame
glimpse(table_12)

# visualizng `Usage/l000` and Temperature
table_12 %>% ggplot(aes(Temperature,`Usage/l000`,col=as.factor(Month)))+geom_point( )+geom_smooth(method="lm")
```

> There seems to be a increasing strong linear relationship.

a. Fit a simple linear regression model to the data.

```{r}
# build the model
model_use_tem <- lm(`Usage/l000` ~ Temperature, data=table_12)

model_use_tem %>% summary()
```

 > The model is `Usage/l000`=-6.33209+9.20847*Temperature

b. Test for significance of regression.

> According to the analysis-of-variance table (a.), the table shows that p-value is less than 2.2e-16. There is strong evidence that the fitted model is statistically significant at 5% significance level because p-value is much smaller than 0.05.

c. Plant management believes that an increase in average ambient temperature of 1 degree will increase average monthly steam consumption by 10,000lb. Do the data support this statement?

 > We made the null hypothesis H0: $\hat\beta_1=10$ the altertative hypothesis H1: $\hat\beta_1\ne10$. Accroding the method of t test (2.3.1)
 
$$t_0=\frac{\hat\beta_1-\beta_{10}}{\sqrt\frac{M SE}{S_{xx}}}=\frac{\hat\beta_1-\beta_{10}}{se(\hat\beta_1)}$$
 
```{r} 
2*pt(q = (9.20847-10)/0.03382, df = nrow(table_12)-2 , lower.tail = TRUE)
```
 > The result shows that the p_value is 4.593537e-10, which is far less than 0.05. Therefore, we can reject the H0 and the true slope is not equal 10. The data don't support the statement.

d. Construct a 99% prediction interval on steam usage in a month with average ambient temperature of 58°.


```{r}

model_use_tem %>% predict(newdata = data.frame(Temperature=58), interval = "prediction", level=0.99)

```

 > When the Temperature is 58°, the 99% prediction interval on the steam usage is (521.2237, 534.2944).

### Problem 4: 

Exercise 2.25 (Page 65): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$, with $E(\epsilon)=0,\ Var(\epsilon)=\sigma^2$, and $\epsilon$ uncorrelated.  

 > According to the definition of covariance,  
 
$$Cov(X,Y)=\sigma_{XY}=E[(X-\bar X)(Y-\bar Y)]=\frac1n\sum_{i=1}^n(x_i-\bar X)(y_i-\bar Y)$$

 > According to the properties of covariance,  

$$Cov(X,a)=0,\quad Cov(X,X)=Var(X)\equiv\sigma^2(X)\equiv\sigma_X^2$$

$$Cov(X,Y)=Cov(Y,X),\quad Cov(aX,bY)=ab\ Cov(X,Y)$$

$$Cov(X+a,Y+b)=Cov(X,Y)$$

$$And\quad Cov(a_1X_1+a_2X_2,\ b_1Y_1+b_2Y_2)=a_1b_1Cov(X_1,Y_1)+a_1b_2Cov(X_1,Y_2)+a_2b_1Cov(X_2,Y_1)+a_2b_2Cov(X_2,Y_2)$$

a. Show that $Cov(\hat\beta_0, \hatβ_1)=−\overline x\sigma^2 S_{xx}$.

 > Accroding to the properties of the Least-Squares Estimators and the Fitted Regression Model (2.2.2)

$$\hat\beta_1=\sum_{i=1}^n c_iy_i=\frac{\sum_{i=1}^n (x_i-\bar x)}{S_{xx}}y_i$$

$$\hat\beta_0=\bar y-\hat\beta_1\bar x=\frac{\sum_{i=1}^n y_i}n-\frac{\bar x\sum_{i=1}^n (x_i-\bar x)}{S_{xx}}y_i=\sum_{i=1}^n\left(\frac1n-\frac{\bar x(x_i-\bar x)}{S_{xx}}\right)y_i$$

> For $Cov(aX,bY)=ab\ Cov(X,Y)$

$$Cov(\hat\beta_0, \hatβ_1)=Cov\left[\sum_{i=1}^n\left(\frac1n-\frac{\bar x(x_i-\bar x)}{S_{xx}}\right)y_i,\ \frac{\sum_{i=1}^n (x_i-\bar x)}{S_{xx}}y_i \right]=\sum_{i=1}^n\left(\frac1n-\frac{\bar x(x_i-\bar x)}{S_{xx}}\right)\frac{(x_i-\bar x)}{S_{xx}}Cov(y_i,y_i)$$

$$=\left[\frac{\sum_{i=1}^n(x_i-\bar x)}{nS_{xx}}-\frac{\bar x\sum_{i=1}^n(x_i-\bar x)^2}{S_{xx}^2}\right]Cov(y_i,y_i)$$

$$\because\quad \sum_{i=1}^n(x_i-\bar x)=0,\quad \sum_{i=1}^n(x_i-\bar x)^2=S_{xx},\quad Cov(y_i,y_i)=Var(y_i)=\sigma^2$$

$$\therefore Cov(\hat\beta_0, \hatβ_1)=\left[\frac0{nS_{xx}}-\frac{\bar x}{S_{xx}}\right]\sigma^2=−\bar x\frac{\sigma^2}{S_{xx}}$$


b. Show that $Cov(\bar y,\hat\beta_1) = 0$.

According to above results,

$$Cov(\bar y,\hat\beta_1)=Cov\left[\frac{\sum_{i=1}^n y_i}n,\ \frac{\sum_{i=1}^n (x_i-\bar x)}{S_{xx}}y_i\right]=\frac{\sum_{i=1}^n(x_i-\bar x)}{nS_{xx}}Cov(y_i,y_i)=\frac0{nS_{xx}}Var(y_i)=0$$

### Problem 5: 
Exercise 2.26 (Page 65): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$, with $E(\epsilon)=0,\ Var(\epsilon)=\sigma^2$, and $\epsilon$ uncorrelated.

a. Show that $E(MS_R)=\sigma^2+\beta_1^2S_{xx}$

$$\because \hat y_i=\hat\beta_0+\hat\beta_1x_i,\quad \hat\beta_0=\bar y-\bar x\hat\beta_1$$

$$SSR=\sum_{i=1}^n(\hat y_i-\bar y)^2=\sum_{i=1}^n(\hat\beta_0+\hat\beta_1x_i-\bar y)^2=\sum_{i=1}^n(\bar y-\bar x\hat\beta_1+\hat\beta_1x_i-\bar y)^2=\hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2=\hat\beta_1^2S_{xx}$$

$$\because Var(\hat\beta_1)=E(\hat\beta_1^2)-[E(\hat\beta_1)]^2=\frac{\sigma^2}{S_{xx}},\quad E(\hat\beta_1)=\hat\beta_1$$

$$\therefore E(\hat\beta_1^2)=Var(\hat\beta_1)+[E(\hat\beta_1)]^2=\frac{\sigma^2}{S_{xx}}+\hat\beta_1^2$$


$$\therefore E(MSR)=\frac{E(SSR)}1=E(\hat\beta_1^2S_{xx})=S_{xx}E(\hat\beta_1^2)=S_{xx}(\frac{\sigma^2}{S_{xx}}+\beta_1^2)=\sigma^2+\beta_1^2S_{xx}$$


b. Show that $E(MS_{Res})=\sigma^2$.

> Step 1:

$$SSE=\sum_{i=1}^n(y_i-\hat y)^2=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}^n(y_i-\bar y+\bar x\hat\beta_1-\hat\beta_1x_i)^2=\sum_{i=1}^n((y_i-\bar y)-\hat\beta_1(x_i-\bar x))^2$$



$$=\sum_{i=1}^n(y_i-\bar y)^2-2\hat\beta_1(y_i-\bar y)(x_i-\bar x)+\hat\beta_1^2(x_i-\bar x)^2=S_{yy}-2\hat\beta_1S_{xy}+\hat\beta_1^2S_{xx}$$
$$\because \hat\beta_1=\frac{S_{xy}}{S_{xx}}\quad\therefore S_{xy}=\hat\beta_1S_{xx}$$

$$\implies SSE=S_{yy}-2\hat\beta_1\hat\beta_1S_{xx}+\hat\beta_1^2S_{xx}=S_{yy}-\hat\beta_1^2S_{xx}=S_{yy}-SSR$$

> Step 2:

$$E(S_{yy})=E\sum_{i=1}^n(y_i-\bar y)^2=E\sum_{i=1}^n\left[(\beta_0+\beta_1x_i+\epsilon_i)-\frac1n\sum_{i=1}^n(\beta_0+\beta_1x_i+\epsilon_i)\right]^2=E\left(\sum_{i=1}^n[\beta_1(x_i-\bar x)+\epsilon_i-\bar\epsilon]^2\right)$$

$$E\left(\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2+2\beta_1\sum_{i=1}^n(x_i-\bar x)(\epsilon_i-\bar\epsilon)+(\epsilon_i-\bar\epsilon)^2\right)=\hat\beta_1^2S_{xx}+2\beta_1\sum_{i=1}^n(x_i-\bar x)[E(\epsilon_i)-E(\bar\epsilon)]+E(S_{\epsilon\epsilon})$$

$$=\hat\beta_1^2S_{xx}+E(S_{\epsilon\epsilon})=\hat\beta_1^2S_{xx}+(n-1)E(S_{\epsilon}^2)=\hat\beta_1^2S_{xx}+(n-1)\sigma^2$$

$$\therefore E(SSE)=E(S_{yy})-E(SSR)=\hat\beta_1^2S_{xx}+(n-1)\sigma^2-(\sigma^2+\hat\beta_1^2S_{xx})=(n-2)\sigma^2$$

> Step 3:

$$E(MSE)=E(\frac{SSE}{n-2})=\frac{E(SSE)}{n-2}=\frac{\sigma^2(n-2)}{n-2}=\sigma^2$$

### Problem 6: 

Exercise 2.27 (Page 65): Suppose that we have fit the straight-line regression model $\hat y=\hat\beta_0+\hat\beta_1x$ but the response is affected by a second variable $x_2$ such that the true regression function is

$$E(y)=\beta_0+\beta_1x_1+\beta_2x_2$$

a. Is the least-squares estimator of the slope in the original simple linear regression model unbiased?

$$\because \sum_{i=1}^n(x_{i1}-\bar x)=\sum_{i=1}^n{x_{i1}}-n\bar x=\sum_{i=1}^n{x_{i1}}-n\frac{\sum_{i=1}^n{x_{i1}}}n=0,\quad S_{xx}=\sum_{i=1}^n(x_{i1}-\bar x)x_{i1}$$

$$\therefore E(\hat\beta_1)=E(\sum_{i=1}^nc_iy_i)=E(\frac{\sum_{i=1}^n(x_i-\bar x)y_i}{S_{xx}})=\frac{\sum_{i=1}^n(x_{i1}-\bar x)}{Sxx}E(y_i)=\frac{\sum_{i=1}^n(x_{i1}-\bar x)}{Sxx}E(\beta_0+\beta_1x_{i1}+\beta_2x_{i2})$$

$$=\frac0{S_{xx}}\beta_0+\frac{\sum_{i=1}^n(x_{i1}-\bar x)x_{i1}}{S_{xx}}\beta_1+\frac{\sum_{i=1}^n(x_{i1}-\bar x)x_{i2}}{S_{xx}}\beta_2=\beta_1+\frac{\sum_{i=1}^n(x_{i1}-\bar x)x_{i2}}{S_{xx}}\beta_2$$

Thus, $\hat\beta_1$ have bias.

b. Show the bias in $\hat\beta_1$

$$E(\hat\beta_1)-\beta_1=\frac{\sum_{i=1}^n(x_{i1}-\bar x)x_{i2}}{Sxx}\beta_2$$

### Problem 7: 
Exercise 2.32 (Page 66): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$ where the intercept $\beta_0$ is known.

a. Find the least-squares estimator of $\beta_1$ for this model. Does this answer seem reasonable?

$$Let\quad SSE=\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)^2$$

> The least-squares estimators of $\beta_1$, must satisfy

$$\left.\frac{\partial SSE}{\partial\beta_1}\right|_{\hat\beta_1}=-2\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)x_i=0$$

$$\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^n(y_i-\beta_0)x_i\implies \hat\beta_1=\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}$$

> The least-squares regression line passes through the point ($\beta_0,0$). Because $y_i-\beta_0=\beta_1x_i+\varepsilon$

$$E(\hat\beta_1)=E\left[\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right]=\frac1{\sum_{i=1}^nx_i^2}E\left[\sum_{i=1}^nx_i(\beta_1x_i+\varepsilon)\right]=\frac{\beta_1\sum_{i=1}^nx_i^2}{\sum_{i=1}^nx_i^2}-\frac{\sum_{i=1}^nx_i}{\sum_{i=1}^nx_i^2}E[\varepsilon]=\beta_1$$

> The estimator $\beta_1$ is unbiased and reasonable. 

b. What is the variance of the slope ($\hat\beta_1$) for the least-squares estimator found in part a?

$$Var(\hat\beta_1)=Var\left(\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i-\sum_{i=1}^n\beta_0x_i)$$

$$=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i)=\frac{\sum_{i=1}^nx_i^2}{(\sum_{i=1}^nx_i^2)^2}Var(y_i)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$$

c. Find a 100(1−α) percent CI for $\beta_1$. Is this interval narrower than the estimator for the case where both slope and intercept are unknown?

 > When $\beta_0$ is known and is a consistant.

$$\therefore se(\hat{\beta_1})=\sqrt{Var(\hat\beta_1)}=\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}=\sqrt{\frac{MSE}{\sum_{i=1}^n x_i^2}}$$

> a 100(1−α) percent CI for $\beta_1$ is given by

$$\hat\beta_1-t_{\frac\alpha2,n-1}\sqrt\frac{MSE}{\sum_{i=1}^n x_i^2}\le\beta_1\le\hat\beta_1+t_{\frac\alpha2,n-1}\sqrt\frac{MSE}{\sum_{i=1}^n x_i^2} $$

$$\because \sum_{i=1}^n x_i^2\ge\sum_{i=1}^n x_i^2-n\bar x^2\quad and\quad t_{\frac\alpha2,n-1}<t_{\frac\alpha2,n-2}$$

 > confidence intervals for $\beta_1$ will be narrower when $\beta_0$ is known,
regardless of sample size (unless $\bar x=0$).

## HW2

### Problem 1
The weight and systolic blood pressure of 26 randomly selected a group of adults are given in Weight vs BP text file

 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE, results='hide', message = FALSE, collapse=TRUE}
library(tidyverse)
# Import Data
table_weight_bp <- read_table2("Weight vs BP.txt")
# Observe the data frame
head(table_weight_bp)
# build the model
model_wei_bp <- lm(SystolicBP ~ Weight, data=table_weight_bp)
model_wei_bp_origin <- lm(SystolicBP ~ Weight+0, data=table_weight_bp)
model_wei_bp%>% summary()
model_wei_bp_origin%>% summary()
confint(model_wei_bp, level = 0.99)
confint(model_wei_bp_origin, level = 0.99)
anova(model_wei_bp)
anova(model_wei_bp_origin)

# visualizng

## table_weight_bp %>% ggplot(aes(Weight,SystolicBP))+geom_point(alpha=0.5)+xlim(0,max(table_weight_bp$Weight))+ylim(0,max(table_weight_bp$SystolicBP))+geom_abline(mapping=aes(slope=0.4194, intercept=69.1044),linetype=2,col="red")+geom_abline(mapping=aes(slope=0.7916, intercept=0),linetype=3,col="blue")+labs(linetype="")

## plot(table_weight_bp$Weight, table_weight_bp$SystolicBP,col="red", pch = 16, xlab = "Weight", ylab = "SystolicBP") 
## abline(model_wei_bp, col = "blue", lwd = 3)
## abline(model_wei_bp_origin, col = "black", lwd = 3, lty = 3)
## legend(30, 780, legend=c("Observed", "Fit 1", "Fit 2 (no intercept)"), col=c("red", "blue", "black"), lty=c(0,1,3), pch = c(16, NA_integer_, NA_integer_), lwd = c(1,3,3), cex=0.8)
```
(a) Fit a simple linear regression model to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

The simple linear model is $\hat{systolic blood pressure}=69.1044+0.4194Weight$

(b) Test for significance of true intercept of this model at 5% significance and provide the conclusion along with p-value.

The p-value is $1.71\times e^{-05}$, which is much smaller than 0.05. We can know the true intercept is not zero on 95% confidence level.

(c) Report a 99% confidence interval for true intercept of this model.

The 99% confidence interval for true intercept of this model is (32.9955223 105.2132233).

(d) Fit a simple linear regression model through origin to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

The model through origin is $\hat{systolic blood pressure}=0.7916Weight$

(e) Compare quality of two models and recommend only one model. Provide valid reasons for your recommendation.

```{r, include = FALSE, collapse=TRUE}
library(ggfortify)
model_wei_bp %>% autoplot()
model_wei_bp_origin %>% autoplot()

texreg::screenreg(l=list(model_wei_bp, model_wei_bp_origin))
huxtable::huxreg(model_wei_bp, model_wei_bp_origin, statistics = NULL)
```

Firstly, the estimated intercept (69.104) is not zero on 95% confident level by p-value ($1.71\times e^{-05}$).Moreover, the 99% confidence interval (32.996~105.213) for true intercept doesn't contain zero. There are strong relationship between weight and blood pressure.

Secondly, the MSE value of model with intercept (75.36) is about half of the MSE with zero intercept (159), which shows the former is better fitted.

Thirdly, the R-suqare value of model with intercept is 0.5983, while the R-suqare value of zero-intercept model is 0.9929. We can explain that the much larger SSR in zero-intercept model contributs most of the additional R-suqare value. The sum square of total variation with intercept ($2693.6+1808.6=4502.2$) is only 1 percent of the SST with zero intercept ($551834+3968=555802$). 

Fourthly, the overlayed fitted models on the scatterplot show significant difference between two models. The line through origin doesn't fit the pattern better.

Overall, the model with intercept is recommended because the factors of P-value, CI, MSE, and pattern support it while R-square is low. To increase R-square or fit the pattern better, we should consider other omitted variables or advanced models. 


### Problem 2: 
Consider the simple linear regression model:$y_i=β_0+β_1 x_i+ε_i\quad    for  i=1,2,…,n$, which can be written for each observation as

$$
\begin{matrix}
  y_1=\beta_0+\beta_1x_1+\varepsilon_1 \\
  y_2=\beta_0+\beta_1x_2+\varepsilon_2 \\
  \vdots\quad\quad  \vdots\quad\quad   \vdots\quad\quad  \vdots  \\
  y_n=\beta_0+\beta_1x_n+\varepsilon_n 
 \end{matrix}
$$
 (a) Write this model in matrix form. Provide all the vectors and design matrix along with dimensions. Use the above format so that you can write only 4 rows for each vector and matrix. For example, Y vector is written as
 
$$
\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1} 
\mathbf{X}=\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}_{2\times1}
\mathbf{ε}=\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots  \\ \varepsilon_n \end{bmatrix}_{n\times1}
$$

 (b) Compute $\mathbf{X'X}$ (show all the main steps)

$$
\mathbf{X'X}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2}=
\begin{bmatrix} n & x_{1}+x_{2}+\cdots+x_{n} \\ x_{1}+x_{2}+\cdots+x_{n} & x_1^2+x_2^2+\cdots+x_n^2  \end{bmatrix}_{2\times2}=\begin{bmatrix} n & \sum_{i=1}^nx_i \\ \sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 \end{bmatrix}_{2\times2}
$$

 (c) Compute $\mathbf{(X'X)^{-1}}$ (show all the main steps)
Hint: The inverse of a nonsingular is obtained by 1) replacing every element of the matrix with its cofactor, 2) transposing the resulting matrix, 3) dividing by the determinant of the origin matrix.

$$
\mathbf{|X'X|}^{-1}=\frac1{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix} \sum_{i=1}^nx_i^2  & -\sum_{i=1}^nx_i \\ -\sum_{i=1}^nx_i & n \end{bmatrix}_{2\times2}
$$

$$=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n\sum_{i=1}^nx_i^2-(n\bar x_i)^2}=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n(\sum_{i=1}^nx_i^2-n\bar x_i^2)}=\frac1{nS_{xx}}{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}$$

(d) Compute X'Y (show all the main steps)

$$
\mathbf{X'Y}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1}=\begin{bmatrix} y_1 + y_2 +\cdots + y_n \\ x_{1}y_1 + x_{2}y_2 + \cdots + x_{n}y_n \end{bmatrix}_{2\times{1}}=\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}}
$$

(e) Compute (X'X)^(-1) X'Y (show all the main steps).
The final answer is the expression for the least squares estimator of β
$$
\mathbf{\hatβ}=\mathbf{(X'X)^{-1} X'Y}=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i \\ -\sum_{i=1}^nx_i & n \end{bmatrix}_{2\times2}\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}}=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i \\ -\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times1}$$

$$=\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2n\bar{y}-n\bar{x}\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \\ \frac{\sum_{i=1}^nx_{i}y_i-\frac{\sum_{i=1}^nx_i\sum_{i=1}^ny_i}{n}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \frac{\bar{y}\sum_{i=1}^nx_i^2-\bar{y}\frac{(\sum_{i=1}^nx_i)^2}n+\bar{y}\frac{(\sum_{i=1}^nx_i)^2}n-\bar{x}\sum_{i=1}^nx_{i}y_i}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}$$

$$=\begin{bmatrix} \bar{y}\frac{S_{xx}}{S_{xx}}+\frac{\bar{x}\bar{y}\sum_{i=1}^nx_i-\bar{x}\sum_{i=1}^nx_{i}y_i}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}\frac{\sum_{i=1}^nx_{i}y_i-\frac{\sum_{i=1}^nx_i\sum_{i=1}^ny_i}n}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}\frac{S_{xy}}{S_{xx}} \\ \frac{S_{xy}}{S_{xx}} \end{bmatrix}_{2\times1}=\begin{bmatrix} \bar{y}-\bar{x}{\hat\beta_1} \\ \hat\beta_1 \end{bmatrix}_{2\times1}$$

in the multiple linear regression model when k=1. It reduces to the same expression you learned for the slope and intercept for simple linear regression model.

(f) Also, we know that if $Var(ε_i )=σ^2\ and\ Cov(ε_i,ε_j )=0 for i≠j$, 

$$Var(\mathbf{\hatβ})=σ^2(\mathbf{X'X})^{-1}=\sigma^2\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2}{nS_{xx}} & \frac{-\sum_{i=1}^nx_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i)}{nS_{xx}} & \frac1{S_{xx}} \end{bmatrix}_{2\times2}=\sigma^2\begin{bmatrix} C_{11} &  C_{12} \\  C_{21} &  C_{22} \end{bmatrix}_{2\times2}$$

From this result, show that 

(i)  $Var(\hatβ_0)= σ^2 [1/n+ x^2/S_{xx}]$  

$$Var(\mathbf{\hat\beta_0})=σ^2C_{11}=\frac{\sigma^2\sum_{i=1}^nx_i^2}{nS_{xx}}=\sigma^2\frac{\sum_{i=1}^nx_i^2-n\bar x_i^2+n\bar x_i^2}{nS_{xx}}=\sigma^2(\frac{1}{n}+\frac{n\bar x_i^2}{nS_{xx}})=\sigma^2(\frac{1}{n}+\frac{\bar x_i^2}{S_{xx}})$$

(ii)  $Var(\hatβ_1)=σ^2/S_{xx}$

$$Var(\mathbf{\hat\beta_1})=σ^2C_{22}=\frac{\sigma^2}{S_{xx}}$$

(iii)  $Cov(β_0,β_1)=-(\bar x σ^2)/S_{xx}$

$$Cov(\mathbf{\hat\beta_0,\hat\beta_1})=σ^2C_{12}=\frac{\sigma^2(-\sum_{i=1}^nx_i)}{nS_{xx}}=\frac{-\sigma^2(n\bar x_i)}{nS_{xx}}=\frac{-\sigma^2\bar x_i}{S_{xx}}$$

## HW3

### Problem 1
Exercise 3.25 (Just report the T matrix, β vector and c vector along with their dimensions, and the rank of T matrix for each part).$y=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+ε$
 (a) $H_0:\ β_1=β_2=β_3=β_4=β$

$$y=β_0+β(x_1+x_2+x_3+x_4)+ε$$

$$
\mathbf{T}=\begin{bmatrix} 0 & 1 & -1 & 0 & 0 \\ 0 & 0 & 1 & -1 & 0 \\ 0 & 0 & 0 & 1 & -1  \\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}_{4\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \\ 0  \\ \beta \end{bmatrix}_{4\times1} rank(T)=4
$$

 (b) $H_0:\ β_1=β_2,\ β_3=β_4$
 
$$y=β_0+β_1(x_1+x_2)+β_3(x_3+x_4)+ε$$
 
 $$\mathbf{T}=\begin{bmatrix} 0 & 1 & -1 & 0 & 0 \\ 0 & 0 & 0 & 1 & -1 \end{bmatrix}_{2\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}_{2\times1} rank(T)=2$$
 
 (c) $H_0:\ β_1-2β_2=4β_3,\ β_1+2β_2=0$
 
$$β_1=-2β_2=2β_3 \\
y=β_0+β_1(x_1-\frac12x_2+\frac12x_3)+β_4 x_4+ε$$


$$\mathbf{T}=\begin{bmatrix} 0 & 1 & -2 & -4 & 0 \\ 0 & 1 & 2 & 0 & 0 \end{bmatrix}_{2\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}_{2\times1} rank(T)=2$$

### Problem 2: 

Exercise 3.27 Show that $Var(\mathbf{\hat y})=\sigma^2\mathbf{H}$

$$\because \mathbf{\hat y=X\hatβ},\  \mathbf{\hatβ}=\mathbf{(X'X)^{-1} X'Y}$$

$$\therefore Var(\mathbf{\hat y})=Var(\mathbf{X\hatβ})=Var(\mathbf{X(X'X)^{-1} X'Y})=Var(\mathbf{HY})=HVar(\mathbf{Y})H'$$

$$\because \mathbf{y=Xβ+ε},\quad  Var(ε)=\sigma^2\mathbf{I}$$

$$\therefore Var(\mathbf{\hat y})=H\cdot Var(\mathbf{Xβ+ε})\cdot H'=H\cdot Var(\mathbf{ε})\cdot H'=H\cdot \sigma^2\mathbf{I}\cdot H'=\sigma^2\mathbf{H}$$

$$\because \sigma\ \text{is a constant, H is symmatric and idenogent.}$$
$$\therefore H\cdot \sigma^2\mathbf{I}\cdot H'=\sigma^2\mathbf{HH'}=\sigma^2\mathbf{HH}=\sigma^2\mathbf{H}$$

### Problem 3: 

The $(X'X)^{(-1)}$ for the $y=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+ε$ is given below. If MSE = 0.513 and n=9, compute the 

(a) $Var(\hatβ_1)$

$$Var(\mathbf{\hat\beta_1})=MSE\times C_{22}=0.513\times4.624=2.372112$$

(b) $se(\hatβ_3)$

$$se(\mathbf{\hat\beta_3})=\sqrt{MSE\times C_{44}}=\sqrt{0.513\times0.031}=0.1261071$$

(c) $Cov(\hatβ_1,\hatβ_3)$

$$Cov(\mathbf{\hat\beta_1,\hat\beta_3})=MSE\times C_{24}=0.513\times(-0.346)=-0.177498$$

(d) $Cor(\hatβ_1\hat,β_3)$

$$Cor(\mathbf{\hat\beta_1,\hat\beta_3})=\frac{Cov(\mathbf{\hat\beta_1,\hat\beta_3})}{se(\mathbf{\hat\beta_1})se(\mathbf{\hat\beta_3})}=\frac{-0.177498}{1.540166\times0.1261071}=-0.913874$$

(e) Based on what you found in the previous part(s), explain the relationship between $\hatβ_1$ and $\hatβ_3$.

The value of $Cor(\mathbf{\hat\beta_1,\hat\beta_3})$ is close to negative 1. There is a strong negative relationship between $\hatβ_1$ and $\hatβ_3$.

(f) Without computing variance for all the estimators, explain which estimator is the least consistent.

$C_{11}=11.423$ has the lagest value. $\hatβ_0$ has the the lagest variance and the least consistent among the estimators.

(g) Without computing covariances for all the pairs of estiamtors, list the pair(s) of estimators that are negatively correlated. Provide a reason.

According to the $(X'X)^{(-1)}$,

$C_{12},\ C_{13},\ C_{15},\ C_{23},\ C_{24},\ C_{45}$ are negative. 

The negatively correlated pairs of parameters are

$\hatβ_0$ and $\hatβ_1$, $\hatβ_0$ and $\hatβ_2$, $\hatβ_0$ and $\hatβ_4$, $\hatβ_1$ and $\hatβ_2$, $\hatβ_1$ and $\hatβ_3$, $\hatβ_3$ and $\hatβ_4$.

### Problem 4: 

a. the fitted model

$$\hat y=-1.808372+0.003598x_2+0.193960x_7-0.004816x_8$$

b. ANOVA

Analysis of Variance Table;
Response: y

|---|Df| Sum Sq| Mean Sq |Fvalue| Pr(>F)    
|---|--|-------| --------| ---- | ---
x2 |1 | 76.193|  76.193 |26.172|3.100e-05 ***
x7 |1 |139.501| 139.501 |47.918|3.698e-07 ***
x8 |1 | 41.400|  41.400 |14.221|0.0009378 ***
Residuals |24 | 69.870  |2.911  

According to the ANOVA table, the P-values is much smaller than 0.05. The regression is significant on 95% confident level.

c. The t test.

|---| Estimate |Std Error|t value|Pr($>|t|$)    
|---|----------| --------| ----- |---
x2 |0.003598  |0.000695 | 5.177 |2.66e-05
x7 |0.193960  |0.088233 | 2.198 |0.037815
x8 |-0.004816 |0.001277 | -3.771|0.000938

Each parameter is significant differnct with zero on 95% confident level. x2 has weak positive affect on response variable. x7 has strong positive affect but not as significant as other variables. x8 has weak negative affect.

d. R-square and adjusted R-square

$$R^2=0.7863,\quad  R_{adjusted}^2=0.7596$$

e. The partial F test statistic

$$F_0=\frac{(SSE_{x_7reduced}-SSE_{full})/r}{MSE}=\frac{(83.938-69.870)/1}{2.911}=4.832704$$

This partial F satistic is the square of the t sattistic for x7 in full modle.

3.2 The correlation

$$Cor(y_i,\hat y_i)=\frac{Cov(y_i,\hat y_i)}{\sqrt{Var(y_i)Var(\hat y_i)}}
=\sqrt{\frac{Var(y_i,\hat y_i)}{Var(y_i)Var(\hat y_i)}}=\sqrt{\frac{\sum(y_i-\hat y_i)^2}{\sum(y_i)^2\sum(\hat y_i)^2}}=\sqrt{0.7863}=0.8867395$$

$$R^2=1-\frac{SSE}{SST}=1-\frac{69.870}{326.964}=0.7863$$

Therefore, square of simple corelation coefficient between $y_i$ and $\hat y_i$ equal R-squared.

3.3 The confidence intervals

a. The 95% CI on $\beta_7$ is

$$\hat\beta_7\pm t_{\frac{\alpha}2,24}se(\hat\beta_7)=(0.011855322,\ 0.376065098)$$

b. For x2=2300, x7=56.0, x8=2100, the 95% CI on the mean numbers of games won by a team is

$$\hat y\pm t_{\frac{\alpha}2,24}se(\hat y)=(6.436203,\ 7.996645)$$

```{R, collapse=TRUE}
# Problem 3
B<- data.frame(
   b0=c(11.423,-4.349,-0.790, 0.486,-0.196),
   b1=c(-4.349, 4.624,-3.057,-0.346, 0.024),
   b2=c(-0.790,-3.057, 3.978, 0.135, 0.063),
   b3=c( 0.486,-0.346, 0.135, 0.031,-0.005),
   b4=c(-0.196, 0.024, 0.063,-0.005, 0.011))
# Var($\hat b_1$)
0.513*B[2,2]
# se(b1)
sqrt(0.513*B[2,2])
# se(b3)
sqrt(0.513*B[4,4])
# cov(b1,b3)
0.513*B[2,4]
# cor(b1,b3)
(0.513*B[2,4])/((sqrt(0.513*B[2,2]))*(sqrt(0.513*B[4,4])))
```

```{r, echo=TRUE, message = FALSE, collapse=TRUE}
# Problem 4
library(tidyverse)
library(broom)
# Import Data
table_b1 <- read_table2("TableB.1.txt")
# Observe the data frame
head(table_b1)
# build the full model
model_2_7_8 <- lm(y ~ x2+x7+x8, data=table_b1)
# build the reduced model on x7
model_2_8 <- lm(y ~ x2+x8, data=table_b1)

# 3.1a/c/d t statistics for each hypotheses.
model_2_7_8%>% summary()
# 3.1b ANOVA for full model
anova(model_2_7_8)
# 3.1e ANOVA for reduced model on x7
anova(model_2_8)
# calculate partial F
anova(model_2_7_8,model_2_8)
(83.938-69.870)/1/2.911

# 3.2 calculate cor(yi,\hat yi)
augment(model_2_7_8)
cor(select(augment(model_2_7_8),y,.fitted))
# another way
cor(table_b1[,1],predict(model_2_7_8, table_b1[,c(3,8,9)]))
# calculate r
sqrt(0.7863)

# 3.3a calculate CI on b7
confint(model_2_7_8, level = 0.95)
# 3.3b calculate CI on the mean response variable
model_2_7_8 %>% predict(newdata = data.frame(x2=2300,x7=56.0,x8=2100), interval = "confidence", level=0.95)
```


```{r, eval=FALSE, include = FALSE, collapse=TRUE}

visualizng
library(GGally)
ggpairs(data=table_b1[c(1,3,8,9)])

library(ggfortify)
model_2_7_8 %>% autoplot()

texreg::screenreg(l=list(model_2_7_8))
huxtable::huxreg(model_2_7_8, statistics = NULL)

cor(table_b1)
vcov(model_2_7_8)

car::lht(model_2_7_8, "x2 = x7")

library(olsrr)
ols_best_subset(model_2_7_8)

```