---
title: "STAT564"
subtitle: "Linear Regression"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document: 
    toc: false
    toc_float: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# {.tabset .tabset-fade .tabset-pills}

## Model 
### From Observation

Sample mean|Corrected sum of squares|Sample Variance|Standard diviation
|---|---|---|---
$\bar x=\frac{\sum_{i=1}^n(x_i)}n$|$S_{xx}=\sum_{i=1}^n(x_i-\bar x)^2$|$Var(x)=S_x^2=\frac{S_{xx}}{n-1}$ | $S_x$  
$\bar y=\frac{\sum_{i=1}^n(y_i)}n$|$S_{yy}=\sum_{i=1}^n(y_i-\bar y)^2$|$Var(y)=S_y^2=\frac{S_{yy}}{n-1}$ | $S_y$  
              -                   |$S_{xy}=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)$|$Cov(x,y)=\frac{S_{xy}}{n}$  

 - Sample means

$\sum_{i=1}^n(y_i)=\sum_{i=1}^n\hat y=n\bar y$

 - Sample Variance and Standard Deviation

$Var(x)=S_x^2=\frac{\sum_{i=1}^n(x_i-\bar x)^2}{n-1}=\frac{S_{xx}}{n-1}$

$Var(y)=S_y^2=\frac{\sum_{i=1}^n(y_i-\bar y)^2}{n-1}=\frac{S_{yy}}{n-1}$

 - Corrected sum of squares

$S_{xx}=\sum_{i=1}^nx_i^2-\frac{(\sum_{i=1}^nx_i)^2}n=\sum_{i=1}^nx_i^2-n\bar x^2=\sum_{i=1}^nx_i(x_i-\bar x)=\sum_{i=1}^n(x_i-\bar x)^2$

$S_{yy}=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ny_i^2-n\bar y^2=\sum_{i=1}^n(y_i-\hat y_i)^2+\sum_{i=1}^n(\hat y_i-\bar y)^2$

$S_{xy}=\sum_{i=1}^nx_iy_i-(\sum_{i=1}^nx_i\sum_{i=1}^ny_i)/n=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)=\sum_{i=1}^ny_i(x_i-\bar x)=\sum_{i=1}^nx_i(y_i-\bar y)$

$c_i=\frac{x_i-\bar x}{S_{xx}};\quad \sum_{i=1}^nc_i=0;\quad \sum_{i=1}^nc_i^2=\frac{\sum_{i=1}^n(x_i-\bar x)^2}{S_{xx}^2}=\frac1{S_{xx}};\quad \sum_{i=1}^nc_ix_i=1$

### Build Model

$\text{response variable}=\text{parameters(coefficients)}\times\text{predictor}+\text{random error}$

 - Assumptions

  [1] Linear relationship between x,y;  
  [2] $E(\varepsilon)=0$   
  [3] $Var(\varepsilon)=\sigma^2$, Homoscedasticity  
  [4] $Cov(\varepsilon_i,\varepsilon_j)=0 for i\ne{j}$  
  [5] $\varepsilon_i\sim^{iid}N(0,\sigma^2\mathbf I)$  

 - Simple Regression
$$y_i=\beta_0+\beta_1x_i+\varepsilon_i,\ i=1,2..n$$
 - Multiple Regression
$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+..+\beta_kx_{ik}+\varepsilon_i=\beta_0+\sum_{\beta_j}x_{ij}+\varepsilon_i$$
$$\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots  \\ y_n 
 \end{bmatrix}_{n\times1} = 
 \begin{bmatrix}
  1 & x_{1,1} & x_{1,2} & \cdots & x_{1,k} \\
  1 & x_{2,1} & x_{2,2} & \cdots & x_{2,k} \\
  \vdots & \vdots  & \vdots  & \ddots & \vdots  \\
  1 & x_{n,1} & x_{n,2} & \cdots & x_{n,k} 
 \end{bmatrix}_{n\times(k+1)}
 \begin{bmatrix}
  \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots  \\ \beta_k 
 \end{bmatrix}_{(k+1)\times1}+
 \begin{bmatrix}
  \varepsilon_1 \\ \varepsilon_2 \\ \vdots  \\ \varepsilon_n 
 \end{bmatrix}_{n\times1}$$
$$\mathbf Y=\mathbf X \mathbf\beta+\mathbf\varepsilon, i=1,2,..n$$

where 0 is a $n\times1$ vector of zero and $\mathbf{I}$ is a $n\times n$ identity matrix.


## Estimation 

### Least-Suqares

Gauss Markov Theorem: The least-Squared is unbiasd

`Assumption 2`

$S(\beta_0,\beta_1)=\sum_{i=1}^n\varepsilon_i=\sum_{i=1}^n(y_i-\hat y)=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2\quad(SSE)$

$$\left.\frac{\partial S}{\partial\beta_0}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-1)=0$$
$$\left.\frac{\partial S}{\partial\beta_1}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-x_i)=0$$

$$\frac{\partial{a'w}}{\partial{w}}=a;\quad \frac{\partial{w'Aw}}{\partial{w}}=2Aw$$

- Normal Equation

$$\sum_{i=1}^ny_i=n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i\implies \bar y=\hat\beta_0+\hat\beta_1\bar x$$

$$\sum_{i=1}^nx_iy_i=\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2;\quad \mathbf{X'Y=X'Xβ}$$

#### Expected estimators

$$\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^n(x_i-\bar x)y_i}{S_{xx}}=\sum_{i=1}^nc_iy_i=(X'X)^{-1}X'Y$$
$$\hat{\beta_0}=\bar y-\hat{\beta_1}\bar x=\frac1n{\sum_{i=1}^ny_i}-\bar x(\sum_{i=1}^n c_iy_i)=\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i$$

#### Fitted model

$$\hat{y_i}=\hat\beta_0+\hat\beta_1x_i;\quad \mathbf{\widehat Y=X\widehatβ=X(X'X)^{-1}X'Y=HY}$$

'There seems to be a decreasing (negative) medium approximately linear relationship.'
'The average y increases by $\beta_1$ units as the x increases by 1 unit.'

- Standardized 

$$\hat{w}=\frac{\hat{y-\hat{y}}}{S_y}=rZ=r\frac{\hat{x-\hat{x}}}{S_x}$$
$$\mathbf{e=Y-\widehat Y=Y-X\widehatβ=(I-H)Y}$$

#### Unbiased estimators of regression coefficients

$$E(\hat{\beta_1})=E[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_iE[\beta_0+\beta_1x_i+\varepsilon_i]=\beta_0\frac{\sum_{i-1}^n(x_i-\bar x)}{S_{xx}}+{\beta_1}\frac{\sum_{i-1}^n(x_i-\bar x)x_i}{S_{xx}}+0=\beta_1$$

$$E(\hat{\beta_0})=E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\bar xE[\hat\beta_1]=E[\frac{\sum_{i-1}^ny_i}n]-\bar x\hat\beta_1=\frac1n\sum_{i=1}^nE[\beta_0+\beta_1x_i+\varepsilon_i]-\bar x\hat\beta_1=\frac{n\beta_0}n+\frac{\beta_1}n\sum_{i=1}^nx_i+0-\bar x\hat\beta_1=\beta_0$$
$$E(\hatβ)=E((X'X)^{-1}X'Y)=(X'X)^{-1}X'E(Y)=(X'X)^{-1}X'Xβ=β$$

$$\mathbf{E(Y)=\mu1,\ E(a'Y)=a'E(Y)\ E(AY)=AE(Y)}$$

#### Variance of Parameters (Coefficients)

As $n\rightarrow \infty,Var[\hat\beta_{0/1}]\rightarrow 0$;

$\hat\beta_{0/1}$ are consistant estimators for $\beta_{0/1}$;

$Var[ax,by]=a^2Var(x)+b^2Var(y)+2Cov(x,y)$
 
$$Var(\hat{\beta_1})=Var[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_i^2Var[y_i]=\frac{\sigma^2}{S_{xx}}=\frac{\sigma^2}{(n-1)S_x^2}$$

$$Var(\hat{\beta_0})=Var[\bar y-\hat\beta_1\bar x]=Var[\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i]=\sum_{i=1}^n(\frac1n-\bar{x}c_i)^2Var[y_i]$$
$$=\sigma^2\Big[\sum_{i=1}^n{\frac1{n^2}}-\frac{2\bar x}{n}\sum_{i=1}^nc_i+\bar x^2\sum_{i=1}^nc_i^2\Big]=\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})=\sigma^2(\frac1n+\frac{\bar x^2}{(n-1)S_x^2})$$

$$E[\hat\beta_1^2]=Var(\hat\beta_1)-E[\hat\beta_1]^2=\frac{\sigma^2}{S_xx}+\beta_1^2$$

#### Covariance

 item|x| y |$\varepsilon$| x,y |$\hat\beta_1$|$\hat\beta_0$
|---|---|---|---|---|---|---|---
Var()|$\sigma_x^2$|$\sigma^2$|$\sigma^2$|$MSE\cdot C_{11}$|$\frac{\sigma^2}{S_{xx}}$|$\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})$|
Cov()|$\sigma_x^2$|   $0$    |   $0$    | $\sigma_{xy}$|$\sigma^2(X'X)^{-1}$| 
Cor()|            |          |          | $r$ |

$Cov(\hat\beta_0, \hatβ_1)=−\overline x\sigma^2 S_{xx}$

$Cov(x,a)=0; Cov(\bar y,\hat\beta_1) = 0$

$$Cov(a'Y)=a'Cov(Y)a,\ Cov(AY)=A'Cov(Y)A$$

$$Cov(\hatβ)=\sigma^2(X'X)^{-1},\ Cov(\hatβ_{i},\hatβ_{j})=\sigma^2C_{ij},\ Var(\hatβ_{i-1})=\sigma^2C_{ii}$$

$Cov(x,x)=Var(x)\equiv\sigma^2(x)\equiv\sigma_x^2$

$$Cov(x,y)=\sigma_{xy}=E[(x-\bar x)(y-\bar y)]=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}n=\frac{S_{xy}}n$$

$Cov(x,y)=Cov(y,x); Cov(ax,by)=abCov(x,y);Cov(x+a,y+b)=Cov(x,y)$

$Cov(a_1x_1+a_2x_2, b_1y_1+b_2y_2)=a_1b_1Cov(x_1,y_1)+a_1b_2Cov(x_1,y_2)+a_2b_1Cov(x_2,y_1)+a_2b_2Cov(x_2,y_2)$

$Cov(\mathbf{a'Y})=\mathbf{a'}Cov(\mathbf{Y})\mathbf{a}; Cov(\mathbf{AY})=\mathbf{A}Cov(\mathbf{Y})\mathbf{A'}$

- Correlation Coefficient

$$|r|=\sqrt{R^2}=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{Cov(x,y)}{\sqrt{Var{x}Var{y}}}=\frac{Cov(x,y)}{S_xS_y}=\frac{Cov(\hat\beta_0,\hat\beta_1)}{se(\hat\beta_0)se(\hat\beta_1)}$$


### Estimation of Error Variance $\sigma^2$

Suppose we fit the least squares regression line which gives the fitted value $\hat y_i$. Then
$$y_i-\bar y=(y_i−\hat y_i)+(\hat y_i−y_i) \implies (y_i-\bar y)^2=(y_i−\hat y_i)^2+(\hat y_i−y_i)^2$$

Name||SS|Simple|Multi|df|MS=SS/df|
|---|---|---|---|---|---|---
Total Sum of Squares    |SST|$\sum_{i=1}^n(y_i-\bar y)^2$     | $S_{yy}$ |$(Y-\bar{y}\mathbf{1})'(Y-\bar{y}\mathbf{1})$ |n-1| 
Explained sum of squares|SSR|$\sum_{i=1}^n(y_i-\hat y_i)^2$   | $\hat\beta_1S_{xy}=\hat\beta_1^2S_{xx}$ | $\hatβX'Y-n\bar{y}^2$ |k| MSR
Residual sum of squares |SSE|$\sum_{i=1}^n(\hat y_i-\bar y)^2$| $S_{yy}-\hat\beta_1S_{xy}$| $Y'Y-\hatβ'X'Y=Y'(I-H)Y$ |n-(k+1)| MSE 

$SST=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n[(\beta_0+\beta_1x_i+\varepsilon_i)-\frac1n\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon_i)]^2$
$=\sum_{i=1}^n[\beta_1(x_i-\bar x)+(\varepsilon_i-\bar\varepsilon_i)]^2=\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2+2\beta_1\sum_{i=1}^n(x_i-\bar x)(\varepsilon_i-\bar\varepsilon_i)+\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)^2$

$SSR=\sum_{i=1}^n(\hat y_i-\bar y)^2=\sum_{i=1}^n(\hat\beta_0+\hat\beta_1x_i-\bar y)^2$
$=\sum_{i=1}^n(\bar y-\bar x\hat\beta_1+\hat\beta_1x_i-\bar y)^2=\hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2=\hat\beta_1^2S_{xx}=\hat\beta_1S_{xy}$


$SSE=\sum_{i=1}^n(y_i-\hat y)=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}^n(y_i-(\bar y-\hat{\beta_1}\bar x)-\hat\beta_1x_i)^2=\sum_{i=1}^n[(y_i-\bar y)-\hat{\beta_1}(x_i-\bar x)]^2$
$=\sum_{i=1}^n(y_i-\bar y)^2-2\hat{\beta_1}\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)+\sum_{i=1}^n(x_i-\bar x)^2=S_{yy}-2\hat\beta_1^2S_{xy}+\hat\beta_1^2S_{xx}=\mathbf{S_{yy}-\hat\beta_1^2S_{xx}}=S_{yy}-\hat\beta_1S_{xy}$

$$=(Y-\hat Y)'(Y-\hat Y)=(Y-Xβ)'(Y-Xβ)=YY'-2Y'Xβ+β'X'Xβ=\mathbf{Y'(I-H)Y=Y'Y-\hatβ'X'Y}$$

$\text{symmetric}\quad H'=H;\quad (I-H)'=I-H$

$\text{idempotent}\quad HH=H;\quad (I-H)(I-H)=I-H$

Similar to partitioning total variation in the response, the degrees of freedom also can be partition. The degrees of freedom (df) indicates the amount of information (number of data values) required to know if some other information is known.

For example, the df of SST gives the number of data values need to know if the mean of the data is known. and the mean square explains the average variation in each (total, regression or error) after taking sample size ( ) and number of regression parameters into account.

 item |Simple|Multi
|---|---|---
E(SST)|$\beta_1^2S_{xx}+(n-1)\sigma^2$| $ $ 
E(SSR)|$\sigma^2+\beta_1^2S_{xx}$ | $k\sigma^2+β^{*'}X'_cX_cβ^{*}$ 
E(SSE)|$(n-2)\sigma^2$                | $ $


`Assumption 3``Assumption 4`

$For\ {i}\ne{j}\quad Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0$

$\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)=S_{\varepsilon_i\varepsilon_i}=(n-1)S_\varepsilon^2;\quad E(S_{\varepsilon\varepsilon})=E(S_{\varepsilon}^2)=\sigma^2$



$$E[SST]=\beta_1^2S_{xx}+2\beta_1\sum_{i=1}^n(x_i-\bar x)(E[\varepsilon_i]-E[\bar\varepsilon_i])+E[(n-1)S_\varepsilon^2]=\beta_1^2S_{xx}+(n-1)\sigma^2$$


$$E[SSR]=\sigma^2+\beta_1^2S_{xx}=k\sigma^2+β^{*'}X'_cX_cβ^{*}$$

$β^{*'}=[β_1 β_2..β_k]_{1\times k}$

$$X_c=\begin{bmatrix}
  x_{11}-\bar{x} & x_{12}-\bar{x} & \cdots & x_{1k}-\bar{x} \\
  x_{21}-\bar{x} & x_{22}-\bar{x} & \cdots & x_{2k}-\bar{x} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n1}-\bar{x} & x_{n2}-\bar{x} & \cdots & x_{nk}-\bar{x} 
 \end{bmatrix}_{n\times k}$$



$$E[SSE]=E[\mathbf{S_{yy}}]-S_{xx}E[\hat\beta_1^2]=(n-2)\sigma^2$$

$$E[MSE]=\sigma^2=Var(y_i)=Var(\varepsilon)$$
MSE is an unbiased estimator for error variance

$$\hat\sigma^2=MSE=\frac{(Y-Xβ)'(Y-Xβ)}{dfE}=\frac{Y'(I-H)Y}{rank(I-H)}$$

$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$
$\mathbf{Var(Y)=\sigma^2,\ Cov(Y)=\sigma^2I}$

$$\sum_{i=1}^n\hat ye_i=\sum_{i=1}^nx_ie_i=0$$



#### 2.6 Coefficient of Determination

If the fitted model is statistically significant, we expect

$$R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$$

 Proportion of variation in response(y) explained by the fitted model with predictor(s) $0\ge R^2\ge1$

-The coefficient of variation and correlation coefficient are related

$$\text{Adjusted}\ R^2=1-\frac{MSE}{MST}=1-\frac{SSE/{[n-(k-1)]}}{SST/(n-1)}$$

-Unfortunately, adding predictor(s) to the model does not decrease $R^2$. Therefore, it cannot be used to compare models with different number of predictors.


### Special Cases

 item |intercept known| through origin | slope known
|---|---|---|---
model$y_i$ |$\beta_0+\hat\beta_1x_i+\varepsilon_i$|$\hat\beta_1x_i+\varepsilon_i$|$\hat\beta_0+\beta_1x_i+\varepsilon_i$
$\hat\beta$|$\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}$|$\frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}$|$\bar y-\beta_1\bar x$
$E()$      |$\beta_1$ | $\beta_1$| $\beta_0$ 
$Var()$    |$\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$| $\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$|$\frac{\sigma^2}{n}$ 
$se()$     |$\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$| $\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$|$\sqrt{\frac{\sigma^2}n}$ 

- intercept known

$Let SSE=\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)^2$;

$\left.\frac{\partial SSE}{\partial\beta_1}\right|_{\hat\beta_1}=-2\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)x_i=0$;

$\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^n(y_i-\beta_0)x_i$

$E(\hat\beta_1)=E\left[\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right]=\frac1{\sum_{i=1}^nx_i^2}E\left[\sum_{i=1}^nx_i(\beta_1x_i+\varepsilon)\right]=\frac{\beta_1\sum_{i=1}^nx_i^2}{\sum_{i=1}^nx_i^2}-\frac{\sum_{i=1}^nx_i}{\sum_{i=1}^nx_i^2}E[\varepsilon]=\beta_1$ is unbiased

Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ 

Therefore $Var(\hat\beta_1)=Var\left(\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i-\sum_{i=1}^n\beta_0x_i)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i)=\frac{\sum_{i=1}^nx_i^2}{(\sum_{i=1}^nx_i^2)^2}Var(y_i)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$

- slope known

$Let SSE=\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)^2$

$\left.\frac{\partial SSE}{\partial\beta_0}\right|_{\hat\beta_0}=-2\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)=0$

$n\hat\beta_0=\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i$

$E(\hat\beta_0)=E\left[\frac1n\sum_{i=1}^ny_i-\frac1n\beta_1\sum_{i=1}^nx_i\right]=\frac1nE\left[\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon)-\beta_1\sum_{i=1}^nx_i\right]=\frac1n\sum_{i=1}^nE(\beta_0+\beta_1x_i)+E(\varepsilon)-\beta_1\sum_{i=1}^nx_i=\beta_0$  is unbiased

Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ 

Therefore $Var(\hat\beta_0)=\frac1{n^2}Var\left(\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i\right)=\frac1{n^2}\sum_{i=1}^nVar(y_i)=\frac{\sigma^2}n$

- 2.10 Regression through the origin

There are situations where we know that intercept of the simple linear regression model is zero. Then we fit the model with intercept,$\beta_0=0$ so the line goes through origin (0,0). 

Find the least squares estimator of slope ($\beta_1$) for the simple linear regression model through origin.

$\hat{\beta_1}\pm t_\frac\alpha2se(\hat{\beta_1})$, where $t_\frac\alpha2$ is critical value from t distribution with df=n-1




## Test and inference 

### Normal Distribution

`Assumption 5`

on the Slope and Intercept

 item |$\sim N(\mu,\sigma^2)$| $Z\sim N(0,1)$
|---|---|---
$\hat\beta_0$ |$N\left(\beta_0,\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})\right)$|$\frac{\hat\beta_0-\beta_0}{\sqrt{\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}}$
$\hat\beta_1$ |$N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)$|$\frac{\hat\beta_1-\beta_1}{\sqrt{\frac{\sigma^2}{S_{xx}}}}$
$\mathbf{\hat{β}}$| $N\left(β,\sigma^2(X'X)^{-1}\right)$
$\hat{y}$       | $N\left(\hat\beta_0+\hat\beta_1x,\sigma^2(\frac1n+\frac{(x-\bar x)^2}{S_{xx}})\right)$
$\hat{y_0}|x_0$ | $N\left(\hat\beta_0+\hat\beta_1x_0,\sigma^2(\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})\right)$


### T Distribution

Typically, $σ^2$ is unknown. MSE is an unbiased estimator of $\sigma^2$.

$$\frac{\hat\beta_j-\beta_j}{se(\hat\beta_j)}\sim t_{df=dfE=n-(k+1)}$$

$$\frac{\hat y_0|x_0-(\hat\beta_0+\hat\beta_1x_0)}{se(\hat y_0|x_0)}=\frac{\hat y_0-\bar y_0}{se(\hat y_0)}\sim t_{df=n-2}$$

| item |$\hat\beta_0$| $\hat\beta_1$ | $\hatβ_j$ | Predict$\hat y_0$ | Multi$\hat y_0$ |
|---|---|---|---|---|---|
|se |$\sqrt{\hat\sigma^2 (\frac1n+\frac{\bar x^2}{S_{xx}})}$ | $\sqrt{\frac{\hat\sigma^2}{S_{xx}}}$| $\sqrt{C_{jj}MSE}$ | $\sqrt{MSE\left[\frac1k+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$ | $\sqrt{\hat\sigma^2(1+\mathbf{x_0'(X'X)^{-1}x_0})}$ |
|T Test| $\frac{\hat\beta_0}{se(\hat\beta_0)}$ | $\frac{\hat\beta_1}{se(\hat\beta_1)}$ | $\frac{\hat\beta_j-c}{se(\beta_j)}$ | $\frac{\hat y_0-\bar y_0}{se(\hat y_0)}$ |  |
|CI | $\hat\beta_0\pm t_{\alpha/2}se(\hat\beta_0)$ | $\hat\beta_1\pm t_{\alpha/2}se(\hat\beta_1)$ | $\mathbf{\hat\beta}\pm t_{\alpha/2}se(\mathbf{\hat\beta})$ | $\hat y_0\pm t_{\alpha/2,n-2}se(\hat y_0)$  | $\hat y_0\pm t_{\alpha/2,n-(k+1)} se(\hat y_0)$ |


#### Standard Error of Parameters

$se(\hat\beta_1)=\sqrt{Var(\hat\beta_1)}=\sqrt \frac{{\hat\sigma}^2}{\sum(x_i-\bar{x})^2}=\sqrt{\frac{MSE}{S{xx}}}=\frac{\sqrt{n}\hat{\sigma}}{\sqrt{n-2} \sqrt{\sum (x_i-\bar{x})^2}}$

$se(\hat{\beta_0})=\sqrt{Var(\hat\beta_0)}=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=\sqrt{\frac{\hat\sigma^2}{n-2}}=\sqrt{\frac{MSE}{n}}$

$se(\hatβ_j)=\sqrt{C_{jj}\hat\sigma^2}=\sqrt{C_{jj}MSE}$

#### Hypothesis Test

- Testing Significance of Regression

$$H_0:\beta_1=0;\quad H_A:\beta_1\ne0$$

There is strong (not enoygh ) evidence that the true slope of fitted model is significantly different from zero at 5% significance level because p value is much smaller than 0.05;

$$H_0:\beta_1=c;\quad H_A:\beta_1\ne c$$

Decision: Since the p-value$>(<)$significance level ($\alpha$=0.05), there is no enough evidence to conclude that the true slope is different from c (reject $H_0$ and conclude $H_1$ is true).

#### Confidence Interval (N~T)

 $100(1-\alpha)%$ Confidence Interval for the true slope $(\beta_1)$:

 $100(1-\alpha)%$ Confidence Interval for the true intercept $(\beta_0)$:

Since zero is (not) inside this interval, true intercept can (not) be zero with $1-\alpha$ confidence.

The 95% confidence interval (does not) contain zero. Therefore, we are 95% confidence that the true intercept is different from (can be) zero;

- 3.4.3 simultaneous confidence region

We will need to estimate several confidence intervals for a set of regression coefficients **simultaneously** with 100(1−a)% confidence level. These intervals are called **simultaneous** or **joint confidence intervals**.

There is a problem of computing confidence interval for each parameter with the same confidence level and use those interval for simultaneous or joint confidence interval with the same confidence level.

Hence, a 100(1−a)% joint confidence region is defined for all the regression coefficients in the model as

$$\frac{\mathbf{(\hatβ-β)'(X'X)(\hatβ-β)}}{dfE\times MSE}=\frac{\mathbf{(\hatβ-β)'(X'X)(\hatβ-β)}}{[n-(k+1)] MSE}\le F_a$$

This inequality describes an elliptically shaped region.

Apply the joint or simultaneous confidence region to coefficients of Simple Linear Regression:

$$A(x-h)^2+B(x-h)(y-k)+C(y-k)^2=1$$


- Bonferroni Method

For each regression coefficient, $\hatβ\pm t_{\frac{a}{2(k+1)}}se(β)$, where $\frac{a}{2(k+1)}$ is the critical value from the t distribution with df=n-(k+1)

There is at least 100(1−a)% confidence about the Bonferroni intervals.

- Scheffé method

For each regression coefficient, $\hatβ\pm\sqrt{2F_a}se(β)$, where $F_a$ is the critical value from the F distribution with df1=k+1 and df2=n-(k+1)

#### Prediction (N~T)

- 2.4.2 Interval Estimation of the Mean Response & 2.5 Prediction of New Observations

One of the goals in regression analysis is to predict the response (y) for a given value/s of predictor/s (x).  
Assume there are n pairs of observatin $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ to fit least squares simple linear regression model.    
Let’s say we observe k number of new values of response (y) at a new value of predictor (x) denoted by $x_0$ where $x_0$ between minimum and maximum observed values of predictor (x).  
Let $\bar y_0|x_0=\bar y_0$ be the true mean of k values of response at new value (x_0) of predictor.   
Then$\hat y_0|x_0=\hat y_0=\hat\beta_0+\hat\beta_1x_0$ is the estimated mean of k values of response at new value ($x_0$)) of predictor using the fitted least squares model.   
Therefore, $100(1−\alpha)%$ **confidence** interval for the **MEAN** response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

Therefore, $100(1−\alpha)%$ **prediction** interval for a **single value** of response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

 Where $t_{\alpha/2}$ is the critical value from t distribution with (n-2) degrees of freedom

 Consider the random variable $\hat y_0-\bar y_0$ and find its expected value and variance.

Be careful when predicting response for a given new value of predictor.

estimated mean (point estimate) of response **(UNITS)** $\hat y_0=\hat{\beta_0}+\hat{\beta_1}x_0$ True mean $\bar y_0$
k number of new values of response at a new value of predictor denoted by $x_0$ **without extapolation**.
$se(\hat y_0)=\sqrt{MSE\left[(1)+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$;
100(1−$\alpha$) % confidence interval for the MEAN (a single value of) response at a given new value $x_0$ of predictor is
$(\hat y_0) \hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}se(\hat y_0)$;
$E[\hat y_0-\bar y_0]=E[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=0$; 
$\hat y_0$ is unbiased estimator of $\bar y_0$;

$Var[\hat y_0-\bar y_0]=Var[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=Var[\bar y-\hat\beta_1\bar x+\hat{\beta_1}x_0-\frac1k\sum_{i=1}^k\varepsilon_i]$

$=Var[\frac{\sum_{i=1}^ny_i}n-\sum_{i=1}^nc_iy_i(\bar x-x_0)-\frac1k\sum_{i=1}^k\varepsilon_i]=Var\Big[\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]y_i\Big]+Var[\sum_{i=1}^k\frac1k\varepsilon_i]-2Cov[y_i,\varepsilon_i]$

Because $y_i$ and $\varepsilon_i$ are independent for new observations.
$=\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]^2Var(y_i)+\sum_{i=1}^k\frac1{k^2}Var(\varepsilon_i)=\sigma^2[\sum_{i=1}^n{\frac1{n^2}}-\frac{2\bar x}{n}\sum_{i=1}^nc_i+\bar x^2\sum_{i=1}^nc_i^2]=\sigma^2[\frac1n+\frac{(\bar x-x_0)^2}{S_{xx}}+\frac1k]$


- 3.5 Prediction of New Observations

If $\mathbf{X_0'}=[1,x_{01},x_{02},..,x_{0k}]$, then a point estimate of the future observation y0 at the point $x_{01},x_{02},..,x_{0k}$ is $\mathbf{\hat y_0=x_0'\hatβ}$

A 100(1−α) percent prediction interval for this future observation is

### Chi-squared Distribution

#### CI

Estimat $\sigma^2$ using MSE

Typically, $\sigma^2$ is unknown and is estimated using MSE (Mean Squared Error).

$$\frac{(n-2)\hat{\sigma}^2}{\sigma^2}=\dfrac{dfE\times MSE}{\sigma^2}=\dfrac{(n-2)MSE}{\sigma^2}=\dfrac{SSE}{\sigma^2}\sim \chi^2_{(n-2)}$$
$$\frac{[n-(k+1)]MSE}{\sigma^2}=\frac{SSE}{\sigma^2}=\frac{Y'(I-H)Y}{\sigma^2}\sim \chi^2_{df=rank(I-H)=n-(k+1)}$$

if $\beta_1=0$, then

$$\frac{dfR\times MSR}{\sigma^2}=\frac{SSR}{\sigma^2}\sim \chi^2_{(1)}$$

if $β^{*'}=0$, then

$$\frac{dfR\times MSR}{\sigma^2}=\frac{kMSR}{\sigma^2}=\dfrac{SSR}{\sigma^2}=\sim \chi^2_{df=k}$$

 $100(1-\alpha)%$ Confidence Interval for the true error variance $(\sigma^2)$:

$$\Bigg[\dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{\frac\alpha2}}, \dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{1-\frac\alpha2}}\Bigg ]$$



### F Distribution

#### Analysis of Variance

The F test statistic ($F_0$) is computed as shown in the ANOVA table for Testing Significance.

Source of Variation|Sum of Squars(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|P-value
|---|---|---|---|---|---
Model(Regresion)|$SSR=\hat\beta_1S{xy}; \hatβX'Y-n\bar{y}^2$|1;k|$MSR=\frac{SSR}k$|$\frac{MSR}{MSE}$|$P(F>F_0)$
Error(Residual)|$SSE=SST-SSR=Y'(I-H)Y$|n-(k+1)|$MSE=\frac{SSE}{n-(k+1)}$||
Total|$SST=\sum_{i=1}^n(y_i-\bar y)^2=(Y-\bar{y}\mathbf{1})'(Y-\bar{y}\mathbf{1})$|n-1|||

SSE and SSR are independent.Therefore, by definition of F distribution

$$\frac{SSR/dfR}{SSE/dfE}=\frac{SSR/k}{SSE/[n-(k+1)]}=\frac{MSR}{MSE}\sim F_{(k,[n-(k+1)])}$$

If the test statistic is much larger than 1 ($F_0>F(\alpha,1,n-2)$) (or p-value $\le$ significance level), then the true slope is different from zero and hence least squares line is statistically significant.

There is strong evidence that the fitted model is statistically significant at 5% significance level because p value is much smaller than 0.05

#### T and F

 If $T\sim t(df=\upsilon)$, then $T^2\sim F(1,\upsilon)$

Therefore, test statistic ($t_0$) for testing true slope ($\beta_1$) different from zero and F test statistic ($F_0$) in ANOVA for simple linear regression are related as

$$t_0^2=\left(\frac{\hat\beta_1}{se(\hat\beta_1)}\right)^2=\frac{MSR}{MSE}=F_0$$

The t test has more flexibility than F test because,  

 t test can be used when testing $H_0:\ \beta_1=c$ versus $H_1:\ \beta_1\ne c$ where c is NOT zero while F test can be used only when c=0

 t test can be used when it is a one-tailed test (such as $H_1:\ \beta_1>c$ or $H_1:\ \beta_1<c$ while F test can be used for only two-tailed test.
 
#### Testing the General Linear Hypothesis

$$SSE_{Full}=Y'Y-\hatβ'X'Y,\quad dfE_{Full}=n-(k+1)$$

$$H_0:\ Tβ=0;\quad H_1:\ Tβ\ne0$$
$$SSE_{Reduced}=\mathbf{Y'Y-\hatγ'Z'Y},\quad dfE_{Reduced}=n-(k+1-r)$$

$$\frac{(SSE_{Reduced}-SSE_{Full})/r}{SSE_{Full}/[n-(k+1)]}\sim F_{(r,[n-(k+1)])}$$

$dfE_{Reduced}-dfE_{Full}=n-(k+1-r)-[n-(k+1)]=r$

additional (extra) sum of squares F test (partial F test), $dfE_{Reduced}-dfE_{Full}=r=1$

$$\frac{SSE_{Reduced}-SSE_{Full}}{MSE_{Full}}\sim F_{(1,[n-(k+1)])}$$



#### Multicollinearity

Multicollinearity is the linear (or near linear) dependence of predictors of the multiple linear regression model.

That means, not all the columns of design matrix (X) are linearly independent and hence $(\mathbf{X'X})$ is not nonsingular resulting $(\mathbf{X'X})^{-1}$ does not exist.
The multicollinearity increases standard error of estimated regression coefficients and hence makes estimators are less precise.

**VIF (Variance Inflation Factor)** computed as $VIF=\frac1{1-R_j^2}$

where $R_j^2$ is the $R^2$ (coefficient of determination) of the fitted model using $j^{th}$redictor as the response variable and all the other predictors (excluding response) as predictors. 

If VIF >10, then there is a serious problem of multicollinearity and the model is not suitable to use.

- Procedure when multicollinearity exists:

Fit the full model (with all the predictors)
Find the predictor with highest VIF and fit a reduced model without that predictor
Check the VIF for the predictors in the new model and repeat the process until all the predictors used in the final reduced model have VIF ≤10

### Example

Consider the following data set, in which the variables of interest are x=conmmuting distance(miles) and y=commuting time (min).  


x|5|10|15|20|25 
|---|---|---|---|---|--- 
y|8|16|22|23|31 
$\hat y=4.1+1.06x$|9.4|14.7|20|25.3|30.6 
$e^2=(y-\hat y)^2$|1.96|1.69|4|5.29|0.16 


$$\hat\sigma^2=MSE=\frac{SSE}{n-2}=\frac{\sum_1^n(y-\hat y)^2}{n-2}=\frac{13.1}{5-2}=4.367 $$

(a) Compute the values of ANOVA table and test the significance of simple linear regression model for these data.

$$n=5,\quad \bar y=20,\quad \sum y_i^2=2294,\quad \hat\sigma^2=4.367,\quad S{xy}=265,\quad S{xx}=250,\quad \hat\beta_1=1.06$$


Source of Variation|Sum of Squars(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|P-value
|---|---|---|---|---|---
Model(Regresion)|$\hat\beta_1S{xy}=1.06\times265=280.9$|1|$\frac{280.9}1$|$\frac{280.9}{4.367}=64.328$|$P(F>64.328)\approx0.01$
Error(Residual)|SST-SSR=294-280.9=13.1|n-2=5-2=3|$\frac{13.1}3=4.3667$||
Total|$\sum(y_i-\bar y)^2=\sum{y_i^2}-n\bar y^2=2294-5\times20^2=294$|n-1=4|||

At 5% significance level, the fitted model is significant because P-value<0.05.

#### Ex 2 Contd Consider the following data set, in which the variables of interest are x and y.

 - [a] Test whether the true slope of the simple linear regression model is significantly different from zero.

$$H_0:\ \beta_1=0,\quad H_1: \beta_1\ne0$$

$$Test\ statistic: t_0=\frac{\hat\beta_1-0}{se(\hat\beta_1)}=\frac{\hat\beta_1}{se(\hat\beta_1)}=\frac{1.06}{0.1322}=8.02$$

$$se(\hat\beta_1)=\sqrt \frac{\hat\sigma^2}{S{xx}}=\sqrt{\frac{4.367}{250}}=0.1322$$

$$df=n-2=5-2=3$$

$$P-value=2\times P(t>|t_0|)2\times0.0025=0.005$$

  P-value<0.05, therefore, ture slope is different from zero.

 - [b] Estimate the 95% confidence interval for the true slope of the simple linear regression model.

$$\because\ df=n-2=5-2=3,\quad t_{\frac\alpha2}=t_{0.025}=3.182$$


$$\hat{\beta_1}\pm t_{\alpha/2}se(\hat{\beta_1})=1.06\pm3.182\times0.1322=(0.639,1.481)$$

 - [c] Estimate the 95% confidence interval for the true intercept of the simple linear regression model.

$$se(\hat{\beta_0})=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=4.367(\frac15+\frac{15^2}{250})=2.1917$$

$$\hat{\beta_0}\pm t_{\alpha/2}se(\hat{\beta_0})=4.1\pm3.182\times2.1917=(-2.874,11.074)$$

  Since zero is inside thsi interval, ture intercept ($\beta_0$) can be zero with 95% confidence.






## Adequacy Checking

Three of the major problem areas in least squares analysis relate to failures of the basic assumptions: normality, common variance, and independence of the errors. The process of checking the validity of the assumptions is an important step in every regression analysis.

Normality is needed only for tests of significance and construction of confidence interval estimates of the parameters. The t-test, F-test, and chi-square test require the underlying random variables to be normally distributed. The tests of significance and confidence intervals, however, are affected by nonnormality.

### 4.2 Residual Analysis

#### 4.2.1 Definition of Residuals

The $i^{th}$ observed residual is defined as $e_i=y_i-\hat y_i$ for $i=1,2..,n$
And the mean squared error (MSE) is an unbiased estimator of error variance $\sigma^2$.

$$\hat\sigma^2=MSE=\frac{(Y-Xβ)'(Y-Xβ)}{dfE}=\frac{Y'(I-H)Y}{rank(I-H)}$$

#### 4.2.2 Methods for Scaling Residuals

The plot of scaled residuals helps to recognize outliers or influential observations.

for $i=1,2..,n$ | Standardized Residuals | Studentized Residuals | PRESS Residuals
|---|---|---|---
the $i^{th}$ | $d_i$ | $r_i$ | $e_i$
$h_{ii}=\frac1n+\frac{(x_i-\bar x)^2}{S_{xx}}$ | $\frac{e_i}{\sqrt{MSE}}$ | $\frac{e_i}{\sqrt{MSE(1-h_{ii})}}$ | $\frac{e_i}{1-h_{ii}}$

- Standardized Residuals

The standardized residuals have zero mean and variance of about 1. Therefore, if $|d_i|>3$, then the corresponding observation is an outlier.

- Studentized Residuals (standardized PRESS residual)

where $h_{ii}$ is the $i^{th}$ diagonal element of hat ($\mathbf{H}$) matrix.

The observed residuals, however, are not independent and do not have common variance, even when the $\mathbf{\sigma^2I}$ assumption is valid. Under the usual least squares assumptions, $\mathbf{e=(I-H)Y}$ has a multivariate normal distribution with $E\mathbf{(e)=0}$ and $Cov\mathbf{(e)=(I-H)\sigma^2}$

> The diagonal elements of $Cov\mathbf{(e)}$ are not equal, so the observed residuals do not have common variance

> The off-diagonal elements of $Cov\mathbf{(e)}$ are not zero, so they are not independent.



The heterogeneous variances in the observed residuals are easily corrected by standardizing each residual. The variances of the residuals are estimated by the diagonal elements of $\mathbf{(I-H)\hat\sigma^2}$


$$Var(e_i)=\hat\sigma^2(1-h_{ij})=MSE(1-h_{ii})$$
$$Cov(e_i,e_j)=\hat\sigma^2(-h_{ij})=-MSE(h_{ij})$$

- PRESS Residuals

The standardized and studentized residuals are effective in detecting outliers.
If we delete the $i^{th}$ observation, fit the regression model to the remaining n-1 observations, and calculate the predicted value ($\hat y_i$) of $y_i$ corresponding to the deleted observation, the corresponding prediction error

$$PRESS=\sum_1^ne_{(i)}^2$$

Residuals associated with points for which ℎ𝑖𝑖 is large will have large PRESS residuals. These points will generally be high influence points. Generally, a large difference between the ordinary residual and the PRESS residual will indicate a point where the model fits the data well, but a model built without that point predicts poorly.

#### 4.2.3 Residual Plots

- Normal Probability Plot

Histogram or Boxplot of residuals

Normal Q-Q Plot

If the response variable represents counts, then it might be important to fit a generalized linear model with a binomial or Poisson distribution.

If the response variable represents continuous proportions (0 to 1), then it might be important to fit a generalized linear model with a beta distribution

- Plot of Residuals against the Fitted Values $\mathbf{\hat y_i}$

- Plot of Residuals against the Regressor or Predictor (for multiple regression models)

Plotting the residuals against the corresponding values of each predictor variable can also be helpful. If there is a pattern in this plot, either higher order term of the predictor is added to the model or a transformation is used to fix the problem.

Further, it is helpful to plot residuals against predictor variables that are not currently in the model but which could potentially be included. Any structure in the plot of residuals versus an omitted predictor indicates that incorporation of that predictor could improve the model.

Residual $e_{i}=y_i-\hat y_i$ for $i=1,2..,n$

There is a medium positive linear pattern. Therefore, it is important to add $x_2$ predictor in the model.

If there was a non-linear pattern, then it suggest adding a polynomial function of predictor.

If there is no pattern, then the predictor may not be significant.

- Plot of Residuals in Time Sequence

Data collected over time on individual observational units will often have **serially correlated residuals**. That is, the residual at one point in time depends to some degree on the previous residuals. Classical time series data, such as the data generated by the continuous monitoring of some process, are readily recognized as such and are expected to have **correlated residuals**.

Durbin–Watson test statistic

$$D=\frac{\sum_{i=2}^n(e_i-e_{i-1})^2}{\sum_{i=1}^ne_i^2}$$

Under the assumption of independent error, E(D)=2

A significant Durbin-Watson test statistic indicates a violation of independent assumption.

When there is a violation of independence, the structure of variance-covariance matrix of errors can be defined along with the model or time variable can be included as a predictor in the model.

#### 4.2.4 Partial Regression Plots

_To check independence of residuals._

Plot of residuals against the regressor or predictor may not show the importance of adding the specific predictor in to the model with all the other predictors.

Let $e_{(j)}$ denote the residuals from the regression of the response variable on all predictors except the $j^{th}$ predictor. Similarly, let $u_{(j)}$ denote the residuals from the regression of the $j^{th}$ predictor on all other predictors. The plot of $e_{(j)}$ versus $u_{(j)}$ is the partial regression plot for the $j^{th}$ predictor.

rediduals | fitted model |
|---|---
$e_{(j)}=y_i-\hat y_i$ for each i | $y=\beta_0+\beta_1x_1+\beta_3x_3+\varepsilon$
$u_{(j)}=x_2-\hat x_2$ for $i=1,2..,n$| $x_2=\alpha_0+\alpha_1x_1+\alpha_3x_3+\varepsilon$

The slope of the linear regression line in the partial regression plot is the regression coefficient for that predictor in the full model.

_If the plots have nonlinear pattern, then ploynomial function of predictor should be used._

Any curvilinear relationships not already taken into account in the model should be evident from the partial regression plots. The plot is useful for detecting outliers and high-leverage points and for showing how several leverage points might be interacting to influence the partial regression coefficients.

These plots may not be helpful if

- Other predictors are incorrectly used in the model
- Multicollinearity exists
- Interaction terms must be included in the model

_A leverage point has x,y values far away from most of the other points but it will be in the pattern of most of data point._

_It does not change model coefficients._


### 4.3 PRESS statistic

This statistic shows the predictive capability of the fitted regression model.

$$R_{Prediction}^2=1-\frac{PRESS}{SST}$$

One very important use of the PRESS statistic is in comparing regression models. Generally, a model with a small value of PRESS is preferable to one where PRESS is large.

### 4.4 Detection and Treatment of Outliers


### 4.5 Lack of Fit of the regression model

$$SSE=\sum_{i=1}^{n}(y_{i}−\hat y_i)^2=\sum_{i=1}^{m}\sum_{j=1}^{n_i}(y_{ij}−\hat y_i)^2$$
$$=\sum_{i=1}^{m}\sum_{j=1}^{n_i}(y_{ij}−\bar y_i)^2+\sum_{i=1}^{m}\sum_{j=1}^{n_i}(\bar y_i−\hat y_{i})^2=SS_{PE}+SS_{LOF}$$

$$dfE=df_{PE}+df_{LOF}=n-(k+1)=(n-m)+[m-(k+1)]=(n-m)+(m-2)=n-2$$

$$F_0=\frac{SS_{LOF}/df_{LOF}}{SS_{PE}/df_{LOF}}=\frac{SS_{LOF}/[m−(k+1)]}{SS_{PE}/(n−m)} $$
  
$$\hat\sigma^2=MS_{PE}=\frac{SS_{PE}}{df_{PE}}$$


## 5 Transformations and Weighting

### 5.2 Variance-stabilizing Transformations
P.172 TABLE 5.1 Useful Variance-Stabilizing Transformations

Relationship of $σ^2$ to $E(y)$ |Transfonnation
|---|---
$σ^2\propto constant$    |$y′=y $ (no transformation)
$σ^2\propto E(y)$        |$y′=\sqrt y $ (square root; Poisson data)
$σ^2\propto E(y)[1−E(y)]$|$y′=\sin^{−1}(\sqrt y)$ (arcsin; binomial proportions $0≤y_i≤1$)
$σ^2\propto [E(y)]^2$    |$y′=\ln(y)$ (log)
$σ^2\propto [E(y)]^3$    |$y′=y^{−1/2} $ (reciprocal square root)
$σ^2\propto [E(y)]^4$    |$y′=y^{−1} $ (reciprocal)

### 5.3 Transformations to linearize the model
P.177 Figure 5.4 Linearizable functions. (From Daniel and Wood [1980])

TABLE 5.4 Linearizable Functions and Corresponding Linear Form

Figure | Linearizable Function | Transformation | Linear Form
|---|---|---|---
5.4a,b |$y=β_0x^{β_1}$        |$y′=\log y,x′=\log x$  |$y′=\logβ_0+β_1x′$
5.4c,d |$y=β_0e^{β_1x}$       |$y′=\ln y$             |$y′=\lnβ_0+β_1x$
5.4e,f |$y=β_0+β_1\log x$     |$x′=\log x$            |$y′=β_0+β_1x′$
5.4g,h |$y=\frac{x}{β_0x−β_1}$|$y′=\frac1y,x′=\frac1x$|$y′=β_0−β_1x′$




### 5.4 Analytical Methods for Selecting a Transformation

#### 5.4.1 Box-Cox transformation for the response (y)

Goal: Find a transformation so that the resulting residuals have normal distribution.

Box and Cox suggested a power transformation of response ($y^\lambda$) where $\lambda$ is a parameter to be determined. This parameter and the regression coefficients can be estimated simultaneously so that they maximize the joint likelihood function of values in the response.

There is no analytical method to find $\lambda$. However, a numerical (computational) method can be used to find a value of $\lambda$ so that it minimizes the residual sum of squares (SSE) of the fitted model with $y^\lambda$ as the response.
In practice, find $\lambda$ using above method and use an appropriate value for $\lambda$ so that it is easier to interpret. For example, $\lambda=\frac12,-\frac12,2,-1, 0$ are easy to interpret. If $\lambda=0$, then use $\ln(y)$ for the response.