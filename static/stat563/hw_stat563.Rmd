---
title: ''
author: "STAT563 HW1 Shen Qu"
date: ""
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**7.12** Let $X_1,..,X_n$, be a random sample from a population with pmf
$P_{\theta}(X=x)=\theta^x(1-\theta)^{1-x}, x=0\ or\ 1, 0\le \theta\le\frac12$

(a) Find the method of moments estimator and MLE of $\theta$

For $X_i\sim iid Bernoulli(\theta), 0\le \theta\le\frac12$, We have $EX=\theta, VX=\theta(1-\theta)$

 - method of moments: 
 
Set $EX=\theta=\frac1n\sum_iX_i=\bar X \implies\hat\theta_{MOM}=\bar X$

 - MLE: 

$L(\theta)=\prod\theta^{x_i}(1-\theta)^{1-x_i}=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$

$l(\theta)=\sum_{i=1}^nx_i\ln\theta+(n-\sum x_i)\ln(1-\theta)$

$l'(\theta)=\frac1\theta\sum_{i=1}^nx_i+\frac1{1-\theta}(n-\sum x_i)\overset{\text{set}}{=}0$

$l''(\theta)<0$


From Example 7.2.7, $L(|x)$ is increasing for $\theta<\bar x$ and is decreasing for $\theta\ge\bar x$. 

For $0\le \theta\le\frac12$, when $\bar X\le1/2$, $\max{L(|\mathbf{x})}=\bar X$, $\hat\theta_{MLE}=\bar X$. 

When $\bar X>1/2$, $L(\theta|\mathbf{x})$ is an increasing function of $\theta$ on $[0, 1/2]$, $\max{L(|\mathbf{x})}=1/2$, $\hat\theta_{MLE}=1/2$. 

So $\hat\theta_{MLE}=\min\{\bar X,1/2\}$.

 ---
 
(b) Find the mean squared errors of each of the estimators.

$MSE(\hat\theta) = E(\hat\theta-\theta)^2$

Let $y=\sum^n_{i=1}X_i=n\bar X\sim Binomial(n,\theta)$, then $\begin{cases}y\le[n/2]&when\bar X\le1/2\\y\ge[n/2]+1&when\bar X>1/2\end{cases}$

$MSE(\hat\theta_{MOM}) = E(\bar X-\theta)^2=\sum^n_{y=0}(\frac{y}n-\theta)^2{n\choose y}\theta^y(1−\theta )^{n−y}$

$MSE(\hat\theta) = E(\hat\theta_{MLE}-\theta)^2=\sum^{[\frac{n}2]}_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y} +\sum^n_{y=[\frac{n}2]+1}(\frac12−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$

<!--Note: The MSE of $\tilde\theta$is $MSE(\tilde\theta) = Var\tilde\theta + bias(\tilde\theta)^2=((1−\theta)/n)+0^2=(1−\theta)/n$. There is no simple formula for $MSE(\theta)$. $[\frac{n}2]=\frac{n}2$, if n is even, and $[\frac{n}2]=\frac{n-1}2$, if $n$ is odd.-->

 ---
 
(c) Which estimator is preferred? Justify your choice

$MSE(\hat\theta_{MOM})−MSE(\hat\theta_{MLE})=\sum^n_{y=0}(\frac{y}n-\theta)^2{n\choose y}\theta^y(1−\theta )^{n−y}-\sum^{[\frac{n}2]}_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y} -\sum^n_{y=[\frac{n}2]+1}(\frac12−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$


$$=\sum^n_{y=[\frac{n}2]+1}\left[(\frac{y}n−\theta)^2-(\frac12−\theta)^2\right]\binom{n}{y}\theta^y(1−\theta)^{n−y}=\sum^n_{y=[\frac{n}2]+1}(\frac{y}n+\frac12−2\theta)(\frac{y}n-\frac12)\binom{n}{y}\theta^y(1−\theta)^{n−y}$$
 
For $y/n>1/2, y/n-1/2>0$. For $\theta\le1/2$, $y/n+1/2>1\ge2\theta$

Therefore, $MSE(\hat\theta_{MOM})−MSE(\hat\theta_{MLE})\ge0$, $MSE(\hat\theta_{MOM})\ge MSE(\hat\theta_{MLE})$. $MSE(\hat\theta_{MLE})$ is better.

 ---  

**7.48** Suppose that $X_i, i=1,..,n$, are iid $Bernoulli(p)$.

(a) Show that the variance of the MLE of $p$ attains the Cramer-Rae Lower Bound.

 -MLE of p

$L(p)=\prod p^{x_i}(1-p)^{1-x_i}=p^{\sum x_i}(1-p)^{n-\sum x_i}$

$l(p)=\sum_{i=1}^nx_i\ln p+(n-\sum x_i)\ln(1-p)$

$l'(p)=\frac1p\sum_{i=1}^nx_i+\frac1{1-p}(n-\sum x_i)\overset{\text{set}}{=}0$

$l''(p)<0$

$\hat p_{MLE}=\bar X$

$E[\hat p_{MLE}]=E[\bar X]=p$

$V[\hat p_{MLE}]=V[\bar X]=\frac{p(1-p)}{n}$

Let $w=\bar x$, then $k(p)=E[w]=p$, $f(x|p)=p^x(1-p)^{1-x}\sim Bern(p)$

$\ln f(x|p)=x\ln p+(1-x)\ln(1-p)$

$\frac\partial{\partial{p}}\ln f(x|p)=\frac{x}p-\frac{1-x}{1-p}$

$\frac{\partial^2}{\partial{p}}\ln f(x|p)=-\frac{x}{p^2}-\frac{1-x}{(1-p)^2}$

$I(p)=-E[\frac{\partial^2}{\partial{p}}\ln f(x|p)]=-E[-\frac{x}{p^2}-\frac{1-x}{(1-p)^2}]=\frac{1}{p(1-p)}$

By Cramer-Rao Inequality,

$$Var_p(W)\ge\frac{(\frac{\partial}{\partial p}E_p[W])^2}{nI(p)}=\frac{(k'(p))^2}{n\frac{1}{p(1-p)}}=\frac{p(1-p)}{n}$$

 ---

(b) For $n\ge4$, show that the product $X_1X_2X_3X_4$ is an unbiased estimator of $p^4$, and use this fact to find the best unbiased estimator of $p^4$.

$E(x_1x_2x_3x_4)=\prod_{i=1}^4 EX_i=p^4$ which is a unbiased estimator.

$T(\vec x)=\sum_{i=1}^n X_i\sim Bino(n,p)$ is a complete sufficient statistic.

By Rao-Blackwell and Lehmann-Scheffe Theorem, $E[x_1x_2x_3x_4|\sum_{i=1}^n X_i=t]$ is MUVE of $p^4$.

$E[x_1x_2x_3x_4|\sum_{i=1}^n X_i=t]=\sum_{x_1=0}^1\sum_{x_2=0}^1\sum_{x_3=0}^1\sum_{x_4=0}^1x_1x_2x_3x_4P(x_1x_2x_3x_4|\sum_{i=1}^n X_i=t)$

$=P(x_1=x_2=x_3=x_4=1|\sum_{i=1}^n X_i=t)=\frac{P(x_1=x_2=x_3=x_4=1,\sum_{i=1}^n X_i=t)}{P(\sum_{i=1}^n X_i=t)}$

$=\frac{P(x_1=x_2=x_3=x_4=1,\sum_{i=5}^n X_i=t-4)}{P(\sum_{i=1}^n X_i=t)}=\frac{p^4{n-4\choose t-4}p^{t-4}(1−p)^{n−t}}{{n\choose t}p^{t}(1−p)^{n−t}}=\frac{n-4\choose t-4}{n\choose t}$

 ---

**7.59** Let $X_1,..,X_n$ be iid $n(\mu, \sigma^2)$. Find the best unbiased estimator of $\sigma^2$, where $p$ is a known positive constant, not necessarily an integer.

 - Find the complete sufficient statistic for $\sigma^p$ called $T$

Let $T=(n-1)S^2/\sigma^2\sim \chi^2_{n-1}=Gamma(\frac{n-1}2,2)$, then 

 Form Theorem 6.2.25, in the exponential familiy, $T=(n-1)S^2/\sigma^2$ is a complete sufficient statistic.
 
 - Find the function of $T$ that is an unbiased estimator of $\sigma^p$
 
$E[T^{\frac{p}2}]=\frac{1}{\Gamma(\frac{n-1}2)2^{\frac{n-1}2}}\int_0^\infty t^{\frac{p+n-1}2-1}e^{-t/2}dt=\frac{2^{\frac{p}2}\Gamma(\frac{p+n-1}2)}{\Gamma(\frac{n-1}2)}$

$E[T^{\frac{p}2}]=E[(\frac{(n-1)S^2}{\sigma^2})^{\frac{p}2}]=E[\frac{(n-1)^{\frac{p}2}S^p}{\sigma^p}]$

$$\therefore E[\frac{(n-1)^{\frac{p}2}S^p}{\sigma^p}]=\frac{2^{\frac{p}2}\Gamma(\frac{p+n-1}2)}{\Gamma(\frac{n-1}2)}\implies E[{(\frac{n-1}2})^{\frac{p}2}\frac{\Gamma(\frac{n-1}2)}{\Gamma(\frac{p+n-1}2)}S^p]=\sigma^p$$
 
 ${(\frac{n-1}2})^{\frac{p}2}\frac{\Gamma(\frac{n-1}2)}{\Gamma(\frac{p+n-1}2)}S^p$ is an unbiased estimator of $\sigma^p$.

By Rao-Blackwell and Lehmann-Scheffe Theorem, when a function of T is an unbiased estimator of $\sigma^p$ and $T$ is a complete sufficient statistic for $\sigma^p$,  this function is the best unbiased estimator.

## HW2

**8.5** A random sample, $X_1,..,X_n$. , is drawn from a Pareto population with pdf $f(x|\theta,\nu)=\frac{\theta\nu^{\theta}}{x^{\theta+1}}I_{[\nu,\infty)}(x),\theta>0,\nu>0$

(a) Find the MLEs of $\theta$ and $\nu$.

When $x_{(1)}=\min_i x_i$,

$$L(\theta,\nu|x)=\frac{\theta^n\nu^{n\theta}}{\prod_i x_i^{\theta+1}}$$
$$\ln L(\theta,\nu|x)=n\ln\theta+n\theta\ln\nu-(\theta+1)\ln(\prod_i x_i), \nu\le x_{(1)}$$

Which is an increasing function of $\nu$ $\forall \theta$. So both the restricted and unrestricted $\hat\nu_{MLE}=x_{(1)}$.

$$\frac{\partial}{\partial\theta}\ln L(\theta,x_{(1)}|x)=\frac{n}\theta+n\ln x_{(1)}-\ln(\prod_i x_i)\overset{set}{=}0$$

$$\frac{\partial^2}{\partial\theta^2}\ln L(\theta,x_{(1)}|x)=-\frac{n}{\theta^2}<0$$

$$\hat\theta_{MLE}=\frac{n}{\ln(\prod_i x_i)-n\ln x_{(1)}}=\frac{n}{\ln(\frac{\prod_i x_i}{x_{(1)}^n})}=\frac{n}T$$

 ---

(b) Show that the LRT of $H_0:\theta=1, \nu$ unknown, versus $H_1: \theta\ne1,\nu$ unknown, has critical region of the form $\{x:T(x)\le C_1 or T(x)\ge c_2\}$, where $0<C_1<C_2$ and
$T=\ln\left[\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\right]$

$T=\ln\left[\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\right]\implies e^T=\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\implies\prod_{i=1}^n X_i=(\min\limits_{i} X_i)^ne^T=x_{(1)}^ne^T$

Under $H_0$, $\hat\theta_{MLE}=1$, $\hat\nu_{MLE}=x_{(1)}$. The likelihood ratio statistic is

$$\Lambda=\frac{\sup L(\hat\theta_0|x)}{\sup L(\hat\theta_{MLE}|x)}=\frac{\frac{1^nx_{(1)}^{n}}{\prod_i x_i^{1+1}}}{\frac{(\frac{n}T)^nx_{(1)}^{n\frac{n}T}}{\prod_i x_i^{\frac{n}T+1}}}=(\frac{T}n)^n\cdot x_{(1)}^{n-\frac{n^2}T}\cdot\prod_i x_i^{\frac{n}T-1}=(\frac{T}n)^n\cdot x_{(1)}^{n-\frac{n^2}T}\cdot x_{(1)}^{\frac{n^2}T-n}\cdot e^{n-T}=(\frac{T}n)^ne^{n-T}$$

$$\ln\Lambda=n\ln T-n\ln n+n-T$$
$$\frac{\partial}{\partial T}\ln\Lambda=\frac{n}{T}-1\begin{cases}>0&\text{if }T\le n\\<0&\text{if }T\ge n\end{cases}$$

Hence, $\Lambda$ is increasing if $T\le n$ and decreasing if $T\ge n$.
Thus, $T\le c$ is equivalent to $T\le c_1$ or $T\ge c_2$, for appropriately chosen constants $c_1$ and $c_2$.

 ---
 
(c) Show that, under $H_0, 2T$ has a $\chi^2$ distribution, and find the number of degrees of freedom. 
(Hint: Obtain the joint distribution of the $n-1$ nontrivial terms $X_i/(\min_i X_i)$ conditional on $\min_i X_i$. Put these $n-1$ terms together, and notice that the distribution of $T$ given $\min_i X_i$ does not depend on $\min_i X_i$, so it is the unconditional distribution of $T$.)

Under $H_0$, $f(x|1,\nu)=\frac{\nu}{x^2}I_{[\nu,\infty)}(x),\nu>0$ is monotone when $x>\nu>0$

Let $Y_i=\ln X_i, i=1,..,n$. $X_i=e^{y_i}$, $\frac{dx}{dy}=e^y$, then

$$f(y|1,\nu)=f(x|1,\nu)|\frac{dx}{dy}|=\frac{\nu}{x^2}e^y=ve^{-y}, y>0$$

$Y_1=\nu e^{-y_1}$,$Y_i=\nu e^{-y_i}$

Let $W_1=Y_1=\nu e^{-y_1}$ and $W_2=Y_i−Y_{(1)}=\nu e^{-y_i}-\nu e^{-y_1},i=2,..,n$.

$y_1=\frac{\ln w_1}{\nu}, y_i=\ln(w_1-w_2)$. $|J|=\begin{vmatrix}\frac{1}{w_1\nu} & 0 \\ \frac1{w_1-w_2} & -\frac1{w_1-w_2} \end{vmatrix}=\frac1{w_1\nu(w_1-w_2)}$

$f_{W_1,W_2}(Y_{(1)},Y_i)=f_{Y_{(1)},Y_i}(\nu e^{-y_1},\nu e^{-y_i})|J|=\frac1{w_1\nu(w_1-w_2)}$

$W_1, W_2$ are independent, 

$f_{W_1}(w)=n\nu^ne^{−nw}, w>\ln\nu$,

$W_i\sim Expo(1), i=2,..,n$. 

$T=\sum_{i=1}^n(\ln X_i-\ln X_{(1)})=\sum_{i=1}^n(Y_i-Y_{(1)})=\sum_{i=2}^nW_i\sim Gam(n−1, 1)$


$$2T\sim Gam(n−1, 2)=\chi^2_{2(n−1)}$$

 ---
 
**8.17** Suppose that $X_1,..,X_n$ are iid with a $beta(\mu,1)$ pdf and $Y_1,..,Y_m$ are iid with a $beta(\theta,1)$ pdf. Also assume that the $X$s are independent of the $Y$s.

(a) Find an LRT of $H_0:(\theta=\mu$ versus $H_1:(\theta\ne\mu$.

$f_X(x)=\frac{1}{B(\mu,1)}x^{\mu-1}(1-x)^{1-1}=\frac{\Gamma(\mu+1)}{\Gamma(\mu)\Gamma(1)}x^{\mu-1}=\mu x^{\mu-1}$

$f_Y(y)=\theta y^{\theta-1}$

$$L(\mu,\theta|x,y)=\mu^n(\prod_{i=1}^nx_i)^{\mu-1}\theta^m(\prod_{j=1}^my_j)^{\theta-1}$$
$$\ln L(\mu,\theta|x,y)=n\ln\mu+(\mu-1)\ln(\prod_{i=1}^nx_i)+m\ln\theta+(\theta-1)\ln(\prod_{j=1}^my_j)$$

$$\frac{\partial}{\partial\mu}\ln L(\mu,\theta|x,y)=\frac{n}\mu+\sum_{i=1}^n\ln x_i\overset{set}{=}0\implies\hat\mu_{MLE}=-\frac{n}{\sum_{i=1}^n\ln x_i}$$
$$\frac{\partial}{\partial\theta}\ln L(\mu,\theta|x,y)=\frac{m}\theta+\sum_{j=1}^m\ln y_j\overset{set}{=}0\implies\hat\theta_{MLE}=-\frac{m}{\sum_{j=1}^m\ln y_j}$$

Under $H_0$, $\theta=\mu$

$$L(\mu|x,y)=\mu^{n+m}(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{\mu-1}$$
$$\ln L(\mu|x,y)=(n+m)\ln\mu+(\mu-1)\ln(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{}$$
$$\frac{\partial}{\partial\mu}\ln L(\mu|x,y)=\frac{n+m}\mu+\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j\overset{set}{=}0\implies\hat\mu_{0}=-\frac{n+m}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j}$$

The LRT statistic is

$$\Lambda=\frac{\sup L(\hat\mu_0|x,y)}{\sup L(\hat\mu_{MLE}|x,y)}=\frac{\hat\mu_0^{n+m}(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{\hat\mu_{0}-1}}{\hat\mu_{MLE}^n(\prod_{i=1}^nx_i)^{\hat\mu_{MLE}-1}\hat\theta_{MLE}^m(\prod_{j=1}^my_j)^{\hat\theta_{MLE}-1}}=\frac{\hat\mu_0^{n+m}}{\hat\mu_{MLE}^n\hat\theta_{MLE}^m}\cdot(\prod_{i=1}^nx_i)^{\hat\mu_0-\hat\mu_{MLE}}(\prod_{j=1}^my_j)^{\hat\mu_0-\hat\theta_{MLE}}$$

 ---
 
(b) Show that the test in part (a) can be based on the statistic
$T=\frac{\sum\ln X_i}{\sum\ln X_i+\sum\ln Y_i}$

Substituting $(\prod_{i=1}^nx_i)^{\hat\mu_0-\hat\mu_{MLE}}(\prod_{j=1}^my_j)^{\hat\mu_0-\hat\theta_{MLE}}=1$

$$\Lambda=\frac{\hat\mu_0^{n+m}}{\hat\mu_{MLE}^n\hat\theta_{MLE}^m}=\frac{(-\frac{n+m}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{n+m}}{(-\frac{n}{\sum_{i=1}^n\ln x_i})^n(-\frac{m}{\sum_{j=1}^m\ln y_j})^m}=(\frac{n+m}{n})^n(\frac{n+m}{m})^m(\frac{\sum_{i=1}^n\ln x_i}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{n}(\frac{\sum_{j=1}^m\ln y_j}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{m}$$
$$=(\frac{n+m}{n})^n(\frac{n+m}{m})^mT^n(1-T)^m$$

which is an unimodal function of $T$, so rejecting $H_0$ when $\Lambda\le c$ equivalent to rejecting $H_0$ when $T\le c_0$ or $T\ge c_1$ where $c_0,c_1$ are appropriate chosen constants.

 ---

(c) Find the distribution of $T$ when $H_0$ is true, and then show how to get a test of size $\alpha=.10$.

We know if $X\sim Gamma(\mu,\beta)$ and $Y\sim Gamma(\theta,\beta)$ are independent, then $T=\frac{X}{X+Y}\sim Beta(\mu,\theta)$.

It also can derive from $-\ln X_i\sim Expo(\mu),-\ln Y_i\sim Expo(\theta)$ to $W=-\sum_{i=1}^n\ln X_i\sim Gamma(n,\mu),V=-\sum_{j=1}^m\ln Y_i\sim Gamma(n,\theta)$. Then $T=\frac{X}{X+Y}\sim Beta(\mu,\theta)$.

Solving this equations

$$\begin{cases}P(T\le c_1)+P(T\ge c_2)=\alpha=0.10\\c_1^n(1-c_1)^m=c_2^n(1-c_2)^m\end{cases}$$
The test $\Lambda\le c_0,\Lambda\ge c_1$ is most powerful $\forall\theta\neq\mu$, so it is UMP at $0.10$ level.

 ---
 
**8.19** The random variable $X$ has pdf $f(x)=e^{-x}, x>0$. One observation is obtained on the random variable $Y=X^{\theta}$, and a test of $H_0:\theta=1$ versus $H_1:\theta=2$ needs to be constructed. Find the UMP level $\alpha=.10$ test and compute the Type II Error probability.

$f(x)=e^{-x}$ is monotone when $x>0$. $X=Y^{1/\theta}$, $\frac{dx}{dy}=\frac1\theta y^{\frac1\theta-1}$, then

$$f(y|\theta)=f(x|\theta)|\frac{dx}{dy}|=e^{-y^{1/\theta}}\frac1\theta y^{\frac1\theta-1}, y>0$$


Neyman-Pearson Theorem says a test of $H_0:\theta_0=1$ versus $H_1:\theta_1=2$, i.e., $\Omega=\{1,2\},\Omega_0=\{1\}$ 

$$\Lambda=\frac{L(\theta_0|y)}{L(\theta_1|y)}=\frac{e^{-y^{1}} y^{1-1}}{e^{-y^{1/2}}\frac12 y^{1/2-1}}=2e^{(y^{1/2}-y)}y^{1/2}\overset{set}{\le}c$$

$$\frac{d}{dy}\Lambda=2e^{(y^{1/2}-y)}(\frac12y^{-1/2}-1)y^{1/2}+e^{(y^{1/2}-y)}y^{-1/2}=e^{(y^{1/2}-y)}(1-2y^{1/2}+y^{-1/2})\begin{cases}>0&\text{if }y< 1\\<0&\text{if }y>1\end{cases}$$

Thus $\Lambda$ is an unimodal function of $y$, $\max\Lambda=2$ when $y=1$.
The rejection region is $R=\{\vec y:\Lambda\le c\}$, and $c$ is chosen so that $P_{H_0}(\vec y\in R)=\alpha$, which equivalent to

$$\begin{cases}P(Y\le c_0|\theta_0)+P(Y\ge c_1|\theta_0)=\int_{-\infty}^{c_0}e^{-y}dy+\int_{c_1}^{\infty}e^{-y}dy=\left.-e^{-y}\right|_{-\infty}^{c_0}+1-(\left.-e^{-y}\right|_{-\infty}^{c_1})=1-e^{-c_0}+e^{-c_1}=0.10\\2e^{(c_0^{1/2}-c_0)}c_0^{1/2}=2e^{(c_1^{1/2}-c_1)}c_1^{1/2}\end{cases}$$

$$\begin{cases}c_1-c_0=\ln0.90\\c_0^{1/2}-c_0+\frac12\ln c_0=c_1^{1/2}-c_1+\frac12\ln c_1\end{cases}$$

```{r eval=FALSE, include=FALSE}
# log(a) + log(b) = log(5)
# 0.5 * (loga + 2 * log(b)) = log(10)
m <- matrix(c(1, .5, 1, 1), 2)
exp(solve(m, c(log(5),log(10))))

-exp(-3.637798^0.5)+exp(-0.076546^0.5)
```

The test $\Lambda\le c_0=0.076546$,$\Lambda\ge c_1=3.637798$ is most powerful $\forall\theta_1\notin\Omega_0$, so it is UMP at $0.10$ level.

The Type II error probability is

$$P(c_0<Y<c_1|\theta_1)=\int_{c_0}^{c_1}\frac12e^{-y^{1/2}} y^{-1/2}dy=\left.-e^{-y^{1/2}}\right|_{c_0}^{c_1}=0.609824$$

## HW3

**8.28** Let $f(x|\theta)$ be the logistic location pdf

$$f(x|\theta)=\frac{e^{x-\theta}}{(1+e^{x-\theta})^2},-\infty<x<\infty,-\infty<\theta<\infty$$

(a) Show that this family has an MLR.

(b) Based on one observation, $X$, find the most powerful size $\alpha$ test of $H_0:\theta=0$ versus $H_1:\theta=1$. For $\alpha=.2$, find the size of the Type II Error.

(c) Show that the test in part (b) is UMP size $\alpha$ for testing $H_0:\theta\le0$ versus $H_1:\theta>1$. What can be said about UMP tests in general for the logistic location family?


**8.33** Let $X_1,..,X_n$ be a random sample from the $uniform(0,\theta+1)$ distribution. $T_0$ test
$H_0:\theta=0$versus $H_1:\theta>0$, use the test reject $H_0$ if $Y_n\ge1$ or $Y_1\ge k$, where k is a constant, $Y_1= \min\{X_1,..,X_n\}, Y_n= \max\{X_1,..,X_n\}$.

(a) Determine k so that the test will have size $\alpha$.

(b) Find an expression for the power function of the test in part (a) .

(c) Prove that the test is UMP size $\alpha$.

(d) Find values of n and k so that the UMP .10 level test will have power at least .8 if $\theta>1$.

## HW4

1. Show $\sum C_id_i=0$


2. Repeat with $\hat\beta_0$, i.e. show $Cov(\hat\beta_0,\hat\varepsilon_j)=0$


## HW5

Show that $\hat x=\frac{by+x-ab}{1+b^2}; \hat y=a+\frac b{1+b^2}(by+x-ab)$



##

**7.30** The EM algorithm is useful in a variety of situation, and the definition of "missing data" can be stretched to accommodate many different models. Suppose that we have a mixture density $pf(x) + (1 p)g(x)$, where $p$ is unknown. If we observe $X = (X_1,..,X_n)$, the sample density is $\Pi_{i=1}^n[pf(x_i)+(1-p)g(x_i)$ which could be difficult to deal with. (Actually, a mixture of two is not terrible, but consider what the likelihood would look like with a mixture $L:=l P;Ji(X)$ for large k.)
The EM solution is to augment the observed (or incomplete) data with $Z (Z_1,..,Z_n)$, where $Z_i$ tells which component of the mixture $X_i$ came from; that is,

$X_i|z_i=1\sim f(X_i)$ and $X_i|z_i=0\sim g(x_i)$,and $P(Z_i=1)=p$

(a) Show that the joint density of $(X,Z)$ is given by $\sum^n_{i=1}[pf(x_i)^{z_i}][(1-p)g(x_i)^{1-z_i}]$.

Joint density $f(x,y)=f(x|y)f(y)$,

$$f(\vec x,\vec z)=f(\vec x|\vec z)f(\vec z)=\prod^n_{i=1}f(x_i|z_i)f(z_i)=\prod^n_{i=1}[pf(x_i)]^{z_i}[(1-p)g(x_i)]^{1-z_i}$$

(b) Show that the missing data distribution, the distribution of $Z_i|x_i,p$ is Bernoulli with success probability $\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$.

Conditional density $f(x|y)=\frac{f(x,y)}{f(y)}$,

$$P(Z_i=1|X_i,p)=\frac{f(x_i,1|p)}{f(x_i|p)}=\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$$

(c) Calculate the expected complete-data log likelihood, and show that the EM sequence is given by

$$f(\vec x,\vec z)\propto p^{\sum^n_{i=1}z_i}(1-p)^{\sum^n_{i=1}(1-z_i)}$$

which is a Bernoulli joint pdf. For complete data,

$$\hat p=\frac1n\sum Z_i$$

Replacing the missing data with its expectation,

$$E(Z_i|X_i,p)=P(Z_i=1|X_i,p)$$

Through r times iteration,

$$\hat p^{(r+1)}=\frac1n\sum_{i=1}^n\frac{\hat p^{(r)}f(x_i)}{\hat p^{(r)}f(x_i)+(1-\hat p^{(r)})g(x_i)}$$