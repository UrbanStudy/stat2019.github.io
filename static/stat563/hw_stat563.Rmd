---
title: ''
author: ""
date: ""
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**7.12** Let $X_1,..,X_n$, be a random sample from a population with pmf

$$P_{\theta}(X=x)=\theta^x(1-\theta)^{1-x}, x=0\ or\ 1, 0\le \theta\le\frac12$$

(a) Find the method of moments estimator and MLE of $\theta$

$X_i\sim iid Bernoulli(\theta), 0\le \theta\le\frac12$

method of moments:

$$EX=\theta=\frac1n\sum_iX_i=\bar X )\implies\hat\theta=\bar X$$

MLE: In Example 7.2.7, we showed that $L(|x)$ is increasing for $\theta<\bar x$ and is decreasing for $\theta\ge\bar x$. For $0\le \theta\le\frac12$. Therefore, when $\bar X\le1/2$, $\bar X$ is the MLE of $\theta$, because ¯X is the overall maximum of $L(|\mathbf{x})$. When $\bar X > 1/2$, $L(\theta|\mathbf{x})$ is an increasing function of $\theta$ on $[0, 1/2]$ and obtains its maximum at the upper bound of $\theta$ which is $1/2$. So the MLE is $\hat\theta=\min\{\bar X,1/2\}$.

 ---
 
(b) Find the mean squared errors of each of the estimators.

The MSE of $\tilde\theta$is $MSE(\tilde\theta) = Var\tilde\theta + bias(\tilde\theta)^2=((1−\theta)/n)+0^2=(1−\theta)/n$. There is no simple formula for $MSE(\theta)$, but an expression is 

$$MSE(\hat\theta) = E(\hat\theta-\theta)^2=\sum^n_{y=0}(\hat\theta-\theta)^2{n\choose y}\theta^y(1−\theta )^{n−y}=\sum^\frac{n}2_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y} +\sum^n_{y=\frac{n}2+1}(\frac12−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$$
where $Y=\sum_iX_i\sim binomial(n,\theta)$ and $[\frac{n}2]=\frac{n}2$, if n is even, and $[\frac{n}2]=\frac{n-1}2$, if $n$ is odd.

 ---
 
(c) Which estimator is preferred? Justify your choice

Using the notation used in (b), we have

$$MSE(\tilde\theta) = E(\bar X-\theta)^2=\sum^n_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$$
Therefore,
$$MSE(\tilde\theta) − MSE(\hat\theta)=\sum^n_{y=[\frac{n}2]+1}\left[(\frac{y}n−\theta)^2-(\frac12−\theta)^2\right]\binom{n}{y}\theta^y(1−\theta)^{n−y}=\sum^n_{y=[\frac{n}2]+1}(\frac{y}n+\frac12−\theta)(\frac{y}n-\frac12)\binom{n}{y}\theta^y(1−\theta)^{n−y}$$
 
The facts that y/n > 1/2 in the sum and     1/2 imply that every term in the sum is positive.
Therefore $MSE(\hat\theta)<MSE(\tilde\theta)=0$ for every $\theta$ in $0<\tilde\theta<\frac12$. (Note: $MSE(\hat\theta)=MSE(\tilde\theta)=0$ at $\theta=0$.)
  
  

**7.30** The EM algorithm is useful in a variety of situation, and the definition of "missing data" can be stretched to accommodate many different models. Suppose that we have a mixture density $pf(x) + (1 p)g(x)$, where $p$ is unknown. If we observe $X = (X_1,..,X_n)$, the sample density is $\Pi_{i=1}^n[pf(x_i)+(1-p)g(x_i)$ which could be difficult to deal with. (Actually, a mixture of two is not terrible, but consider what the likelihood would look like with a mixture $L:=l P;Ji(X)$ for large k.)
The EM solution is to augment the observed (or incomplete) data with $Z (Z_1,..,Z_n)$, where $Z_i$ tells which component of the mixture $X_i$ came from; that is,

$X_i|z_i=1\sim f(X_i)$ and $X_i|z_i=0\sim g(x_i)$,and $P(Z_i=1)=p$

(a) Show that the joint density of $(X,Z)$ is given by $\sum^n_{i=1}[pf(x_i)^{z_i}][(1-p)g(x_i)^{1-z_i}]$.

Joint density $f(x,y)=f(x|y)f(y)$,

$$f(\vec x,\vec z)=f(\vec x|\vec z)f(\vec z)=\prod^n_{i=1}f(x_i|z_i)f(z_i)=\prod^n_{i=1}[pf(x_i)]^{z_i}[(1-p)g(x_i)]^{1-z_i}$$

(b) Show that the missing data distribution, the distribution of $Z_i|x_i,p$ is Bernoulli with success probability $\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$.

Conditional density $f(x|y)=\frac{f(x,y)}{f(y)}$,

$$P(Z_i=1|X_i,p)=\frac{f(x_i,1|p)}{f(x_i|p)}=\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$$

(c) Calculate the expected complete-data log likelihood, and show that the EM sequence is given by

$$f(\vec x,\vec z)\propto p^{\sum^n_{i=1}z_i}(1-p)^{\sum^n_{i=1}(1-z_i)}$$

which is a Bernoulli joint pdf. For complete data,

$$\hat p=\frac1n\sum Z_i$$

Replacing the missing data with its expectation,

$$E(Z_i|X_i,p)=P(Z_i=1|X_i,p)$$

Through r times iteration,

$$\hat p^{(r+1)}=\frac1n\sum_{i=1}^n\frac{\hat p^{(r)}f(x_i)}{\hat p^{(r)}f(x_i)+(1-\hat p^{(r)})g(x_i)}$$

**7.48** Suppose that $X_i, i=1,..,n$, are iid $Bernoulli(p)$.

(a) Show that the variance of the MLE of $p$ attains the Cramer-Rae Lower Bound.

(b) For $n\ge4$, show that the product $X_1X_2X_3X_4$ is an unbiased estimator of $p^4$, and use this fact to find the best unbiased estimator of $p^4$.



**7.59** Let $X_1,..,X_n$ be iid $n(\mu, \sigma^2)$. Find the best unbiased estimator of $\sigma^2$, where $p$ is a known positive constant, not necessarily an integer.


## HW2

**8.5** A random sample, $X_1,..,X_n$. , is drawn from a Pareto population with pdf $f(x|\theta,\nu)=\frac{\theta\nu^{\theta}}{x^{\theta+1}}I_{[\nu,\infty)}(x),\theta>0,\nu>0$

(a) Find the MLEs of $\theta$ and $\nu$.


(b) Show that the LRT of $H_0:\theta=1, \nu$ unknown, versus $H_1: \theta\ne1,\nu$ unknown, has critical region of the form $\{x:T(x)\le C_1 or T(x)\ge c_2\}$, where $0<C_1<C_2$ and

$$T=\ln\left[\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\right]$$

(c) Show that, under $H_0, 2T$ has a chi squared distribution, and find the number of degrees of freedom. 

(Hint: Obtain the joint distribution of the $n-1$ nontrivial terms $X_i/(\min_i X_i)$ conditional on $\min_i X_i$. Put these $n-1$ terms together, and notice that the distribution of $T$ given $\min_i X_i$ does not depend on $\min_i X_i$, so it is the unconditional distribution of $T$.)



**8.17** Suppose that $X_1,..,X_n$ are iid with a $beta(\mu,1)$ pdf and $Y_1,..,Y_m$ are iid with a $beta(\theta,1)$ pdf. Also assume that the $X_S$ are independent of the $Y_s$.

(a) Find an LRT of $H_0:(\theta=\mu$ versus $H_1:(\theta\ne\mu$.

(b) Show that the test in part (a) can be based on the statistic

$$T=\frac{\sum\ln X_i}{\sum\ln X_i+\sum\ln Y_i}$$

(c) Find the distribution of $T$ when $H_0$ is true, and then show how to get a test of size $\alpha=.10$.

**8.19** The random variable $X$ has pdf $f(x)=e{-x}, x>O$. One observation is obtained on the random variable $Y=X^{\theta}$, and a test of $H_0:\theta=1$ versus $H_1:\theta=2$ needs to be constructed. Find the UMP level $\alpha=.10$ test and compute the Type II Error probability.

## HW3

**8.28** Let $f(x|\theta)$ be the logistic location pdf

$$f(x|\theta)=\frac{e^{x-\theta}}{(1+e^{x-\theta})^2},-\infty<x<\infty,-\infty<\theta<\infty$$

(a) Show that this family has an MLR.

(b) Based on one observation, $X$, find the most powerful size $\alpha$ test of $H_0:\theta=0$ versus $H_1:\theta=1$. For $\alpha=.2$, find the size of the Type II Error.

(c) Show that the test in part (b) is UMP size $\alpha$ for testing $H_0:\theta\le0$ versus $H_1:\theta>1$. What can be said about UMP tests in general for the logistic location family?


**8.33** Let $X_1,..,X_n$ be a random sample from the $uniform(0,\theta+1)$ distribution. $T_0$ test
$H_0:\theta=0$versus $H_1:\theta>0$, use the test reject $H_0$ if $Y_n\ge1$ or$ Y_1\ge k$, where k is a constant, $Y_1= \min\{X_1,..,X_n\}, Y_n= \max\{X_1,..,X_n\}$.

(a) Determine k so that the test will have size $\alpha$.

(b) Find an expression for the power function of the test in part (a) .

(c) Prove that the test is UMP size $\alpha$.

(d) Find values of n and k so that the UMP .10 level test will have power at least .8 if $\theta>1$.



