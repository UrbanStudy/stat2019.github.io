---
title: ''
author: "STAT563 HW1 Shen Qu"
date: ""
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**7.12** Let $X_1,..,X_n$, be a random sample from a population with pmf
$P_{\theta}(X=x)=\theta^x(1-\theta)^{1-x}, x=0\ or\ 1, 0\le \theta\le\frac12$

(a) Find the method of moments estimator and MLE of $\theta$

For $X_i\sim iid Bernoulli(\theta), 0\le \theta\le\frac12$, We have $EX=\theta, VX=\theta(1-\theta)$

 - method of moments: 
 
Set $EX=\theta=\frac1n\sum_iX_i=\bar X \implies\hat\theta_{MOM}=\bar X$

 - MLE: 

$L(\theta)=\prod\theta^{x_i}(1-\theta)^{1-x_i}=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$

$l(\theta)=\sum_{i=1}^nx_i\ln\theta+(n-\sum x_i)\ln(1-\theta)$

$l'(\theta)=\frac1\theta\sum_{i=1}^nx_i+\frac1{1-\theta}(n-\sum x_i)\overset{\text{set}}{=}0$

$l''(\theta)<0$


From Example 7.2.7, $L(|x)$ is increasing for $\theta<\bar x$ and is decreasing for $\theta\ge\bar x$. 

For $0\le \theta\le\frac12$, when $\bar X\le1/2$, $\max{L(|\mathbf{x})}=\bar X$, $\hat\theta_{MLE}=\bar X$. 

When $\bar X>1/2$, $L(\theta|\mathbf{x})$ is an increasing function of $\theta$ on $[0, 1/2]$, $\max{L(|\mathbf{x})}=1/2$, $\hat\theta_{MLE}=1/2$. 

So $\hat\theta_{MLE}=\min\{\bar X,1/2\}$.

 ---
 
(b) Find the mean squared errors of each of the estimators.

$MSE(\hat\theta) = E(\hat\theta-\theta)^2$

Let $y=\sum^n_{i=1}X_i=n\bar X\sim Binomial(n,\theta)$, then $\begin{cases}y\le[n/2]&when\bar X\le1/2\\y\ge[n/2]+1&when\bar X>1/2\end{cases}$

$MSE(\hat\theta_{MOM}) = E(\bar X-\theta)^2=\sum^n_{y=0}(\frac{y}n-\theta)^2{n\choose y}\theta^y(1−\theta )^{n−y}$

$MSE(\hat\theta) = E(\hat\theta_{MLE}-\theta)^2=\sum^{[\frac{n}2]}_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y} +\sum^n_{y=[\frac{n}2]+1}(\frac12−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$

<!--Note: The MSE of $\tilde\theta$is $MSE(\tilde\theta) = Var\tilde\theta + bias(\tilde\theta)^2=((1−\theta)/n)+0^2=(1−\theta)/n$. There is no simple formula for $MSE(\theta)$. $[\frac{n}2]=\frac{n}2$, if n is even, and $[\frac{n}2]=\frac{n-1}2$, if $n$ is odd.-->

 ---
 
(c) Which estimator is preferred? Justify your choice

$MSE(\hat\theta_{MOM})−MSE(\hat\theta_{MLE})=\sum^n_{y=0}(\frac{y}n-\theta)^2{n\choose y}\theta^y(1−\theta )^{n−y}-\sum^{[\frac{n}2]}_{y=0}(\frac{y}n−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y} -\sum^n_{y=[\frac{n}2]+1}(\frac12−\theta)^2\binom{n}{y}\theta^y(1−\theta)^{n−y}$


$$=\sum^n_{y=[\frac{n}2]+1}\left[(\frac{y}n−\theta)^2-(\frac12−\theta)^2\right]\binom{n}{y}\theta^y(1−\theta)^{n−y}=\sum^n_{y=[\frac{n}2]+1}(\frac{y}n+\frac12−2\theta)(\frac{y}n-\frac12)\binom{n}{y}\theta^y(1−\theta)^{n−y}$$
 
For $y/n>1/2, y/n-1/2>0$. For $\theta\le1/2$, $y/n+1/2>1\ge2\theta$

Therefore, $MSE(\hat\theta_{MOM})−MSE(\hat\theta_{MLE})\ge0$, $MSE(\hat\theta_{MOM})\ge MSE(\hat\theta_{MLE})$. $MSE(\hat\theta_{MLE})$ is better.

 ---  

**7.48** Suppose that $X_i, i=1,..,n$, are iid $Bernoulli(p)$.

(a) Show that the variance of the MLE of $p$ attains the Cramer-Rae Lower Bound.

 -MLE of p

$L(p)=\prod p^{x_i}(1-p)^{1-x_i}=p^{\sum x_i}(1-p)^{n-\sum x_i}$

$l(p)=\sum_{i=1}^nx_i\ln p+(n-\sum x_i)\ln(1-p)$

$l'(p)=\frac1p\sum_{i=1}^nx_i+\frac1{1-p}(n-\sum x_i)\overset{\text{set}}{=}0$

$l''(p)<0$

$\hat p_{MLE}=\bar X$

$E[\hat p_{MLE}]=E[\bar X]=p$

$V[\hat p_{MLE}]=V[\bar X]=\frac{p(1-p)}{n}$

Let $w=\bar x$, then $k(p)=E[w]=p$, $f(x|p)=p^x(1-p)^{1-x}\sim Bern(p)$

$\ln f(x|p)=x\ln p+(1-x)\ln(1-p)$

$\frac\partial{\partial{p}}\ln f(x|p)=\frac{x}p-\frac{1-x}{1-p}$

$\frac{\partial^2}{\partial{p}}\ln f(x|p)=-\frac{x}{p^2}-\frac{1-x}{(1-p)^2}$

$I(p)=-E[\frac{\partial^2}{\partial{p}}\ln f(x|p)]=-E[-\frac{x}{p^2}-\frac{1-x}{(1-p)^2}]=\frac{1}{p(1-p)}$

By Cramer-Rao Inequality,

$$Var_p(W)\ge\frac{(\frac{\partial}{\partial p}E_p[W])^2}{nI(p)}=\frac{(k'(p))^2}{n\frac{1}{p(1-p)}}=\frac{p(1-p)}{n}$$

 ---

(b) For $n\ge4$, show that the product $X_1X_2X_3X_4$ is an unbiased estimator of $p^4$, and use this fact to find the best unbiased estimator of $p^4$.

$E(x_1x_2x_3x_4)=\prod_{i=1}^4 EX_i=p^4$ which is a unbiased estimator.

$T(\vec x)=\sum_{i=1}^n X_i\sim Bino(n,p)$ is a complete sufficient statistic.

By Rao-Blackwell and Lehmann-Scheffe Theorem, $E[x_1x_2x_3x_4|\sum_{i=1}^n X_i=t]$ is MUVE of $p^4$.

$E[x_1x_2x_3x_4|\sum_{i=1}^n X_i=t]=\sum_{x_1=0}^1\sum_{x_2=0}^1\sum_{x_3=0}^1\sum_{x_4=0}^1x_1x_2x_3x_4P(x_1x_2x_3x_4|\sum_{i=1}^n X_i=t)$

$=P(x_1=x_2=x_3=x_4=1|\sum_{i=1}^n X_i=t)=\frac{P(x_1=x_2=x_3=x_4=1,\sum_{i=1}^n X_i=t)}{P(\sum_{i=1}^n X_i=t)}$

$=\frac{P(x_1=x_2=x_3=x_4=1,\sum_{i=5}^n X_i=t-4)}{P(\sum_{i=1}^n X_i=t)}=\frac{p^4{n-4\choose t-4}p^{t-4}(1−p)^{n−t}}{{n\choose t}p^{t}(1−p)^{n−t}}=\frac{n-4\choose t-4}{n\choose t}$

 ---

**7.59** Let $X_1,..,X_n$ be iid $n(\mu, \sigma^2)$. Find the best unbiased estimator of $\sigma^2$, where $p$ is a known positive constant, not necessarily an integer.

 - Find the complete sufficient statistic for $\sigma^p$ called $T$

Let $T=(n-1)S^2/\sigma^2\sim \chi^2_{n-1}=Gamma(\frac{n-1}2,2)$, then 

 Form Theorem 6.2.25, in the exponential familiy, $T=(n-1)S^2/\sigma^2$ is a complete sufficient statistic.
 
 - Find the function of $T$ that is an unbiased estimator of $\sigma^p$
 
$E[T^{\frac{p}2}]=\frac{1}{\Gamma(\frac{n-1}2)2^{\frac{n-1}2}}\int_0^\infty t^{\frac{p+n-1}2-1}e^{-t/2}dt=\frac{2^{\frac{p}2}\Gamma(\frac{p+n-1}2)}{\Gamma(\frac{n-1}2)}$

$E[T^{\frac{p}2}]=E[(\frac{(n-1)S^2}{\sigma^2})^{\frac{p}2}]=E[\frac{(n-1)^{\frac{p}2}S^p}{\sigma^p}]$

$$\therefore E[\frac{(n-1)^{\frac{p}2}S^p}{\sigma^p}]=\frac{2^{\frac{p}2}\Gamma(\frac{p+n-1}2)}{\Gamma(\frac{n-1}2)}\implies E[{(\frac{n-1}2})^{\frac{p}2}\frac{\Gamma(\frac{n-1}2)}{\Gamma(\frac{p+n-1}2)}S^p]=\sigma^p$$
 
 ${(\frac{n-1}2})^{\frac{p}2}\frac{\Gamma(\frac{n-1}2)}{\Gamma(\frac{p+n-1}2)}S^p$ is an unbiased estimator of $\sigma^p$.

By Rao-Blackwell and Lehmann-Scheffe Theorem, when a function of T is an unbiased estimator of $\sigma^p$ and $T$ is a complete sufficient statistic for $\sigma^p$,  this function is the best unbiased estimator.

## HW2

**8.5** A random sample, $X_1,..,X_n$. , is drawn from a Pareto population with pdf $f(x|\theta,\nu)=\frac{\theta\nu^{\theta}}{x^{\theta+1}}I_{[\nu,\infty)}(x),\theta>0,\nu>0$

(a) Find the MLEs of $\theta$ and $\nu$.

When $x_{(1)}=\min_i x_i$,

$$L(\theta,\nu|x)=\frac{\theta^n\nu^{n\theta}}{\prod_i x_i^{\theta+1}}$$
$$\ln L(\theta,\nu|x)=n\ln\theta+n\theta\ln\nu-(\theta+1)\ln(\prod_i x_i), \nu\le x_{(1)}$$

Which is an increasing function of $\nu$ $\forall \theta$. So both the restricted and unrestricted $\hat\nu_{MLE}=x_{(1)}$.

$$\frac{\partial}{\partial\theta}\ln L(\theta,x_{(1)}|x)=\frac{n}\theta+n\ln x_{(1)}-\ln(\prod_i x_i)\overset{set}{=}0$$

$$\frac{\partial^2}{\partial\theta^2}\ln L(\theta,x_{(1)}|x)=-\frac{n}{\theta^2}<0$$

$$\hat\theta_{MLE}=\frac{n}{\ln(\prod_i x_i)-n\ln x_{(1)}}=\frac{n}{\ln(\frac{\prod_i x_i}{x_{(1)}^n})}=\frac{n}T$$

 ---

(b) Show that the LRT of $H_0:\theta=1, \nu$ unknown, versus $H_1: \theta\ne1,\nu$ unknown, has critical region of the form $\{x:T(x)\le C_1 or T(x)\ge c_2\}$, where $0<C_1<C_2$ and
$T=\ln\left[\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\right]$

$T=\ln\left[\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\right]\implies e^T=\frac{\prod_{i=1}^n X_i}{(\min\limits_{i} X_i)^n}\implies\prod_{i=1}^n X_i=(\min\limits_{i} X_i)^ne^T=x_{(1)}^ne^T$

Under $H_0$, $\hat\theta_{MLE}=1$, $\hat\nu_{MLE}=x_{(1)}$. The likelihood ratio statistic is

$$\Lambda=\frac{\sup L(\hat\theta_0|x)}{\sup L(\hat\theta_{MLE}|x)}=\frac{\frac{1^nx_{(1)}^{n}}{\prod_i x_i^{1+1}}}{\frac{(\frac{n}T)^nx_{(1)}^{n\frac{n}T}}{\prod_i x_i^{\frac{n}T+1}}}=(\frac{T}n)^n\cdot x_{(1)}^{n-\frac{n^2}T}\cdot\prod_i x_i^{\frac{n}T-1}=(\frac{T}n)^n\cdot x_{(1)}^{n-\frac{n^2}T}\cdot x_{(1)}^{\frac{n^2}T-n}\cdot e^{n-T}=(\frac{T}n)^ne^{n-T}$$

$$\ln\Lambda=n\ln T-n\ln n+n-T$$
$$\frac{\partial}{\partial T}\ln\Lambda=\frac{n}{T}-1\begin{cases}>0&\text{if }T\le n\\<0&\text{if }T\ge n\end{cases}$$

Hence, $\Lambda$ is increasing if $T\le n$ and decreasing if $T\ge n$.
Thus, $T\le c$ is equivalent to $T\le c_1$ or $T\ge c_2$, for appropriately chosen constants $c_1$ and $c_2$.

 ---
 
(c) Show that, under $H_0, 2T$ has a $\chi^2$ distribution, and find the number of degrees of freedom. 
(Hint: Obtain the joint distribution of the $n-1$ nontrivial terms $X_i/(\min_i X_i)$ conditional on $\min_i X_i$. Put these $n-1$ terms together, and notice that the distribution of $T$ given $\min_i X_i$ does not depend on $\min_i X_i$, so it is the unconditional distribution of $T$.)

Under $H_0$, $f(x|1,\nu)=\frac{\nu}{x^2}I_{[\nu,\infty)}(x),\nu>0$ is monotone when $x>\nu>0$

Let $Y_i=\ln X_i, i=1,..,n$. $X_i=e^{y_i}$, $\frac{dx}{dy}=e^y$, then

$f(y|1,\nu)=f(x|1,\nu)|\frac{dx}{dy}|=\frac{\nu}{x^2}e^y=ve^{-y}, y>0$

$F(y|1,\nu)=\int_0^{\infty}ve^{-y}dy=1-ve^{-y}$

$f_{Y_{(1)}}(y)=\frac{n!}{(1-1)!(n-1)!}f_Y(y)[F_Y(y)]^{1-1}[1-F_Y(y)]^{n-1}=n\cdot\nu e^{−y}\cdot(\nu e^{−y})^{n-1}=n\nu^ne^{−ny}$

Let $Z_1=Y_{(1)}$, $Z_2,..,Z_i$ equal to the remaining $Y_i$s. $Z_1$ and $Z_i$ are independent.

$f_{Z_1,Z_i}(y_{(1)},y_i)=f_{Y_{(1)}}(y_{(1)})f_{Y_i}(y_i)=n\nu^ne^{−ny_{(1)}}\nu e^{-y_i}=n\nu^{n+1}e^{−(ny_{(1)}+y_i)}, i=2,..,n$

Let $W_1=Z_{1}=n\nu^ne^{−ny}$ and $W_i=Z_i−Z_{1}, i=2,..,n$, then $Z_{1}=w_1, Z_i=w_1+w_i$. $|J|=\begin{vmatrix}1 & 0 \\ 1 & 1 \end{vmatrix}=1$

$$f_{W_1,W_i}(w_1,w_i)=f_{Z_{1},Z_i}(w_1,w_1+w_i)|J|=n\nu^{n+1}e^{−(nw_1+w_1+w_i)}=n\nu^{n+1}e^{−(n+1)w_1}e^{−w_i}$$

$f_{W_1}(w)=n\nu^{n+1}e^{−(n+1)w}, w>\ln\nu$, $f_{W_i}(w)=e^{-w}$, $W_1, W_i$ are independent.

$W_i=Z_i−Z_{1}=e^{-w_i}\sim Expo(1), i=2,..,n$. 

$T=\sum_{i=1}^n(Y_i-Y_{(1)})=Y_1-Y_{(1)}+\sum_{i=2}^n(Y_i-Y_{(1)})=\sum_{i=2}^n(Z_i−Z_{1})=\sum_{i=2}^nW_i\sim Gam(n−1, 1)$

$$2T\sim Gam(n−1, 2)=\chi^2_{2(n−1)}$$

 ---
 
**8.17** Suppose that $X_1,..,X_n$ are iid with a $beta(\mu,1)$ pdf and $Y_1,..,Y_m$ are iid with a $beta(\theta,1)$ pdf. Also assume that the $X$s are independent of the $Y$s.

(a) Find an LRT of $H_0: \theta=\mu$ versus $H_1: \theta\ne\mu$.

$f_X(x)=\frac{1}{B(\mu,1)}x^{\mu-1}(1-x)^{1-1}=\frac{\Gamma(\mu+1)}{\Gamma(\mu)\Gamma(1)}x^{\mu-1}=\mu x^{\mu-1}$

$f_Y(y)=\theta y^{\theta-1}$

$$L(\mu,\theta|x,y)=\mu^n(\prod_{i=1}^nx_i)^{\mu-1}\theta^m(\prod_{j=1}^my_j)^{\theta-1}$$
$$\ln L(\mu,\theta|x,y)=n\ln\mu+(\mu-1)\ln(\prod_{i=1}^nx_i)+m\ln\theta+(\theta-1)\ln(\prod_{j=1}^my_j)$$

$$\frac{\partial}{\partial\mu}\ln L(\mu,\theta|x,y)=\frac{n}\mu+\sum_{i=1}^n\ln x_i\overset{set}{=}0\implies\hat\mu_{MLE}=-\frac{n}{\sum_{i=1}^n\ln x_i}$$
$$\frac{\partial}{\partial\theta}\ln L(\mu,\theta|x,y)=\frac{m}\theta+\sum_{j=1}^m\ln y_j\overset{set}{=}0\implies\hat\theta_{MLE}=-\frac{m}{\sum_{j=1}^m\ln y_j}$$

Under $H_0$, $\theta=\mu$

$$L(\theta|x,y)=\theta^{n+m}(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{\theta-1}$$
$$\ln L(\theta|x,y)=(n+m)\ln\theta+(\theta-1)\ln(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{}$$
$$\frac{\partial}{\partial\theta}\ln L(\theta|x,y)=\frac{n+m}\theta+\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j\overset{set}{=}0\implies\hat\theta_{0}=-\frac{n+m}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j}$$

The LRT statistic is

$$\Lambda=\frac{\sup L(\hat\theta_0|x,y)}{\sup L(\hat\mu_{MLE},\hat\theta_{MLE}|x,y)}=\frac{\hat\theta_0^{n+m}(\prod_{i=1}^nx_i\prod_{j=1}^my_j)^{\hat\theta_{0}-1}}{\hat\mu_{MLE}^n(\prod_{i=1}^nx_i)^{\hat\mu_{MLE}-1}\hat\theta_{MLE}^m(\prod_{j=1}^my_j)^{\hat\theta_{MLE}-1}}=\frac{\hat\theta_0^{n+m}}{\hat\mu_{MLE}^n\hat\theta_{MLE}^m}\cdot(\prod_{i=1}^nx_i)^{\hat\theta_0-\hat\mu_{MLE}}(\prod_{j=1}^my_j)^{\hat\theta_0-\hat\theta_{MLE}}$$

 ---
 
(b) Show that the test in part (a) can be based on the statistic
$T=\frac{\sum\ln X_i}{\sum\ln X_i+\sum\ln Y_i}$

Substituting $(\prod_{i=1}^nx_i)^{\hat\theta_0-\hat\mu_{MLE}}(\prod_{j=1}^my_j)^{\hat\theta_0-\hat\theta_{MLE}}=1$

$$\Lambda=\frac{\hat\theta_0^{n+m}}{\hat\mu_{MLE}^n\hat\theta_{MLE}^m}=\frac{(-\frac{n+m}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{n+m}}{(-\frac{n}{\sum_{i=1}^n\ln x_i})^n(-\frac{m}{\sum_{j=1}^m\ln y_j})^m}=(\frac{n+m}{n})^n(\frac{n+m}{m})^m(\frac{\sum_{i=1}^n\ln x_i}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{n}(\frac{\sum_{j=1}^m\ln y_j}{\sum_{i=1}^n\ln x_i+\sum_{j=1}^m\ln y_j})^{m}$$
$$=(\frac{n+m}{n})^n(\frac{n+m}{m})^mT^n(1-T)^m$$

which is an unimodal function of $T$, so rejecting $H_0$ when $\Lambda\le c$ equivalent to rejecting $H_0$ when $T\le c_0$ or $T\ge c_1$ where $c_0,c_1$ are appropriate chosen constants.

 ---

(c) Find the distribution of $T$ when $H_0$ is true, and then show how to get a test of size $\alpha=.10$.(Hint: Exercise 4.19)

For $X_i\sim Beta(\mu,1), Y_i\sim Beta(\theta,1)$, $-\ln X_i\sim Expo(1/\mu),-\ln Y_i\sim Expo(1/\theta)$, then

$W=-\sum_{i=1}^n\ln X_i\sim Gamma(n,1/\mu),V=-\sum_{j=1}^m\ln Y_i\sim Gamma(m,1/\theta)$. For $\mu=\theta$, $W$ and $V$ are independent, $T=\frac{W}{W+V}\sim Beta(n,m)$.

Solving this equations

$$\begin{cases}P(T\le c_0)+P(T\ge c_1)=\alpha=0.10\\c_01^{n}(1-c_0)^{m}=c_1^{n}(1-c_1)^{m}\end{cases}$$
The test $\Lambda\le c_0,\Lambda\ge c_1$ is most powerful $\forall\theta\neq\mu$, so it is a UMP level $\alpha=0.10$ test.

 ---
 
**8.19** The random variable $X$ has pdf $f(x)=e^{-x}, x>0$. One observation is obtained on the random variable $Y=X^{\theta}$, and a test of $H_0:\theta=1$ versus $H_1:\theta=2$ needs to be constructed. Find the UMP level $\alpha=.10$ test and compute the Type II Error probability.

$f(x)=e^{-x}$ is monotone when $x>0$. $X=Y^{1/\theta}$, $\frac{dx}{dy}=\frac1\theta y^{\frac1\theta-1}$, then

$$f(y|\theta)=f(x|\theta)|\frac{dx}{dy}|=e^{-y^{1/\theta}}\frac1\theta y^{\frac1\theta-1}, y>0$$


Neyman-Pearson Theorem says a test of $H_0:\theta_0=1$ versus $H_1:\theta_1=2$, i.e., $\Omega=\{1,2\},\Omega_0=\{1\}$ 

$$\Lambda=\frac{L(\theta_0|y)}{L(\theta_1|y)}=\frac{e^{-y^{1}} y^{1-1}}{e^{-y^{1/2}}\frac12 y^{1/2-1}}=2e^{(y^{1/2}-y)}y^{1/2}\overset{set}{\le}c$$

$$\frac{d}{dy}\Lambda=2e^{(y^{1/2}-y)}(\frac12y^{-1/2}-1)y^{1/2}+e^{(y^{1/2}-y)}y^{-1/2}=e^{(y^{1/2}-y)}(1-2y^{1/2}+y^{-1/2})\begin{cases}>0&\text{if }y< 1\\<0&\text{if }y>1\end{cases}$$

Thus $\Lambda$ is an unimodal function of $y$, $\max\Lambda=2$ when $y=1$.
The rejection region is $R=\{\vec y:\Lambda\le c\}$, and $c$ is chosen so that $P_{H_0}(\vec y\in R)=\alpha$, which equivalent to

$$\begin{cases}P(Y\le c_0|\theta_0)+P(Y\ge c_1|\theta_0)=\alpha& (1)\\2e^{(c_0^{1/2}-c_0)}c_0^{1/2}=2e^{(c_1^{1/2}-c_1)}c_1^{1/2}& (2)\end{cases}$$

$(1)=\int_{-\infty}^{c_0}e^{-y}dy+\int_{c_1}^{\infty}e^{-y}dy=\left.-e^{-y}\right|_{-\infty}^{c_0}+1-(\left.-e^{-y}\right|_{-\infty}^{c_1})=1-e^{-c_0}+e^{-c_1}=0.10\implies c_1-c_0=\ln0.90$

$(2)\implies c_0^{1/2}-c_0+\frac12\ln c_0=c_1^{1/2}-c_1+\frac12\ln c_1$, get $c_0=0.076546, c_1=3.637798$.

```{r eval=FALSE, include=FALSE}
# log(a) + log(b) = log(5)
# 0.5 * (loga + 2 * log(b)) = log(10)
m <- matrix(c(1, .5, 1, 1), 2)
exp(solve(m, c(log(5),log(10))))

-exp(-3.637798^0.5)+exp(-0.076546^0.5)
```

The test $\Lambda\le c_0=0.076546$,$\Lambda\ge c_1=3.637798$ is most powerful $\forall\theta_1\notin\Omega_0$, so it is a UMP level $\alpha=0.10$ test.

The Type II error probability is

$$P(c_0<Y<c_1|\theta_1)=\int_{c_0}^{c_1}\frac12e^{-y^{1/2}} y^{-1/2}dy=\left.-e^{-y^{1/2}}\right|_{c_0}^{c_1}=e^{-c_0^{1/2}}-e^{-c_1^{1/2}}=0.609824$$

## HW3

**8.28** Let $f(x|\theta)$ be the logistic location pdf

$$f(x|\theta)=\frac{e^{x-\theta}}{(1+e^{x-\theta})^2},-\infty<x<\infty,-\infty<\theta<\infty$$

(a) Show that this family has an MLR.

Let $\theta_2>\theta_1$,

$$\frac{f(x|\theta_2)}{f(x|\theta_1)}=\frac{\frac{e^{x-\theta_2}}{(1+e^{x-\theta_2})^2}}{\frac{e^{x-\theta_1}}{(1+e^{x-\theta_1})^2}}=e^{\theta_1-\theta_2}\left[\frac{1+e^{x-\theta_1}}{1+e^{x-\theta_2}}\right]^2$$

Take the derivative of the part in brackets.

$$\frac{d}{dx}(\frac{1+e^{x-\theta_1}}{1+e^{x-\theta_2}})=\frac{(1+e^{x-\theta_1})'(1+e^{x-\theta_2})-(1+e^{x-\theta_1})(1+e^{x-\theta_2})'}{(1+e^{x-\theta_2})^2}=\frac{e^{x-\theta_1}(1+e^{x-\theta_2})-(1+e^{x-\theta_1})e^{x-\theta_2}}{(1+e^{x-\theta_2})^2}=\frac{e^{x-\theta_1}-e^{x-\theta_2}}{(1+e^{x-\theta_2})^2}$$

For $\theta_2>\theta_1$, $\Lambda$ is a monotone function of $f(x|\theta)$. This family has a Monotone Likelihood ratio (MLR).

 ---

(b) Based on one observation, $X$, find the most powerful size $\alpha$ test of $H_0:\theta=0$ versus $H_1:\theta=1$. For $\alpha=.2$, find the size of the Type II Error.

The MLR $\frac{f(x|\theta_2)}{f(x|\theta_1)}=\frac{f(x|1)}{f(x|0)}$ is increasing in $x$. The best test is to reject $H_0$ when $\frac{f(x|1)}{f(x|0)}>k$, which is equivalent to rejecting if $x>k'$

$F(x|\theta)=\int_{-\infty}^{x}\frac{e^{x-\theta}}{(1+e^{x-\theta})^2}dx=\frac{e^{x-\theta}}{1+e^{x-\theta}}$

For $\alpha=1-F(k'|0)=1-\frac{e^{k'}}{1+e^{k'}}=\frac{1}{1+e^{k'}}=0.2$, 

$k'=\ln{\frac{1-\alpha}{\alpha}}=\ln{\frac{1-0.2}{0.2}}=1.386294$

Type II Error $\beta=F(k'|1)=\frac{e^{k'-1}}{1+e^{k'-1}}=0.5953902$

```{r eval=FALSE, include=FALSE}
log(4)
exp(1.386294-1)/(1+exp(1.386294-1))
```

 ---

(c) Show that the test in part (b) is UMP size $\alpha$ for testing $H_0:\theta\le0$ versus $H_1:\theta>1$. What can be said about UMP tests in general for the logistic location family?

From **Theorem 8.3.17 (Karlin-Rubin)** $H_0:\theta\le\theta_0$ versus $H_1:\theta>\theta_0$. T is a sufficient statistic for $\theta$ and the logistic location family of pdfs $\{g(t|\theta):\theta\in\Theta\}$ of $T$ has an MLR. Then for any $t_0$, the test that rejects $H_0$ if and only if $T>t_0$ is a UMP level $\alpha$ test, where $\alpha= P_{\theta_0}(T>t_0)$.

 ---
 
**8.33** Let $X_1,..,X_n$ be a random sample from the $uniform(\theta,\theta+1)$ distribution. $T_0$ test $H_0:\theta=0$ versus $H_1:\theta>0$, use the test reject $H_0$ if $Y_n\ge1$ or $Y_1\ge k$, where k is a constant, $Y_1= \min\{X_1,..,X_n\}, Y_n= \max\{X_1,..,X_n\}$.

(a) Determine k so that the test will have size $\alpha$.

$f_{X}(x|\theta)=\frac{1}{\theta+1-\theta}=1,\quad F_{X}(x|\theta)=\frac{x-\theta}{\theta+1-\theta}=x-\theta$

From theorems 5.4.4 and 5.6.6, the marginal pdf of $Y_{1},Y_{n}$, the joint pdf of $(Y_1,Y_n) are$

$f_{Y_{1}}(y_1|\theta)=\frac{n!}{(1-1)!(n-1)!}f_{X}(y_1|\theta)[F_{X}(y_1|\theta)]^{1-1}[1-F_{X}(y_1|\theta)]^{n-1}=n[1+\theta-y_1]^{n-1},\ \theta\le y_1\le\theta+1$

$f_{Y_{n}}(y_n|\theta)=\frac{n!}{(n-1)!(n-n)!}f_{X}(y_n|\theta)[F_{X}(y_n|\theta)]^{n-1}[1-F_{X}(y_n|\theta)]^{n-n}=n[y_n-\theta]^{n-1},\ \theta\le y_n\le\theta+1$

$f_{Y_{1},Y_{n}}(y_1,y_n|\theta)=\frac{n!f_{X}(y_1|\theta)f_{X}(y_n|\theta)}{(1-1)!(n-1-1)!(n-n)!}[F_{X}(y_1|\theta)]^{1-1}[F_{X}(y_n|\theta)-F_{X}(y_1|\theta)]^{n-1-1}[1-F_{X}(y_n|\theta)]^{n-n}$
$$=n(n-1)(y_n-y_1)^{n-2},\ \theta\le y_1<y_n\le\theta+1$$

Under $H_0:\theta=0, Y_n\le\theta+1=1$, $P(Y_n\ge1)=0$, 

$$\alpha=P(Y_1\ge k\cup Y_n\ge 1|\theta=0)=\int_k^1n[1+\theta-y_1]^{n-1}dy_1=(1-k)^n$$
Thus, use $k=1-\alpha^{1/n}, 0\le k\le1$ to have a size $\alpha$ test.

 ---
 
(b) Find an expression for the power function of the test in part (a) .

When $\theta+1\le k\le1$, all $Y_1$ and $Y_n$ locate outside the rejection region. $\beta(\theta)=P(\text{reject} H_0|H_1\text{true})=P(Y_1\ge k, Y_n\ge 1|\theta>0)=0$

When $0< k\le\theta$, all $Y_1$ and $Y_n$ locate inside the rejection region. $\beta(\theta)=P(\text{reject} H_0|H_1\text{true})=P(Y_1\ge k, Y_n\ge 1|\theta>0)=1$

When $k\le\theta+1\le1$, a part of $Y_1$ locate inside the $k\le Y_1$ rejection region. All $Y_n$ locate outside the $1\le Y_n$ rejection region.

$$\beta(\theta)=P(Y_1\ge k,Y_n\ge 1|\theta>0)=\int_k^{\theta+1}(\theta+1-y_1)^{n-1}dy_1=(\theta+1-k)^n$$

When $\theta\le k\le1\le\theta+1$, a part of $Y_1$ locate inside the $k\le Y_1\le\theta+1$ rejection region. All $Y_n$ locate outside the $1\le Y_n$ rejection region.

$$\beta(\theta)=P(k\le Y_1\le\theta+1|\theta>0)+P(\theta\le Y_1\le k,1\le Y_n\le\theta+1 |\theta>0)=\int_k^{\theta+1}(\theta+1-y_1)^{n-1}dy_1+\int_\theta^k\int_1^{\theta+1}n(n-1)(y_n-y_1)^{n-2}dy_ndy_1$$

$$=(\theta+1-k)^n+\int_\theta^kn[(\theta+1-y_1)^{n-1}-(1-y_1)^{n-1}]dy_1=(\theta+1-k)^n-(\theta+1-k)^n+1+(1-k)^n-(1-\theta)^{n}=\alpha+1-(1-\theta)^n$$

 ---
 
(c) Prove that the test is UMP size $\alpha$.
**Corollary 8.3.13** Consider the hypothesis problem posed in **Theorem 8.3.12**. Suppose $T(\vec X)$ is a sufficient statistic for $\theta$ and $g(t|\theta_i)$ is the pdf or pmf of $T$ corresponding to $\theta_i,i=0,1$. Then any test based on $T$ with rejection region $S$ (a subset of the sample space of T) is a UMP level a test if it satisfies 
$t\in S\ \text{ if }\ g(t|\theta_1)>kg(t|\theta)$ and $t\in S^c\ \text{ if }\ g(t|\theta_1)<kg(t|\theta)$
for some $k\ge 0$, where $\alpha=P_{\theta_0}(T\in S)$

$(Y_1, Y_n)$ are sufficient statistics.

For $0<\theta<1$, the ratio of pdfs is

$$\frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}=\begin{cases}0& \text{if }\ 0<y_1\le\theta,y_1<y_n<1 \\1& \text{if }\ \theta<y_1<y_n<1 \\\infty& \text{if }\ 1\le y_n<\theta,\theta<y_1<y_n \end{cases}$$

For $1\le\theta$, the ratio of pdfs is

$$\frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}=\begin{cases}0& \text{if }\ y_1<y_n<1 \\\infty& \text{if }\ \theta<y_1<y_n<\theta+1 \end{cases}$$

For $0<\theta<k$, use $k'=1$, 

$$\begin{cases}\text{reject}& \text{if }\ \frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}>1 \\\text{fail to reject}& \text{if }\ \frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}<1 \end{cases}$$

For $\theta\ge k$, use $k'=0$,

$$\begin{cases}\text{reject}& \text{if }\ \frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}>0 \\\text{fail to reject}& \text{if }\ \frac{f_{Y_{1},Y_{n}}(y_1,y_n|\theta)}{f_{Y_{1},Y_{n}}(y_1,y_n|0)}<0 \end{cases}$$

Form **Corollary 8.3.13**, $T(\vec X)$ is a sufficient statistic for $\theta$ and $g(t|\theta_i)$ is the pdf of $T$ corresponding to $\theta_i,i=0,1$.  if it satisfies 
$t\in S\ \text{ if }\ g(t|\theta_1)>kg(t|\theta)$ and $t\in S^c\ \text{ if }\ g(t|\theta_1)<kg(t|\theta)$

for some $k\ge 0$, where $\alpha=P_{\theta_0}(T\in S)$

Therefore, any test based on $T$ with rejection region $S$ is a UMP level a test.

(d) Find values of n and k so that the UMP .10 level test will have power at least .8 if $\theta>1$.

When $0< k\le\theta$, all $Y_1$ and $Y_n$ locate inside the rejection region, $\beta(\theta)=1>0.8$.

$$k=1-\alpha^{1/n}=1-0.1^{1/n}=0.9, 0.68377, 0.53584,...,n=1,2,3...$$

## HW4

1. Show $\sum C_id_i=0$


2. Repeat with $\hat\beta_0$, i.e. show $Cov(\hat\beta_0,\hat\varepsilon_j)=0$


## HW5

Show that $\hat x=\frac{by+x-ab}{1+b^2}; \hat y=a+\frac b{1+b^2}(by+x-ab)$



##

**7.30** The EM algorithm is useful in a variety of situation, and the definition of "missing data" can be stretched to accommodate many different models. Suppose that we have a mixture density $pf(x) + (1 p)g(x)$, where $p$ is unknown. If we observe $X = (X_1,..,X_n)$, the sample density is $\Pi_{i=1}^n[pf(x_i)+(1-p)g(x_i)$ which could be difficult to deal with. (Actually, a mixture of two is not terrible, but consider what the likelihood would look like with a mixture $L:=l P;Ji(X)$ for large k.)
The EM solution is to augment the observed (or incomplete) data with $Z (Z_1,..,Z_n)$, where $Z_i$ tells which component of the mixture $X_i$ came from; that is,

$X_i|z_i=1\sim f(X_i)$ and $X_i|z_i=0\sim g(x_i)$,and $P(Z_i=1)=p$

(a) Show that the joint density of $(X,Z)$ is given by $\sum^n_{i=1}[pf(x_i)^{z_i}][(1-p)g(x_i)^{1-z_i}]$.

Joint density $f(x,y)=f(x|y)f(y)$,

$$f(\vec x,\vec z)=f(\vec x|\vec z)f(\vec z)=\prod^n_{i=1}f(x_i|z_i)f(z_i)=\prod^n_{i=1}[pf(x_i)]^{z_i}[(1-p)g(x_i)]^{1-z_i}$$

(b) Show that the missing data distribution, the distribution of $Z_i|x_i,p$ is Bernoulli with success probability $\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$.

Conditional density $f(x|y)=\frac{f(x,y)}{f(y)}$,

$$P(Z_i=1|X_i,p)=\frac{f(x_i,1|p)}{f(x_i|p)}=\frac{pf(x_i)}{pf(x_i)+(1-p)g(x_i)}$$

(c) Calculate the expected complete-data log likelihood, and show that the EM sequence is given by

$$f(\vec x,\vec z)\propto p^{\sum^n_{i=1}z_i}(1-p)^{\sum^n_{i=1}(1-z_i)}$$

which is a Bernoulli joint pdf. For complete data,

$$\hat p=\frac1n\sum Z_i$$

Replacing the missing data with its expectation,

$$E(Z_i|X_i,p)=P(Z_i=1|X_i,p)$$

Through r times iteration,

$$\hat p^{(r+1)}=\frac1n\sum_{i=1}^n\frac{\hat p^{(r)}f(x_i)}{\hat p^{(r)}f(x_i)+(1-\hat p^{(r)})g(x_i)}$$