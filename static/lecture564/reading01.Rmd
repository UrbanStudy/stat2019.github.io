---
title: "STAT564"
subtitle: "Linear Regression"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document: 
    toc: false
    toc_float: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## {.tabset .tabset-fade .tabset-pills}

### Model 
response variable=parameters(coefficients).predictor+random error;

#### Simple Regression
$$y_i=\beta_0+\beta_1x_i+\varepsilon_i,\ i=1,2..n$$


#### Multiple Regression

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+..+\beta_kx_{ik}+\varepsilon_i=\beta_0+\sum_{\beta_j}x_{ij}+\varepsilon_i$$

$$\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots  \\ y_n 
 \end{bmatrix}_{n\times1} = 
 \begin{bmatrix}
  1 & x_{1,1} & x_{1,2} & \cdots & x_{1,k} \\
  1 & x_{2,1} & x_{2,2} & \cdots & x_{2,k} \\
  \vdots & \vdots  & \vdots  & \ddots & \vdots  \\
  1 & x_{n,1} & x_{n,2} & \cdots & x_{n,k} 
 \end{bmatrix}_{n\times(k+1)}
 \begin{bmatrix}
  \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots  \\ \beta_k 
 \end{bmatrix}_{(k+1)\times1}+
 \begin{bmatrix}
  \varepsilon_1 \\ \varepsilon_2 \\ \vdots  \\ \varepsilon_n 
 \end{bmatrix}_{n\times1}$$
$$\mathbf Y=\mathbf X \mathbf\beta+\mathbf\varepsilon, i=1,2,..n$$

where 0 is a $n\times1$ vector of zero and $\mathbf{I}$ is a $n\times n$ identity matrix.

#### Assumptions

Gauss Markov Theorem
 
- [1]linear relationship between x,y;
- [2]$E(\varepsilon)=0$, $Var(\varepsilon)=\sigma^2$
- [3]homoscedasticity
- [4]$Cov(\varepsilon_i,\varepsilon_j)=0 for i\ne{j}$
- [5]$\varepsilon_i\sim^{iid}N(0,\sigma^2\mathbf I)$



#### Sum

$$\sum_{i=1}^n(x_i)=n\bar x;\quad \sum_{i=1}^n(y_i)=\sum_{i=1}^n\hat y=n\bar y$$

$$\text{Corrected sum of squares}\quad S_{xx}=\sum_{i=1}^nx_i^2-\frac{(\sum_{i=1}^nx_i)^2}n=\sum_{i=1}^nx_i^2-n\bar x^2=\sum_{i=1}^nx_i(x_i-\bar x)=\sum_{i=1}^n(x_i-\bar x)^2$$

$$\text{Sample squares}\quad S_x^2=\frac{\sum_{i=1}^n(x_i-\bar x)^2}{n-1}=\frac{S_{xx}}{n-1}=Var(x)$$

$$c_i=\frac{x_i-\bar x}{S_{xx}};\quad \sum_{i=1}^nc_i=0;\quad \sum_{i=1}^nc_i^2=\frac{\sum_{i=1}^n(x_i-\bar x)^2}{S_{xx}^2}=\frac1{S_{xx}};\quad \sum_{i=1}^nc_ix_i=1$$

$$\text{Corrected sum of squares}\quad SST=S_{yy}=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ny_i^2-n\bar y^2=\sum_{i=1}^n(y_i-\hat y_i)^2+\sum_{i=1}^n(\hat y_i-\bar y)^2=SSR+SSE$$

$$$$

$$\text{Sample squares}\quad S_y^2=\frac{\sum_{i=1}^n(y_i-\bar y)^2}{n-1}=\frac{S_{yy}}{n-1}=Var(y)$$

$$SSR=\sum_{i=1}^n(\hat y_i-\bar y)^2=\sum_{i=1}^n(\hat\beta_0+\hat\beta_1x_i-\bar y)^2=\sum_{i=1}^n(\bar y-\bar x\hat\beta_1+\hat\beta_1x_i-\bar y)^2=\hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2=\hat\beta_1^2S_{xx}=\hat\beta_1S_{xy}$$

- Covariance

$$S_{xy}=\sum_{i=1}^nx_iy_i-(\sum_{i=1}^nx_i\sum_{i=1}^ny_i)/n=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)=\sum_{i=1}^ny_i(x_i-\bar x)=\sum_{i=1}^nx_i(y_i-\bar y)$$

$$Cov(x,y)=\sigma_{xy}=E[(x-\bar x)(y-\bar y)]=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}n=\frac{S_{xy}}n$$

$Cov(x,a)=0; Cov(x,x)=Var(x)\equiv\sigma^2(x)\equiv\sigma_x^2$

$Cov(x,y)=Cov(y,x); Cov(ax,by)=abCov(x,y)$

$Cov(x+a,y+b)=Cov(x,y)$

$Cov(a_1x_1+a_2x_2, b_1y_1+b_2y_2)=a_1b_1Cov(x_1,y_1)+a_1b_2Cov(x_1,y_2)+a_2b_1Cov(x_2,y_1)+a_2b_2Cov(x_2,y_2)$

$Cov(\hat\beta_0, \hatβ_1)=−\overline x\sigma^2 S_{xx}$

$Cov(\bar y,\hat\beta_1) = 0$

$$Cov(a'Y)=a'Cov(Y)a,\ Cov(AY)=A'Cov(Y)A$$

- Correlation Coefficient

$$|r|=\sqrt{R^2}=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{Cov(x,y)}{\sqrt{Var{x}Var{y}}}=\frac{Cov(x,y)}{S_xS_y}=\frac{Cov(\hat\beta_0,\hat\beta_1)}{se(\hat\beta_0)se(\hat\beta_1)}$$

- 2.6 Coefficient of Determination

If the fitted model is statistically significant, we expect

$$\text{Coefficient of Determination}\quad R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$$

 Proportion of variation in response(y) explained by the fitted model with predictor(s) $0\ge R^2\ge1$

-The coefficient of variation and correlation coefficient are related

$$\text{Adjusted}\ R^2=1-\frac{MSE}{MST}=1-\frac{SSE/{[n-(k-1)]}}{SST/(n-1)}$$

-Unfortunately, adding predictor(s) to the model does not decrease $R^2$. Therefore, it cannot be used to compare models with different number of predictors.


### Estimation 

#### Least-Suqares Estimation of Parameters (Coefficients)

`Assumption 2`

- Residual Sum of Squares

$$SSE=S(\hat\beta_0,\hat\beta_1)=\sum_{i=1}^n\hat\varepsilon_i=\sum_{i=1}^n(y_i-\hat y)=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}^n(y_i-(\bar y-\hat{\beta_1}\bar x)-\hat\beta_1x_i)^2$$

$$=\sum_{i=1}^n[(y_i-\bar y)-\hat{\beta_1}(x_i-\bar x)]^2=\sum_{i=1}^n(y_i-\bar y)^2-2\hat{\beta_1}\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)+\sum_{i=1}^n(x_i-\bar x)^2=S_{yy}-2\hat\beta_1^2S_{xy}+\hat\beta_1^2S_{xx}$$

$$=\mathbf{S_{yy}-\hat\beta_1^2S_{xx}}=S_{yy}-\hat\beta_1S_{xy}$$

$$=(Y-\hat Y)'(Y-\hat Y)=(Y-Xβ)'(Y-Xβ)=YY'-2Y'Xβ+β'X'Xβ=Y'(I-H)Y=Y'Y-\hatβ'X'Y$$

$$\text{symmetric}\quad H'=H;\quad (I-H)'=I-H$$

$$\text{idempotent}\quad HH=H;\quad (I-H)(I-H)=I-H$$

$$\left.\frac{\partial SSE}{\partial\beta_0}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-1)=0$$
$$\left.\frac{\partial SSE}{\partial\beta_1}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-x_i)=0$$

$$\frac{\partial{a'w}}{\partial{w}}=a;\quad \frac{\partial{w'Aw}}{\partial{w}}=2Aw$$

- Normal Equation

$$\sum_{i=1}^ny_i=n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i\implies \bar y=\hat\beta_0+\hat\beta_1\bar x$$

$$\sum_{i=1}^nx_iy_i=\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2;\quad \mathbf{X'Y=X'Xβ}$$

- Expected value

$$\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^n(x_i-\bar x)y_i}{S_{xx}}=\sum_{i=1}^nc_iy_i=(X'X)^{-1}X'Y$$
$$\hat{\beta_0}=\bar y-\hat{\beta_1}\bar x=\frac1n{\sum_{i=1}^ny_i}-\bar x(\sum_{i=1}^n c_iy_i)=\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i$$

'There seems to be a decreasing (negative) medium approximately linear relationship.'

'The average y increases by $\beta_1$ units as the x increases by 1 unit.'

$$\hat{y_i}=\hat\beta_0+\hat\beta_1x_i;\quad \mathbf{Y=X\hatβ=X(X'X)^{-1}X'Y=HY}$$

$$\mathbf{e=Y-\hat Y=Y-X\hatβ=(I-H)Y}$$

$$\hat{w}=rZ=\frac{\hat{y-\hat{y}}}{S_y}=r\frac{\hat{x-\hat{x}}}{S_x}$$

- Unbiased estimators of regression coefficients

$$E(\hat{\beta_1})=E[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_iE[\beta_0+\beta_1x_i+\varepsilon_i]=\beta_0\frac{\sum_{i-1}^n(x_i-\bar x)}{S_{xx}}+{\beta_1}\frac{\sum_{i-1}^n(x_i-\bar x)x_i}{S_{xx}}+0=\beta_1$$

$$E(\hat{\beta_0})=E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\bar xE[\hat\beta_1]=E[\frac{\sum_{i-1}^ny_i}n]-\bar x\hat\beta_1=\frac1n\sum_{i=1}^nE[\beta_0+\beta_1x_i+\varepsilon_i]-\bar x\hat\beta_1=\frac{n\beta_0}n+\frac{\beta_1}n\sum_{i=1}^nx_i+0-\bar x\hat\beta_1=\beta_0$$
$$E(\hatβ)=E((X'X)^{-1}X'Y)=(X'X)^{-1}X'E(Y)=(X'X)^{-1}X'Xβ=β$$

$$\mathbf{E(Y)=\mu1,\ E(a'Y)=a'E(Y)\ E(AY)=AE(Y)}$$

#### Variancesand Estimation of $\sigma^2$

As $n\rightarrow \infty,Var[\hat\beta_{0/1}]\rightarrow 0$;

$\hat\beta_{0/1}$ are consistant estimators for $\beta_{0/1}$;

$Var[ax,by]=a^2Var(x)+b^2Var(y)+2Cov(x,y)$
 
`Assumption 3``Assumption 4`

$$For\ {i}\ne{j}\quad Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0$$

$$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$$
$$\mathbf{Var(Y)=\sigma^2,\ Cov(Y)=\sigma^2I}$$

$$Var(\hat{\beta_1})=Var[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_i^2Var[y_i]=\frac{\sigma^2}{S_{xx}}=\frac{\sigma^2}{(n-1)S_x^2}$$

$$Var(\hat{\beta_0})=Var[\bar y-\hat\beta_1\bar x]=Var[\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i]=\sum_{i=1}^n(\frac1n-\bar{x}c_i)^2Var[y_i]$$
$$=\sigma^2\Big[\sum_{i=1}^n{\frac1{n^2}}-\frac{2\bar x}{n}\sum_{i=1}^nc_i+\bar x^2\sum_{i=1}^nc_i^2\Big]=\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})=\sigma^2(\frac1n+\frac{\bar x^2}{(n-1)S_x^2})$$

$$Cov(\hatβ)=\sigma^2(X'X)^{-1},\ Cov(\hatβ_{i},\hatβ_{j})=\sigma^2C_{ij},\ Var(\hatβ_{i-1})=\sigma^2C_{ii}$$

- Standard Deviation (Error)

$$se(\hat\beta_1)=\sqrt{Var(\hat\beta_1)}=\sqrt \frac{\hat\sigma^2}{S{xx}}$$
$$se(\hat{\beta_0})=\sqrt{Var(\hat\beta_0)}=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}$$

$$\frac{\sigma^2}{n}$$


#### Estimation of Variance


$$\sum_{i=1}^n\hat ye_i=\sum_{i=1}^nx_ie_i=0$$

$$\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)=S_{\varepsilon_i\varepsilon_i}=(n-1)S_\varepsilon^2;\quad E(S_{\varepsilon\varepsilon})=E(S_{\varepsilon}^2)=\sigma^2$$

$$\mathbf{S_{yy}}=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n[(\beta_0+\beta_1x_i+\varepsilon_i)-\frac1n\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon_i)]^2=\sum_{i=1}^n[\beta_1(x_i-\bar x)+(\varepsilon_i-\bar\varepsilon_i)]^2$$
$$=\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2+2\beta_1\sum_{i=1}^n(x_i-\bar x)(\varepsilon_i-\bar\varepsilon_i)+\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)^2$$

$$E[SST]=\beta_1^2S_{xx}+2\beta_1\sum_{i=1}^n(x_i-\bar x)(E[\varepsilon_i]-E[\bar\varepsilon_i])+E[(n-1)S_\varepsilon^2]=\mathbf{\beta_1^2S_{xx}+(n-1)\sigma^2}$$


$$E[SSR]=\sigma^2+\hat\beta_1^2S_{xx}$$

$$E[SSR]=k\sigma^2+β^{*'}X'_cX_cβ^{*}$$

$β^{*'}=[β_1 β_2..β_k]_{1\times k}$

$$X_c=\begin{bmatrix}
  x_{11}-\bar{x} & x_{12}-\bar{x} & \cdots & x_{1k}-\bar{x} \\
  x_{21}-\bar{x} & x_{22}-\bar{x} & \cdots & x_{2k}-\bar{x} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n1}-\bar{x} & x_{n2}-\bar{x} & \cdots & x_{nk}-\bar{x} 
 \end{bmatrix}_{n\times k}$$

$$E[\hat\beta_1^2]=Var(\hat\beta_1)-E[\hat\beta_1]^2=\frac{\sigma^2}{S_xx}+\beta_1^2$$

$$E[SSE]=E[\mathbf{S_{yy}}]-S_{xx}E[\hat\beta_1^2]=(n-2)\sigma^2$$

$$E[MSE]=\sigma^2=Var(y_i)=Var(\varepsilon)$$
MSE is an unbiased estimator for error variance

$$\hat\sigma^2=MSE=\frac{(Y-Xβ)'(Y-Xβ)}{dfE}=\frac{Y'(I-H)Y}{rank(I-H)}$$


#### Special Cases

- intercept known

$y=\beta_0+\hat\beta_1x+\varepsilon$;

$Let SSE=\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)^2$;

$\left.\frac{\partial SSE}{\partial\beta_1}\right|_{\hat\beta_1}=-2\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)x_i=0$;

$\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^n(y_i-\beta_0)x_i$;

$\hat\beta_1=\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}$;

$E(\hat\beta_1)=E\left[\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right]=\frac1{\sum_{i=1}^nx_i^2}E\left[\sum_{i=1}^nx_i(\beta_1x_i+\varepsilon)\right]=\frac{\beta_1\sum_{i=1}^nx_i^2}{\sum_{i=1}^nx_i^2}-\frac{\sum_{i=1}^nx_i}{\sum_{i=1}^nx_i^2}E[\varepsilon]=\beta_1$ is unbiased;

Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ 

Therefore $Var(\hat\beta_1)=Var\left(\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i-\sum_{i=1}^n\beta_0x_i)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i)=\frac{\sum_{i=1}^nx_i^2}{(\sum_{i=1}^nx_i^2)^2}Var(y_i)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$;

$se(\hat{\beta_1})=\sqrt{Var(\hat\beta_1)}=\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$

- slope known

$y=\hat\beta_0+\beta_1x+\varepsilon$

$Let SSE=\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)^2$

$\left.\frac{\partial SSE}{\partial\beta_0}\right|_{\hat\beta_0}=-2\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)=0$

$n\hat\beta_0=\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i$

$\hat\beta_0=\bar y-\beta_1\bar x$

$E(\hat\beta_0)=E\left[\frac1n\sum_{i=1}^ny_i-\frac1n\beta_1\sum_{i=1}^nx_i\right]=\frac1nE\left[\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon)-\beta_1\sum_{i=1}^nx_i\right]=\frac1n\sum_{i=1}^nE(\beta_0+\beta_1x_i)+E(\varepsilon)-\beta_1\sum_{i=1}^nx_i=\beta_0$  is unbiased

Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ 

Therefore $Var(\hat\beta_0)=\frac1{n^2}Var\left(\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i\right)=\frac1{n^2}\sum_{i=1}^nVar(y_i)=\frac{\sigma^2}n$

$se(\hat{\beta_0})=\sqrt{Var(\hat\beta_0)}=\sqrt{\frac{\sigma^2}n}=\sqrt{\frac{MSE}n}$


- 2.10 Regression through the origin

There are situations where we know that intercept of the simple linear regression model is zero. Then we fit the model with intercept,$\beta_0=0$ so the line goes through origin (0,0). 

$$y_i=\beta_1x_i+\epsilon_i\quad where\ \epsilon_i\sim iid N(0,\sigma^2)$$
Find the least squares estimator of slope ($\beta_1$) for the simple linear regression model through origin.

$$\hat\beta_1=\frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}$$

$$E(\hat\beta_1)=\beta_1$$

$$E(y_i)=\beta_0+\hat\beta_1x_i$$ is unbiased;
$$Var(\hat\beta_1)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$$

$$se(\hat{\beta_1})=\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$$

$$\hat{\beta_1}\pm t_\frac\alpha2se(\hat{\beta_1})$$

Where $t_\frac\alpha2$ is critical value from t distribution with df=n-1



### Test and inference 

#### Hypothesis Test

on the Slope and Intercept

`Assumption 5`

$$\hat{\beta_0}\sim N\left(\beta_0,\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})\right)\implies Z=\frac{\hat\beta_0-\beta_0}{\sqrt{\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}}\sim N(0,1)$$

$$\hat{\beta_1}\sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)\implies Z=\frac{\hat\beta_0-\beta_0}{\sqrt{\frac{\sigma^2}{S_{xx}}}}\sim N(0,1)$$

$$\hat{β}\sim N\left(β,\sigma^2(X'X)^{-1}\right)$$

`Assumption or distribution transform?`

Typically, $\sigma^2$ is unknown and is estimated using MSE (Mean Squared Error).

$$\frac{(n-2)\hat{\sigma}^2}{\sigma^2}\sim \chi^2_{(n-2)};\quad \frac{[n-(k+1)]MSE}{\sigma^2}=\frac{SSE}{\sigma^2}=\frac{Y'(I-H)Y}{\sigma^2}\sim \chi^2_{df=rank(I-H)=n-(k+1)}$$

- T Tests

$$H_0:\beta_1=c;\quad H_A:\beta_1\ne c$$

$$\frac{\hat{β_j}-β_j}{\sqrt{C_{jj}MSE}}=\frac{\hat{β_j}-β_j}{\sqrt{C_{jj}\hat\sigma^2}}\sim t_{df=dfE=n-(k+1)}$$

$$t_0=\frac{\hat{β_j}-c}{se(β_j)}=\frac{\hat{β_1}-c}{\sqrt{MSE/S_{xx}}}=\frac{\hat{β_j}-c}{\sqrt{C_{jj}MSE}}=\frac{\hat{β_j}-c}{\sqrt{C_{jj}\hat\sigma^2}}$$

Decision: Since the p-value$>(<)$significance level ($\alpha$=0.05), there is no enough evidence to conclude that the true slope is different from c (reject $H_0$ and conclude $H_1$ is true).

- Testing Significance of Regression

$$H_0:\beta_1=0;\quad H_A:\beta_1\ne0$$

$$t=\dfrac{\hat{\beta_1}}{se(\hat{\beta_1})}=\dfrac{\hat{\beta_1}}{\sqrt{MSE/\sum(x_i-\bar{x})^2}}$$

There is strong (not enoygh ) evidence that the true slope of fitted model is significantly different from zero at 5% significance level because p value is much smaller than 0.05;


#### Analysis of Variance

Partitioning the Total Variability in the response ($y_i$’s)

The total variability in the ’s is measured by

$$SST=\sum_{i=1}^n(y_i-\bar y)^2$$

Suppose we fit the least squares regression line which gives the fitted value $\hat y_i$. Then
$$y_i-\bar y=(y_i−\hat y_i)+(\hat y_i−y_i) \implies$$

|$\sum_{i=1}^n(y_i-\bar y)^2$|$\sum_{i=1}^n(y_i-\hat y_i)^2$|$\sum_{i=1}^n(\hat y_i-\bar y)^2$||
|---|---|---|---|
|SST=|SSE+|SSR|where SSR=$\hat\beta_1S_{xy}=\hat\beta_1^2S_{xx}$|
|dfT=|dfE+|dfR||
|n-1=|n-2+|2-1||

Similar to partitioning total variation in the response, the degrees of freedom also can be partition. The degrees of freedom (df) indicates the amount of information (number of data values) required to know if some other information is known.
For example, the df of SST gives the number of data values need to know if the mean of the data is known.

The mean square (MS) of each sum of square (SS) is computed by

$$MS=\frac{SS}{df}$$

and the mean square explains the average variation in each (total, regression or error) after taking sample size ( ) and number of regression parameters into account.

The ANOVA is useful when testing about the true slope in simple linear regression analysis.

$$H_0:\beta_1=0;\quad H_1:\beta_1\ne0$$

Appendix C.3 shows that

$$\dfrac{(n-2)\hat{\sigma}^2}{\sigma^2}=\dfrac{dfE\times MSE}{\sigma^2}=\dfrac{(n-2)MSE}{\sigma^2}=\dfrac{SSE}{\sigma^2}\sim \chi^2_{(n-2)}$$

and if $\beta_1=0$, then

$$\frac{dfR\times MSR}{\sigma^2}=\frac{SSR}{\sigma^2}\sim \chi^2_{(1)}$$

if $β^{*'}=0$, then

$$\frac{dfR\times MSR}{\sigma^2}=\frac{kMSR}{\sigma^2}=\dfrac{SSR}{\sigma^2}=\sim \chi^2_{df=k}$$

$$\because E(MSR)=\sigma^2+\beta_1^2S_{xx}=\sigma^2+\beta_1S_{xy}$$

Further, SSE and SSR are independent.

Therefore, by definition of F distribution

$$\frac{SSR/dfR}{SSE/dfE}=\frac{SSR/k}{SSE/[n-(k+1)]}=\frac{MSR}{MSE}\sim F_{(k,[n-(k+1)])}$$

$$SSE_{Full}=Y'Y-\hatβ'X'Y,\quad dfE_{Full}=n-(k+1)$$

$$H_0:\ Tβ=0;\quad H_1:\ Tβ\ne0$$
$$SSE_{Reduced}=\mathbf{Y'Y-\hatγ'Z'Y},\quad dfE_{Reduced}=n-(k+1-r)$$

$$\frac{(SSE_{Reduced}-SSE_{Full})/r}{SSE_{Full}/[n-(k+1)]}\sim F_{(r,[n-(k+1)])}$$

$dfE_{Reduced}-dfE_{Full}=n-(k+1-r)-[n-(k+1)]=r$

- additional (extra) sum of squares F test (partial F test), $dfE_{Reduced}-dfE_{Full}=r=1$

$$\frac{SSE_{Reduced}-SSE_{Full}}{MSE_{Full}}\sim F_{(1,[n-(k+1)])}$$

The F test statistic ($F_0$) is computed as shown in the ANOVA table for the testing significance of simple linear regression model.

Analysis of Variance (ANOVA) for Testing Significance of Simple Linear Regression

Source of Variation|Sum of Squars(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|P-value
---|---|---|---|---|---
Model(Regresion)|$SSR=\hat\beta_1S{xy}; \hatβX'Y-n\bar{y}^2$|1;k|$MSR=\frac{SSR}k$|$\frac{MSR}{MSE}$|$P(F>F_0)$
Error(Residual)|$SSE=SST-SSR=Y'(I-H)Y$|n-(k+1)|$MSE=\frac{SSE}{n-(k+1)}$||
Total|$SST=\sum_{i=1}^n(y_i-\bar y)^2=(Y-\bar{y}\mathbf{1})'(Y-\bar{y}\mathbf{1})$|n-1|||

If the test statistic is much larger than 1 ($F_0>F(\alpha,1,n-2)$) (or p-value $\le$ significance level), then the true slope is different from zero and hence least squares line is statistically significant.

There is strong evidence that the fitted model is statistically significant at 5\% significance level because p value is much smaller than 0.05


#### Confidence Interval

- $\beta_0$

 $100(1-\alpha)%$ Confidence Interval for the true slope $(\beta_1)$:

$$\hat{\beta_1}\pm t_{\alpha/2}se(\hat{\beta_1})=\hat{\beta_1}\pm t_{\alpha/2} \sqrt{\frac{\hat\sigma^2}{S_{xx}}}=\hat{\beta_1}\pm t_{\alpha/2,n-2}\times \sqrt{\frac{MSE}{\sum (x_i-\bar{x})^2}}=\hat{\beta_1}\pm t_{\alpha/2,n-2}\times \left(\frac{\sqrt{n}\hat{\sigma}}{\sqrt{n-2} \sqrt{\sum (x_i-\bar{x})^2}}\right)$$

- $\beta_1$
 $100(1-\alpha)%$ Confidence Interval for the true intercept $(\beta_0)$:

$$\hat{\beta_0}\pm t_{\alpha/2}se(\hat{\beta_0})=\hat{\beta_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=\hat{\beta_0}\pm t_{\alpha/2,n-2}\times \left(\sqrt{\dfrac{MSE}{n}}\right)=\hat{\beta_0}\pm t_{\alpha/2,n-2}\times \left(\sqrt{\dfrac{\hat{\sigma}^2}{n-2}}\right)$$

Since zero is (not) inside this interval, true intercept can (not) be zero with $1-\alpha$ confidence.

The 95% confidence interval (does not) contain zero. Therefore, we are 95% confidence that the true intercept is different from (can be) zero;

- $\sigma^2$

 $100(1-\alpha)%$ Confidence Interval for the true error variance $(\sigma^2)$:

$$\Bigg[\dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{\frac\alpha2}}, \dfrac{(n-2)\hat{\sigma}^2}{\chi^2_{1-\frac\alpha2}}\Bigg ]$$

 If $T\sim t(df=\upsilon)$, then $T^2\sim F(1,\upsilon)$

Therefore, test statistic ($t_0$) for testing true slope ($\beta_1$) different from zero and F test statistic ($F_0$) in ANOVA for simple linear regression are related as

$$t_0^2=\left(\frac{\hat\beta_1}{se(\hat\beta_1)}\right)^2=\frac{MSR}{MSE}=F_0$$

The t test has more flexibility than F test because,  

 t test can be used when testing $H_0:\ \beta_1=c$ versus $H_1:\ \beta_1\ne c$ where c is NOT zero while F test can be used only when c=0

 t test can be used when it is a one-tailed test (such as $H_1:\ \beta_1>c$ or $H_1:\ \beta_1<c$ while F test can be used for only two-tailed test.




#### Prediction

- 2.4.2 Interval Estimation of the Mean Response & 2.5 Prediction of New Observations

One of the goals in regression analysis is to predict the response (y) for a given value/s of predictor/s (x).  
Assume there are n pairs of observatin $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ to fit least squares simple linear regression model.    
Let’s say we observe k number of new values of response (y) at a new value of predictor (x) denoted by $x_0$ where $x_0$ between minimum and maximum observed values of predictor (x).  
Let $\bar y_0|x_0=\bar y_0$ be the true mean of k values of response at new value (x_0) of predictor.   
Then$\hat y_0|x_0=\hat y_0=\hat\beta_0+\hat\beta_1x_0$ is the estimated mean of k values of response at new value ($x_0$)) of predictor using the fitted least squares model.   
Therefore, $100(1−\alpha)%$ **confidence** interval for the **MEAN** response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

Therefore, $100(1−\alpha)%$ **prediction** interval for a **single value** of response at a given new value ($x_0$) of predictor is given by

$$\hat{y_0}\pm t_{\alpha/2} \sqrt{\hat\sigma^2(1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}})}=\hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}\sqrt{MSE\left[1+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$$

 Where $t_{\alpha/2}$ is the critical value from t distribution with (n-2) degrees of freedom

- Consider the random variable $\hat y_0-\bar y_0$ and find its expected value and variance.


We know $\hat\beta_1$ and $\hat\beta_1$ have normal distributions. Therefore,

$$\hat y=\hat\beta_0+\hat\beta_1x \sim N$$

Similarly,

$$\hat y_0|x_0=\hat y_0=\hat\beta_0+\hat\beta_1x_0 \sim N$$

$$\frac{\hat y_0|x_0-(\hat\beta_0+\hat\beta_1x_0)}{se(\hat y_0|x_0)}=\frac{\hat y_0-\bar y_0}{se(\hat y_0)}= \sim t_{df=n-2}$$

The figure below is an output graph from SAS for the scatterplot of data in Computer Lab 1 with overlaid fitted model, 95% confidence interval for mean response, and 95% prediction interval for a single new observation.

Be careful when predicting response for a given new value of predictor.

-Do not extrapolate!

estimated mean (point estimate) of response \textbf{(UNITS)} $\hat y_0=\hat{\beta_0}+\hat{\beta_1}x_0$ True mean $\bar y_0$
k number of new values of response at a new value of predictor denoted by $x_0$ \textbf{without extapolation}.
$se(\hat y_0)=\sqrt{MSE\left[(1)+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$;
100(1−$\alpha$) \% confidence interval for the MEAN (a single value of) response at a given new value $x_0$ of predictor is
$(\hat y_0) \hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}se(\hat y_0)$;
$E[\hat y_0-\bar y_0]=E[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=0$; 
$\hat y_0$ is unbiased estimator of $\bar y_0$;

$Var[\hat y_0-\bar y_0]=Var[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=Var[\bar y-\hat\beta_1\bar x+\hat{\beta_1}x_0-\frac1k\sum_{i=1}^k\varepsilon_i]$

$=Var[\frac{\sum_{i=1}^ny_i}n-\sum_{i=1}^nc_iy_i(\bar x-x_0)-\frac1k\sum_{i=1}^k\varepsilon_i]=Var\Big[\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]y_i\Big]+Var[\sum_{i=1}^k\frac1k\varepsilon_i]-2Cov[y_i,\varepsilon_i]$

Because $y_i$ and $\varepsilon_i$ are independent for new observations.
$=\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]^2Var(y_i)+\sum_{i=1}^k\frac1{k^2}Var(\varepsilon_i)=\sigma^2[\sum_{i=1}^n{\frac1{n^2}}-\frac{2\bar x}{n}\sum_{i=1}^nc_i+\bar x^2\sum_{i=1}^nc_i^2]=\sigma^2[\frac1n+\frac{(\bar x-x_0)^2}{S_{xx}}+\frac1k]$



### Example

Consider the following data set, in which the variables of interest are x=conmmuting distance(miles) and y=commuting time (min).  


x|5|10|15|20|25 
---|---|---|---|---|--- 
y|8|16|22|23|31 
$\hat y=4.1+1.06x$|9.4|14.7|20|25.3|30.6 
$e^2=(y-\hat y)^2$|1.96|1.69|4|5.29|0.16 


$$\hat\sigma^2=MSE=\frac{SSE}{n-2}=\frac{\sum_1^n(y-\hat y)^2}{n-2}=\frac{13.1}{5-2}=4.367 $$

(a) Compute the values of ANOVA table and test the significance of simple linear regression model for these data.

$$n=5,\quad \bar y=20,\quad \sum y_i^2=2294,\quad \hat\sigma^2=4.367,\quad S{xy}=265,\quad S{xx}=250,\quad \hat\beta_1=1.06$$


Source of Variation|Sum of Squars(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|P-value
---|---|---|---|---|---
Model(Regresion)|$\hat\beta_1S{xy}=1.06\times265=280.9$|1|$\frac{280.9}1$|$\frac{280.9}{4.367}=64.328$|$P(F>64.328)\approx0.01$
Error(Residual)|SST-SSR=294-280.9=13.1|n-2=5-2=3|$\frac{13.1}3=4.3667$||
Total|$\sum(y_i-\bar y)^2=\sum{y_i^2}-n\bar y^2=2294-5\times20^2=294$|n-1=4|||

At 5% significance level, the fitted model is significant because P-value<0.05.

#### Ex 2 Contd Consider the following data set, in which the variables of interest are x and y.

 - [a] Test whether the true slope of the simple linear regression model is significantly different from zero.

$$H_0:\ \beta_1=0,\quad H_1: \beta_1\ne0$$

$$Test\ statistic: t_0=\frac{\hat\beta_1-0}{se(\hat\beta_1)}=\frac{\hat\beta_1}{se(\hat\beta_1)}=\frac{1.06}{0.1322}=8.02$$

$$se(\hat\beta_1)=\sqrt \frac{\hat\sigma^2}{S{xx}}=\sqrt{\frac{4.367}{250}}=0.1322$$

$$df=n-2=5-2=3$$

$$P-value=2\times P(t>|t_0|)2\times0.0025=0.005$$

  P-value<0.05, therefore, ture slope is different from zero.

 - [b] Estimate the 95% confidence interval for the true slope of the simple linear regression model.

$$\because\ df=n-2=5-2=3,\quad t_{\frac\alpha2}=t_{0.025}=3.182$$


$$\hat{\beta_1}\pm t_{\alpha/2}se(\hat{\beta_1})=1.06\pm3.182\times0.1322=(0.639,1.481)$$

 - [c] Estimate the 95% confidence interval for the true intercept of the simple linear regression model.

$$se(\hat{\beta_0})=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}=4.367(\frac15+\frac{15^2}{250})=2.1917$$

$$\hat{\beta_0}\pm t_{\alpha/2}se(\hat{\beta_0})=4.1\pm3.182\times2.1917=(-2.874,11.074)$$

  Since zero is inside thsi interval, ture intercept ($\beta_0$) can be zero with 95% confidence.
