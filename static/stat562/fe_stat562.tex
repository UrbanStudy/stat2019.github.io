\documentclass[12pt,]{article}
\usepackage[]{mathpazo}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=0.5in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={STAT562 Final Exam},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{STAT562 Final Exam}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Winter 2019}


\begin{document}
\maketitle

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{$X_1,X_2,..X_n$ is a random sample from a distribution having a p.d.f of the form.
  $f(x)=\begin{cases}\lambda x^{\lambda-1}&0<x<1\\0&\text{otherwise}\end{cases}$
  Find a complete sufficient statistic for $\lambda$. Justify your answer}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Step1: Proof sufficient}
\end{itemize}

From \emph{Fisher--Neyman factorization theorem} (\texttt{2019-2-14p5})

\[f(x|\lambda)=L(\lambda)=\lambda^n(\prod x_i)^{\lambda-1}=\lambda^ne^{(\lambda-1)\sum^n_{i=1} \ln x_i}\cdot1=k(t|\lambda)h(\vec x)\]

\(h(\vec x)=1\) is free of \(\lambda\). So \(T=\sum^n_{i=1} \ln x_i\) is
a sufficient statistic for \(\lambda\).

\begin{itemize}
\tightlist
\item
  \textbf{Step2: Proof complete}
\end{itemize}

\(f(x|\lambda)\) is a member of the exponential family
(\texttt{2019-2-19p12}). By the Theorem of Complete Statistics in the
exponential family

\[f(x|\vec\lambda)=\lambda^ne^{\sum^n_{i=1} (\lambda-1)\ln x_i}=h(x)c(\vec \lambda)e^{\sum^k_{j=1}W_j(\vec \lambda)t_j(x)}\]

For pdf \(f(x)>0\) and \(x^{\lambda-1}>0\), \(\lambda>0\).
\(\{W_1(\vec \lambda),..,W_k(\vec \lambda)\}\) contains an open interval
in \(\Bbb R\), so \(T(\vec x)=\sum^n_{i=1} \ln x_i)\) is a complete
sufficient statistic for \(\lambda\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Let $Y_n$ be the $n^th$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Step1: \(\theta\) is a location parameter}
\end{itemize}

Let \(x=y-\theta\). For \(N(\theta,\sigma^2)\) is a location family of
densities (\texttt{2018.11.20p7}),

\(g(y|\theta)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(y-\theta)^2}{2\sigma^2}}=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{x^2}{2\sigma^2}}=f(x)=f(y-\theta)\)

Thus, \(\theta\) is a location parameter.

\begin{itemize}
\tightlist
\item
  \textbf{Step2: \(Y_n-\bar Y\) is location invariant}
\end{itemize}

For \(Y_n\sim N(\theta,\sigma^2)\), \(\bar Y\sim N(\theta,\sigma^2/n)\)
\texttt{2019-2-21p4-6}

Consider the group of transformations defined by
\(\mathcal{G}=\{Y_n-\bar Y,-\infty<\bar Y<\infty\}\),
\(Y_n+a-(\bar Y+a)=Y_n-\bar Y\).

Thus, the joint distribution of \(Y_n-\bar Y\) is in \(\mathcal{F}\) and
hence \(\mathcal{F}\) is invariant under \(\mathcal{G}\).

\begin{itemize}
\tightlist
\item
  \textbf{step3: \(Y_n-\bar Y\) is ancillary statistic for \(\theta\)}
\end{itemize}

\(f(y|\theta)\) is a location exponential family. Let \(X_n=Y_n-\theta\)
is a random sample from \(f(y|0)\) \texttt{2019-2-19p6}

\(Y_n-\bar Y=Y_n-\frac1{n}\sum_{i=1}^nY_i=(Y_n-\theta)-\frac1{n}\sum_{i=1}^n(Y_i-\theta)= X_n-\frac1{n}\sum_{i=1}^nX_i\)

\(Y_n-\bar Y\) is a function of only \(X_1,..,X_n\) and be free of
\(\theta\). It is an ancillary statistic for \(\theta\).

\begin{itemize}
\item
  \textbf{step4: For \(Y\sim N(\theta,\sigma^2)\), \(\bar Y\) is
  sufficient statistic for \(\theta\)}
\item
  \textbf{step5: \(\bar Y\) is complete statistic for \(\theta\)}
\end{itemize}

\(\bar Y\sim N(\theta,\sigma^2/n)\) is a member of the exponential
family. It is a complete sufficient statistic.

\begin{itemize}
\tightlist
\item
  \textbf{step6: By Basu's theorem, an acillary statistc \(Y_n-\bar Y\)
  and a complete sufficient statistic \(\bar Y\) are
  independent.}\texttt{2019-2-19p10}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Suppose that $X_1,X_2,..X_n\sim$ idd. $f(x|\theta)=\theta e^{-\theta x}, x>0$. Assume that the prior distribution of $\theta$ is $\pi(\theta)=\lambda e^{-\lambda\theta},\theta>0$}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}
\end{enumerate}

For
\(L(\theta)=\hat\theta e^{-\theta\sum x_i},\quad \pi(\theta)=\lambda e^{-\lambda\theta}\),
and the kernel of a function is the main part of the function, the part
that remains when constants are disregarded
(\texttt{2019-2-26p8-9,p11-p13} \texttt{2019-2-28p8\ Exapmle\ 2.3.8}).
that is

\(\pi(\theta|\vec x)\propto L(\theta)\pi(\theta)=\theta^n e^{-\theta\sum x_i}\lambda e^{-\lambda\theta}\propto\theta^{n+1-1}e^{-\theta(\lambda+\sum x_i)}\)

which is \(Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})\)
distribution.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}
\end{enumerate}

Suppose \(L_0(\hat\theta)=(\hat\theta-\theta)^2\). \texttt{2019-2-28p1}
\(E[L_0(\hat\theta)|\vec x]\) is minimized when
\[\hat\theta_{Bayes}=E[\theta|\vec x]=\alpha\beta=\frac{n+1}{\lambda+\sum x_i}\]
which is the posterior mean.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}
\end{enumerate}

\[\hat\theta_{Bayes}=\frac1{\frac1{n+1}(\lambda+n\bar x)}=\frac1{\frac1{n+1}(\frac1{1/\lambda}+\frac{n}{1/\bar x})}\]
which is the weighted hamonic mean of \(1/\lambda\), which is the prior
mean, and \(1/\bar x\), which is the MLE of \(\theta\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}
\end{enumerate}

\texttt{2019-2-28p4,9}

Suppose \(L_1(\hat\theta)=|\hat\theta-\theta|\).
\(E[|\hat\theta-\theta|]\) is minimized when
\[\hat\theta_{Bayes}=median[\theta|\vec x]\] For the median of Gamma
distribution doesn't have a closed form, the posterior median would not
have a closed form.

Postmedian \(\hat\theta=F^{-1}(\frac12)\) where \(F(x)\) is the
\(Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})\) cdf, for which
there is no closed form.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}
  \texttt{2019-2-28p5-6}
\end{enumerate}

Suppose
\(L_0(\hat\theta)=\begin{cases}0&\hat\theta=\theta\\1&\text{elsewhere}\end{cases}\).

\[E[L_0(\hat\theta)|\vec x]=0\cdot P[\theta=\hat\theta|\vec x]+1\cdot P[\theta\neq\hat\theta|\vec x]=P[\theta\neq\hat\theta|\vec x]=1-P[\theta=\hat\theta|\vec x]\]

To minimized this, maximize \(P[\theta=\hat\theta|\vec x]\)

When \(\hat\theta\) is the posterior mode, it is a Maximum A Posteriori
estimator. For the mode of Gamma distribution is \((\alpha-1)\beta\)

\[\hat\theta_{Bayes}=mode[\theta|\vec x]=(\alpha-1)\beta=\frac{n}{\lambda+\sum x_i}\]
which is the posterior mode.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Redo all of problem 3, using the non-informative prior $\pi(\theta)=1,\theta>0$. Note that this is not a valid density function since its intergral is infinite, but proceed with it anyway}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}
  \texttt{2019-2-26p8-9}
\end{enumerate}

For \(\pi(\theta)=1,\theta>0\),
\(L(\theta)=\theta^n e^{-\theta\sum x_i}\), from the kernel of function,

\[\pi(\theta|\vec x)\propto L(\theta)\pi(\theta)=\theta^n e^{-\theta\sum x_i}\sim Gamma(\alpha=n+1,\beta=\frac1{\sum x_i})\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}
\end{enumerate}

Suppose \(L_0(\hat\theta)=(\hat\theta-\theta)^2\).
\(E[L_0(\hat\theta)|\vec x]\) is minimized when
\[\hat\theta_{Bayes}=E[\theta|\vec x]=\alpha\beta=\frac{n+1}{\sum x_i}\]
which is the posterior mean.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}
\end{enumerate}

\[\hat\theta_{Bayes}=\frac1{\frac1{n+1}(1\times0+n\bar x)}=\lim_{c\to\infty}\frac1{\frac1{n+1}(\frac1{c}+\frac{n}{1/\bar x})}\]
which is the weighted hamonic mean of \(c\), which is the prior mean
when \(c\to\infty\), and \(1/\bar x\), which is the MLE of \(\theta\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}
\end{enumerate}

Suppose \(L_1(\hat\theta)=|\hat\theta-\theta|\).
\(E[|\hat\theta-\theta|]\) is minimized when
\[\hat\theta_{Bayes}=median[\theta|\vec x]\] For the median of Gamma
distribution doesn't have a closed form, the posterior median would not
have a closed form.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}
\end{enumerate}

Suppose
\(L_0(\hat\theta)=\begin{cases}0&\hat\theta=\theta\\1&\text{elsewhere}\end{cases}\).

\[E[L_0(\hat\theta)|\vec x]=0\cdot P[\theta=\hat\theta|\vec x]+1\cdot P[\theta\neq\hat\theta|\vec x]=P[\theta\neq\hat\theta|\vec x]=1-P[\theta=\hat\theta|\vec x]\]

To minimized this, maximize \(P[\theta=\hat\theta|\vec x]\)

When \(\hat\theta\) is the posterior mode, it is a Maximum A Posteriori
estimator. For the mode of Gamma distribution is \((\alpha-1)\beta\)

\[\hat\theta_{Bayes}=mode[\theta|\vec x]=(\alpha-1)\beta=\frac{n}{\sum x_i}=\frac{1}{\bar x}\]
which is the posterior mode.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Let $X_1,X_2,..X_n\sim$ idd. $f(x|\theta)=\theta x^{-\theta-1}, x_i>1, \theta>2$.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MLE}$, the maximum likelihood estimator of $\theta$.}
  \texttt{2019-2-2p12}
\end{enumerate}

\[f(\vec x|\theta)=L(\theta)=\theta^n(\prod x_i)^{\theta-1}=\theta^n e^{(-\theta-1)\sum^n_{i=1} \ln x_i}\]
\[l(\theta)=n\ln\theta-(\theta+1)\sum^n_{i=1} \ln x_i\]
\[l'(\theta)=\frac{n}\theta-\sum^n_{i=1} \ln x_i\overset{\text{set}}{=}0\]

\[\hat\theta_{MLE}=\frac{n}{\sum\ln x_i}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the expected value of $\hat\theta_{MLE}$.}
\end{enumerate}

Let \(Y_i=\ln x_i\), then \(X=e^y\), \(\frac{dx}{dy}e^y\)

\[g(Y)=\theta(e^y)^{-\theta-1}e^y=\theta e^{-y\theta}, y>0\]

So \(Y_i=\ln x_i\sim Gamma(\alpha=n,\beta=\frac1{\theta})\)

\[E[\hat\theta]=nE[Y^{-1}]=n\frac{\beta^{-1}\Gamma(-1+\alpha)}{\Gamma(\alpha)}=\frac{n\theta\Gamma(n-1)}{\Gamma(n)}=\frac{n\theta(n-2)!}{(n-1)!}=\frac{n\theta}{n-1}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{MLE}$.}
\end{enumerate}

\[E[\hat\theta^2]=n^2E[Y^{-2}]=n^2\frac{\beta^{-2}\Gamma(-2+\alpha)}{\Gamma(\alpha)}=\frac{n^2\theta^2\Gamma(n-2)}{\Gamma(n)}=\frac{n^2\theta^2(n-3)!}{(n-1)!}=\frac{n^2\theta^2}{(n-1)(n-2)}\]
\[Var[\hat\theta^2]=\frac{n^2\theta^2}{(n-1)(n-2)}-\frac{n^2\theta^2}{(n-1)^2}=\frac{n^2\theta^2}{(n-1)}[\frac1{n-2}-\frac1{n-1}]=\frac{n^2\theta^2}{(n-1)^2(n-2)}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.}
\end{enumerate}

\[\hat\theta_{U}=\frac{n-1}{n}\hat\theta_{MLE}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{U}$.}
\end{enumerate}

\[Var[\hat\theta_{U}]=(\frac{n-1}{n})^2\frac{n^2\theta^2}{(n-1)^2(n-2)}=\frac{\theta^2}{n-2}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Refer to problem 5.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MOM}$, the method of moments estimator of $\theta$.}
  \texttt{2019-2-21p8\ 7.2.1}
\end{enumerate}

\(EX=\mu=\int_1^{\infty}x\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+1}}{-\theta+1}\right|_1^{\infty}=\frac\theta{\theta-1}\)

Set
\(\bar X=\frac\theta{\theta-1}\implies\theta\bar x-\bar x=\theta\implies\theta(\bar x-1)=\bar x\),

\[\hat\theta_{MOM}=\frac{\bar x}{\bar x-1}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the expected value of $\hat\theta_{MOM}$.}
\end{enumerate}

For \(EX=\mu=\frac\theta{\theta-1}\)

\(E[X^2]=\int_1^{\infty}x^2\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+2}}{-\theta+2}\right|_1^{\infty}=\frac\theta{\theta-2}\)

\(Var[X]=\sigma^2=E[X^2]-E[X]^2=\frac\theta{\theta-2}-(\frac\theta{\theta-1})^2\)

\[=\frac{\theta(\theta-1)^2-\theta^2(\theta-2)}{(\theta-1)^2(\theta-2)}=\frac{\theta^3-2\theta^2+\theta-\theta^3+2\theta^2}{(\theta-1)^2(\theta-2)}=\frac{\theta}{(\theta-1)^2(\theta-2)}\]

Use a \(2^{nd}\) order Taylar series \texttt{2019-3-5p1}

\[g(x)=g(x_0)+g'(x_0)(x-x_0)+g''(x_0)\frac{(x-x_0)^2}2+R\]

Consider
\(g(x)=\frac{x}{x-1},\quad g'(x)=\frac{(x-1)\times1-x\times1}{(x-1)^2}=\frac{-1}{(x-1)^2},\quad g''(x)=\frac{2}{(x-1)^3}\)

Choose \(x_0=EX=\mu\)

\[g(x)\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(x-\mu)+\frac{2}{(\mu-1)^3}\frac{(x-\mu)^2}2\]

\[\hat\theta_{MOM}=\frac{\bar x}{\bar x-1}\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(\bar x-\mu)+\frac{1}{(\mu-1)^3}(\bar x-\mu)^2\]

\[E[\hat\theta_{MOM}]\approx \frac{\mu}{\mu-1}+0+\frac{1}{(\mu-1)^3}\frac{\sigma^2}n=\frac{\frac\theta{\theta-1}}{\frac\theta{\theta-1}-1}+\frac{1}{(\frac\theta{\theta-1}-1)^3}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\theta+\frac{\theta(\theta-1)}{n(\theta-2)}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the variance of $\hat\theta_{MOM}$.}
  \texttt{2019-3-5p3}
\end{enumerate}

\(Var[\hat\theta_{MOM}]\approx Var[g(x_0)+g'(x_0)(x-x_0)]=Var[\frac{\mu}{\mu-1}+\frac{1}{(\mu-1)^2}(\bar x-\mu)]\)
\[=\frac{1}{(\mu-1)^4}\frac{\sigma^2}n=\frac{1}{(\frac\theta{\theta-1}-1)^4}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\frac{\theta(\theta-1)^2}{n(\theta-2)}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{End}
\end{Highlighting}
\end{Shaded}


\end{document}
