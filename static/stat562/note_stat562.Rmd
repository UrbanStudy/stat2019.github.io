---
title: 'STAT562 Notes'
author: ""
date: "Winter 2019"
output: html_document
---

# {.tabset .tabset-fade .tabset-pills}

## 4. Multiple Random Variables

### 4.3 Bivariate Transformations

**Example 4.3.1 (Distribution of the sum of Poisson variables)** Let X and
Y be independent Poisson random variables with parameters $\theta$ and $\lambda$, respectively. Thus the joint pmf of (X,Y) is

$$f_{X,Y}(x,y)=\frac{\theta^xe^{-\theta}}{x!}\frac{\lambda^ye^{-\lambda}}{y!},\quad x=0,1,2,..,y=0,1,2,..$$

define $U=X+Y$ and $V=Y$

$$f_{U,V}(u,v)=f_{X,Y}(u-v,v)=\frac{\theta^{u-v}e^{-\theta}}{(u-v)!}\frac{\lambda^ve^{-\lambda}}{v!},\quad v=0,1,2,..,u=v,v+1,v+2,..$$

`2019.01.08`
`p.12`

Using Moment Generating Function (Theorem 4.2.12)

$$M_W(t)=M_X(t)M_Y(t)=e^{\mu_1(e^t-1)}e^{\mu_2(e^t-1)}=e^{(\mu_1+\mu_2)(e^t-1)}$$

**Theorem 4.3.2** if $X\sim Poisson(\theta)$ and $Y\sim Poisson(\lambda)$ and X and Y are indepedent, then $X+Y\sim Poisson(\theta+\lambda)$.

If (X,Y) is a continuous random vector with joint pdf $f_{X,Y}(x,y)$, then the joint pdf of (U, V) can be expressed in terms of $f_{X,Y}(x,y)$ in a manner analogous to (2.1.8). As before, $A=\{(x,y):f_{X,Y}(x,y)>0\}$ and $B=\{(u,v): u=g_1(x,y)\ \text{and}\ v=g_2(x,y)\ \text{for some}\ (x,y)\in A\}$. The joint pdf $f_{U,V}(u,v)$ will be positive on the set B. For the simplest version of this result we assume that the transformation $u=g_1(x,y)$ and $v=g_2(x,y)$ defines a one-to-one transformation of A onto B. The transformation is
onto because of the definition of B. We are assuming that for each $(u,v)\in B$ there is only one $(x,y)\in A$ such that $(u,v)=(g_1(x,y),g_2(x,y))$. For such a one-to-one, onto transformation, we can solve the equations $u=g_1(x,y)$ and $v=g_2(x,y)$ for x and y in terms of u and v. We will denote this inverse transformation by $x=h_1(u,v)$ and $y=h_2(u,v)$. The role played by a derivative in the univariate case is now played by a quantity called the Jacobian of the transformation. This function of (u,v), denoted by J, is the determinant of a matrix of partial derivatives. It is defined by

$$J=\begin{vmatrix}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial y}{\partial u}\frac{\partial x}{\partial v}$$
where

$$\frac{\partial x}{\partial u}=\frac{\partial h_1(u,v)}{\partial u},\ \frac{\partial x}{\partial v}=\frac{\partial h_1(u,v)}{\partial v},\ \frac{\partial y}{\partial u}=\frac{\partial h_2(u,v)}{\partial u},\ \frac{\partial y}{\partial v}=\frac{\partial h_2(u,v)}{\partial v}$$


We assume that J is not identically 0 on B. Then the joint pdf of (U,V) is 0 outside the set B and on the set B is given by

$$f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),\ h_2(u,v))|J|,$$

The "reproductive" property: adding independent r.v.s from a family of distribution produces a new r.v. from the same family.

`2019.01.10`
`p.1-5`

**A continuous example**: Let X and Y have joint pdf $f(x,y)=\frac14e^{-\frac{x+y}2}$, $0<x<\infty,0<y<\infty$. Find the pdf for $u=\frac{X-Y}2$.

Let $v=Y$ so that the system is invertible. $X=2u+v, Y=v$

$$J=\begin{vmatrix}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}=\begin{vmatrix}2 & 1 \\ 0 & 1 \end{vmatrix}=2$$

$$g(u,v)=f(x,y)|J|=\frac14e^{-\frac{x+y}2}2=\frac12e^{-\frac{2u+v+v}2}=\frac12e^{-(u+v)}$$

$$0<x<\infty,0<y<\infty\implies0<2u+v<\infty,0<v<\infty\implies v>-2u$$

$$g_U(u)=\left\{ \begin{array}\ \int_{-2u}^\infty\frac12e^{-(u+v)}dv=\frac12e^{-u}\int_{-2u}^\infty e^{-v}dv=\frac12e^{-u}\left[-e^{-v}\right]_{-2u}^\infty=\frac12e^{-u}\left[0+e^{2u}\right] & u<0 \\ \int_{0}^\infty\frac12e^{-(u+v)}dv=\frac12e^{-u}\int_{0}^\infty e^{-v}dv=\frac12e^{-u}\left[-e^{-v}\right]_{0}^\infty=\frac12e^{-u}\left[0+1\right] & u\ge0 \end{array} \right\}=\frac12e^{|u|}$$
which is Double Exponential or Laplace Distribution

`p.6-9`

**Example 4.3.6 (Distribution of the ratio of normal variables)** Let X and Y be independent n(O, 1 ) random variables. Consider the transformation $U =\frac{X}Y$ (*and V=|Y| in textbook*) find the pdf of u

$$\left.\begin{array}\mathcal{A_0}=\{(x,y):y=0\}\\ \mathcal{A_1}=\{(x,y):y>0\} \\ \mathcal{A_2}=\{(x,y):y<0\} \\ \end{array}\right\}\to\mathcal{B}=\{(u,v):v>0\}$$


Let $V=Y$, $Y=V$,$U=\frac XV$,$X=UV$

$$J=\begin{vmatrix}v & u \\ 0 & 1 \end{vmatrix}=v$$

$$g(u,v)=f(x,y)|J|=\frac1{\sqrt{2\pi}}e^{-\frac{x^2}2}\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}|V|=\frac1{2\pi}e^{-\frac{u^2v^2+v^2}2}|V|=\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}|V|$$

$$-\infty<x<\infty,-\infty<y<\infty\implies-\infty<uv<\infty,-\infty<v<\infty$$
Because the inegrated was an even function

$$g_U(u)=\int_{-\infty}^{\infty}\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}|V|dv=2\int_{0}^{\infty}\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}vdv$$

Let $s=\frac{(u^2+1)v^2}2$, $ds=(u^2+1)vdv$

$$g_U(u)=\frac1\pi\int_0^{\infty}e^{-s}\frac1{u^2+1}ds=\frac1\pi\frac1{u^2+1}\left[-e^{-s}\right]_0^\infty=\frac1\pi\frac1{u^2+1},-\infty<u<\infty$$
which is the Cauchy distribution.

**Theorem 4.3.5** Let X and Y be independent random variables. Let g(x) be a function only of a; and h(y) be a function only of y. Then the random variables U=g(X)and V=h(Y) are independent.

Proof

We will prove the theorem assuming U and V are continuous random variables. For any $u\in R,v\in R$,define

$$\begin{array}\mathcal{A_u}=\{x:g(x)\le u\}\\ \mathcal{B_v}=\{y:h(y)>v\}\end{array}$$

Then the joint cdf of (U,V) is

$$F_{U,V}(u,v)\begin{array}{l} =P(U\le u,V\le v) &  \text{definition of cdf} \\
= P(X\in A_u, Y\in B_v) & \text{definition of U and V} \\
= P(X\in A_u)P(Y\in B_v) & \text{Theorem 4.2.10} \end{array}$$

The joint pdf of (U,V) is
fj2 - fu, v{u, v) = 8u8v Fu,v {u, v)
= (:UP(X E Au)) (:VP(Y E Bv)) ,
(by (4.1 .4))
where, as the notation indicates, the first factor is a function only of u and the second
factor is a function only of v. Hence, by Lemma 4.2.7, U and V are independent.



`p.10-13`

**Example** $f(x,y)=x+y$, $0<x<1,0<y<1$, find $\rho_{XY}$

$\rho_{XY}=-\frac1{11}$

`p.14-17`

*What does actually measure?*

Suppose $E[Y|X]$ is a linear function of X, or suppose that $E[Y|X]=a+bx$, then

$$E[Y]=E[E[Y|X]]=E[a+bX]=a+bE[X]$$

Also,

Alternately,

$\rho_{XY}=b\frac{\sigma_X}{\sigma_Y}$




### 4.4 Hierarchical Models and Mixture Distributions

`2019.01.08`
`p.1`

$E[X]=\int_{-\infty}^{\infty}xf(x)dx$ is a real number.

$E[X|y]=\int_{-\infty}^{\infty}xf(x|y)dx$ is a function of y.

$E[X|Y]=\int_{-\infty}^{\infty}xf(x|y)dx$ is a random variable, and is a function of the r.v Y.

Definition: $V[X|Y]=E[(X-E[X|Y])^2|Y]$

`p.2-3`

**Theorem 4.4.3 Law of iterated Expectation** if X and Y are any two random variables, then

$$EX=E(E(X|Y))$$

provided that the expectation exist.

Proof:

$E(E(X|Y))=\int_{-\infty}^{\infty}E[X|y]g(y)dy=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}xf(x|y)dx\right]g(y)dy=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}x\frac{f(x,y)}{g(y)}dx\right]g(y)dy=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}{f(x,y)dy}\right]xdx=\int_{-\infty}^{\infty}f_X(x)xdx=E[X]$

**Theorem 4.4.7 (Conditional variance identity)** For any two random varibles X and Y,

`p.4-6`

$$VarX=E(Var(X|Y))+Var(E(X|Y))$$

provided that the expectation exist.

Proof:

$$V[X]=E[(X-\mu_x)^2]=E[(X-E[X|Y]+E[X|Y]-\mu_x)^2]=E[(X-E[X|Y])^2]+E[(E[X|Y]-\mu_x)^2]+2E[(X-E[X|Y])(E[X|Y]-\mu_x)]$$

$2E[(x-E[X|y])(E[X|y]-\mu_x)]=2\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-E[X|y])(E[X|y]-\mu_x)f(x,y)dxdy=2\int_{-\infty}^{\infty}(E[X|y]-\mu_x)\left\{\int_{-\infty}^{\infty}(x-E[X|y])f(x,y)dx\right\}dy=0$

$\int_{-\infty}^{\infty}(x-E[X|y])f(x,y)dx=\int_{-\infty}^{\infty}xf(x,y)dx-E[X|y]\int_{-\infty}^{\infty}f(x,y)dx=\int_{-\infty}^{\infty}xf(x|y)g(y)dx-E[X|y]g(y)=g(y)E[X|y]-E[X|y]g(y)$

(using law of iterated expcetations) $E[(X-E[X|Y])^2]=E[E(X-E[X|Y])^2|Y]=E(Var(X|Y))$ 

Let $W=E[X|Y]$, then $E[w]=E(E[X|Y])=E[X]=\mu_x$, $E[(E[X|Y]-\mu_x)^2]=E[(W-E[w])^2]=V[W]=V[E(X|Y)]$

### 4.5 Covariance and Correlation

`p.8`

**Definition 4.5.1** The covariance of X and Y is the number defined by

$$Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))=\sigma_{XY}$$

`p.10`

**Definition 4.5.2** The correlation of X and Y is the number defined by 

$$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y};\quad Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$

`p.9`
Alternate formate:


**Theorem 4.5.3** For any two random varibles X and Y,

$$\sigma_{XY}=Cov(X,Y)=E[XY]-\mu_X\mu_Y$$

$\sigma_{XY}=E[(X-\mu_X)(Y-\mu_Y)]=E[XY-Y\mu_X-X\mu_Y+\mu_X\mu_Y]=E[XY]-E[Y]\mu_X-E[X]\mu_Y+\mu_X\mu_Y=E[XY]-\mu_Y\mu_X-\mu_X\mu_Y+\mu_X\mu_Y=E[XY]-\mu_X\mu_Y$

$$\begin{array}{l} Cov(aX,bY)=abCov(X,Y) \\
Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)  \\
Cov(X,c)=0 \end{array}$$


**Theorem 4.5.5** If X and Y are independent (uncorrelated) random variables, then $Cov(X,Y)=0$ and $\rho_{XY}=0$



**Theorem 4.5.6** If X and Y are any two random variables and a and b are any two constants, then

$$Var(aX+bY)=a^2VarX+b^2VarY+2abCov(X,Y)$$
If X and Y are indepent random variables, then

$$Var(aX+bY)=a^2VarX+b^2VarY$$

**Theorem 4.5.7** For any random variables X and Y,

 a. $-1\le \rho_{XY}\le1$.
 b. $|\rho_{XY}|= 1$ if and only if there exist numbers $a\neq0$ and b such that $P(Y=aX+b)=1$. If $\rho_{XY}=1$, then $a>0$, and if $\rho_{XY}=-1$, then $a<0$.

**Definition 4.5.10** bivariate normal pdf

### 4.6 Multivariate Distributions

**Definition 4.6.2** multinomial distribution with m trials and cell probabilities

**Theorem 4.6.4 (Multinomial Theorem)**

**Definition 4.6.5** mutually independent random vectors

**Theorem 4.6.6 (Generalization of Theorem 4.2. 10)**

$$E(g_1(X_1)\cdots g_n(X_n))=(E(g_1(X_1))\cdots(E(g_n(X_n))$$

**Theorem 4.6.7 (Generalization of Theorem 4.2.12)**

$$M_Z(t)=(M_X(t))^n$$

**Theorem 4.6. 11 (Generalization of Lemma 4.2.1)** Let $X_1,.., X_n$ be random vectors. Then $X_1,.., X_n$ are mutually independent random vectors if and only if there exist functions $g_i(X_i),i=1,..,n$, such that the joint pdf or pmf of ($X_1,..,X_n$) can be written as

$$f(xI , . . . , xn) = Yl (Xl )· · · · ·Yn(xn)$$

**Theorem 4.6.12 (Generalization of Theorem 4.3.5)** Let $X_1,.., X_n$ be independent random vectors. Let $g_i(X_i)$ be a function only of $X_i,i=1,..,n$. Then the random variables $U_i=g_i(X_i),i=1,..,n$, are **mutually independent**.

**Example 4.6.13 (Multivariate change of variables)**

**Theorem 4.7.3 (Cauchy-Schwarz Inequality)** For any two random variables X and Y,

$$|EXY|\le E|XY|\le(E|X|^2)^{\frac12}(E|X|^2)^{\frac12}$$

**Example 4.7.4 (Covariance inequality)** If X and Y have means $\mu_X,\mu_Y$ and variances $\sigma_X^2,\sigma_Y^2$ , respectively, we can apply the Cauchy-Schwarz Inequality to
get 

$$E|(X-\mu_X)(Y-\mu_Y)|\le\{E(X-\mu_X)^2\}^{\frac12}\{E(Y-\mu_Y)^2\}^{\frac12}$$

Squaring both sides and using statistical notation, we have $(Cov(X, y))^2\le \sigma_X^2\sigma_Y^2$

Recalling the definition of the correlation coefficient,$\rho$, we have proved that $-1\le \rho^2\le1$1. Furthermore, the condition for equality in Lemma 4.7.1 still carries over, and equality is attained here only if $(X-\mu_X)=c(Y-\mu_Y)$, for some constant c. That is, the correlation is $\pm1$ **if and only if X and Y are linearly related**.

## 5. Properties of a Random Sample

### 5.2 Sums of Random Variables from a Random Sample

**Definition 5.2.1** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

**Definition 5.2.2** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

$$\bar X=\frac{X_1+..+X_n}{n}=\frac1n\sum_{i=1}^nX_i$$

**Definition 5.2.3** The sample variance is the statistic defined by

$$S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2=\frac1{n-1}\sum_{i=1}^n(X_i^2-n\bar X^2)$$
The sample standard deviation is the statistic defined by $S=\sqrt{S^2}$

**Theorem 5.2.4** $\sum_{i=1}^n(X_i-a)^2$ is minimized when $a=\bar x$


**Lemma 5.2.5** 

$$E\left(\sum_{i=1}^ng(X_i) \right)=nE(g(X_1))$$
$$Var\left(\sum_{i=1}^ng(X_i) \right)=nVar(g(X_1))$$

**Theorem 5.2.6** 

$$E\bar X=\mu$$
$$Var\bar X=\frac{\sigma^2}n$$
$$ES^2=\sigma^2$$

**Theorem 5.2.9 Convolution formula** If X and Y are independent continuous random variables with pdfs $f_X(x)$ and $f_Y(y)$, then the pdf of $Z=X+Y$ is
$$f_Z(z) =\int_{-\infty}^{\infty}f_X(w)f_Y(z-w)dw$$


### 5.3 Sampling from the Normal Distribution

#### 5.3.1 Properties of the Sample Mean and Variance

**Theorem 5.3.1**

#### 5.3.2 The Derived Distributions: Student's t and Snedecor's F

**Definition 5.3.4**


**Theorem 5.3.8**

### 5.4 Order Statistics

### 5.5 Convergence Concepts

#### 5.5.1 Convergence in Probability

#### 5.5.2 Almost Sure Convergence

#### 5.5.3 Convergence in Distribution

**Theorem 5.5.14 (Central Limit Theorem)** Let $X_1,X_2,..$ be a sequence of iid random variables whose mgfs exist in a neighborhood of 0 (that is, $M_{X_i}(t)$ exists for $|t|<h$, for some positive h). Let $EX_i =\mu$ and $VarX_i=\sigma^2>0$. (Both $\mu$ and $\sigma^2$ are finite since the mgf exists.) Define $\bar X_n=(\frac1n)\sum_{i=1}^nX_i$. Let $G_n(x)$ denote the cdf of $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$. Then, for any x, $-\infty< x <\infty$,

$$\lim_{n\to\infty}G_n(x)=\int_{-\infty}^x\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}dy$$

that is, $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$ has a limiting standard normal distribution.

**Theorem 5.5.17 (Slutsky's Theorem)** If $X_n\to X$ in distribution and $Y_n\to a$, a constant, in probability, then

 a. $Y_nX_n\to aX$ in distribution

 b. $X_n+Y_n\to X+a$ in distribution

#### 5.5.4 The Delta Method

## 6. Principles of Data Reduction

### 6.2 The Sufficiency Principle

#### 6.2.1 Sufficient Statistics

**Definition 6.2.1** A satistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$.


**Example 6.2.3 (Binomial sufficient statistic)**

**Example 6.2.4 (Normal sufficient statistic)**

**Example 6.2.5 (Sufficient order statistics)**

**Theorem 6.2.6 (Factorization Theorem)**

**Example 6.2.8 (Uniform sufficient statistic)**

**Example 6.2.9 (Normal sufficient statistic, both parameters unknown)**

#### 6.2.2 Minimal Sufficient Statistics

**Definition 6.2.11** A sufficient statistic $T(\mathbf{X})$ is called a minimal sufficient statistic if, for any other sufficient statistic $T'(\mathbf{X})$, $T(\mathbf{x})$ is a function of $T'(\mathbf{X})$.

**Definition 6.2.13** Let $f(x|\theta)$, be the pmf or pdf of a sample $\mathbf{X}$. Suppose there exists a function $T'(\mathbf{x})$ such that, for every two sample points x and y, the ratio $\frac{f(x|\theta)}{f(y|\theta)}$ is constant as a function of $\theta$ if and only if $T(\mathbf{x})=T(\mathbf{y})$, then $T(\mathbf{x})$ is a minimal sufficient statistic for $\theta$.


#### 6.2.3 Ancillary Statistics

**Definition 6.2.16** A satistic $S(\mathbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic.

#### 6.2.4 Sufficient, Ancillary, and Complete Statistics

**Theorem 6.2.24 (Basu's Theorem)** If $T(\mathbf{X})$ is a complete and minimal sufficient statistic, then $T(\mathbf{X})$ is independent of every ancillary statistic.


### 6.3 The Likelihood Principle

### 6.4 The Equivariance Principle

>Equivariance Principle: if $Y=g(\mathbf{X}$ is a change of measurement scale such that the model for $\mathbf{Y}$ has the same formal structure as the model for $\mathbf{X}$, then an inference procedure should be both measurement equivariant and formally equivariant.

## 7. Point Estimation

#### 7.2.1 Method of Moments

#### 7.2.2 Maximum Likelihood Estimators

**Theorem 7.2.10 (Invariance property of MLEs)** If $\hat\theta$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\tau(\theta)$ is $\tau(\hat\theta)$.


#### 7.2.3 Bayes Estimators


**Example 7.2.16 (Normal Bayes estimators)**

$$E(\theta|x)=\frac{\tau^2}{\tau^2+\sigma^2}x+\frac{\sigma^2}{\sigma^2+\tau^2},$$

$$Var(\theta|x)=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}$$

#### 7.2.4 The EM Algorithm

(Expectation-Maximization)

**Theorem 7.2.20 (Monotonic EM sequence)** The sequence {$\hat\theta_{(r)}$} defined by 7.2.20 satisfies

$$L\left(\hat\theta^{(r+1)}|y\right)\ge L\left(\hat\theta^{(r)}|y\right)$$

### 7.3 Methods of Evaluating Estimators

#### 7.3.1 Mean Squared Error

#### 7.3.2 Best Unbiased Estimators

**Corollary 7.3.10 (Cramer-Rao Inequality, iid case)**

#### 7.3.3 Sufficiency and Unbiasedness

#### 7.3.4 Losst Function Optimality

