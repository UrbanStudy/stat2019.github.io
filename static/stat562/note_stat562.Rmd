---
title: 'STAT562 Notes'
author: ""
date: "Winter 2019"
output: html_document
---

#  

## 4. Multiple Random Variables

### 4.3 Bivariate Transformations

**Example 4.3.1 (Distribution of the sum of Poisson variables)** Let X and
Y be independent Poisson random variables with parameters $\theta$ and $\lambda$, respectively. Thus the joint pmf of (X,Y) is

$$f_{X,Y}(x,y)=\frac{\theta^xe^{-\theta}}{x!}\frac{\lambda^ye^{-\lambda}}{y!},\quad x=0,1,2,..,y=0,1,2,..$$

define $U=X+Y$ and $V=Y$

$$f_{U,V}(u,v)=f_{X,Y}(u-v,v)=\frac{\theta^{u-v}e^{-\theta}}{(u-v)!}\frac{\lambda^ve^{-\lambda}}{v!},\quad v=0,1,2,..,u=v,v+1,v+2,..$$

**Theorem 4.3.2** if $X\sim Poisson(\theta)$ and $Y\sim Poisson(\lambda)$ and X and Y are indepedent, then $X+Y\sim Poisson(\theta+\lambda)$.

If (X,Y) is a continuous random vector with joint pdf $f_{X,Y}(x,y)$, then the joint pdf of (U, V) can be expressed in terms of $f_{X,Y}(x,y)$ in a manner analogous to (2.1.8). As before, $A=\{(x,y):f_{X,Y}(x,y)>0\}$ and $B=\{(u,v): u=g_1(x,y)\ \text{and}\ v=g_2(x,y)\ \text{for some}\ (x,y)\in A\}$. The joint pdf $f_{U,V}(u,v)$ will be positive on the set B. For the simplest version of this result we assume that the transformation $u=g_1(x,y)$ and $v=g_2(x,y)$ defines a one-to-one transformation of A onto B. The transformation is
onto because of the definition of B. We are assuming that for each $(u,v)\in B$ there is only one $(x,y)\in A$ such that $(u,v)=(g_1(x,y),g_2(x,y))$. For such a one-to-one, onto transformation, we can solve the equations $u=g_1(x,y)$ and $v=g_2(x,y)$ for x and y in terms of u and v. We will denote this inverse transformation by $x=h_1(u,v)$ and $y=h_2(u,v)$. The role played by a derivative in the univariate case is now played by a quantity called the Jacobian of the transformation. This function of (u,v), denoted by J, is the determinant of a matrix of partial derivatives. It is defined by

$$J=\begin{vmatrix}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial y}{\partial u}\frac{\partial x}{\partial v}$$
where

$$\frac{\partial x}{\partial u}=\frac{\partial h_1(u,v)}{\partial u},\ \frac{\partial x}{\partial v}=\frac{\partial h_1(u,v)}{\partial v},\ \frac{\partial y}{\partial u}=\frac{\partial h_2(u,v)}{\partial u},\ \frac{\partial y}{\partial v}=\frac{\partial h_2(u,v)}{\partial v}$$


We assume that J is not identically 0 on B. Then the joint pdf of (U,V) is 0 outside the set B and on the set B is given by

$$f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),\ h_2(u,v))|J|,$$

### 4.4 Hierarchical Models and Mixture Distributions

**Theorem 4.4.3** if X and Y are any two random variables, then

$$EX=E(E(X|Y)),$$

provided that the expectation exist.

**Theorem 4.4.7 (Conditional variance identity)** For any two random varibles X and Y,

$$VarX=E(Var(X|Y))+Var(E(X|Y))$$

provided that the expectation exist.

### 4.5 Covariance and Correlation

**Definition 4.5.1** The covariance of X and Y is the number defined by

$$Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))$$



**Definition 4.5.2** The correlation of X and Y is the number defined by 

$$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y};\quad Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$

**Theorem 4.5.3** For any two random varibles X and Y,

$$Cov(X,Y)=EXY-\mu_X\mu_Y$$
**Theorem 4.5.5** If X and Y are independent random variables, then $Cov(X,Y)=0$ and $\rho_{XY}=0$


**Theorem 4.5.6** If X and Y are any two random variables and a and b are any two constants, then

$$Var(aX+bY)=a^2VarX+b^2VarY+2abCov(X,Y)$$
If X and Y are indepent random variables, then

$$Var(aX+bY)=a^2VarX+b^2VarY$$

**Theorem 4.5.7** For any random variables X and Y,

 a. $-1\le \rho_{XY}\le1 $.
 b. $|\rho_{XY}|= 1$ if and only if there exist numbers $a\neq0$ and b such that $P(Y=aX+b)=1$. If $\rho_{XY}=1$, then $a>0$, and if $\rho_{XY}=-1$, then $a<0$.

**Definition 4.5.10** bivariate normal pdf

### 4.6 Multivariate Distributions

**Definition 4.6.2** multinomial distribution with m trials and cell probabilities

**Theorem 4.6.4 (Multinomial Theorem)**

**Definition 4.6.5** mutually independent random vectors

**Theorem 4.6.6 (Generalization of Theorem 4.2. 10)**

$$E(g_1(X_1)\cdots g_n(X_n))=(E(g_1(X_1))\cdots(E(g_n(X_n))$$

**Theorem 4.6.7 (Generalization of Theorem 4.2.12)**

$$M_Z(t)=(M_X(t))^n$$

## 5. Properties of a Random Sample

### 5.2 Sums of Random Variables from a Random Sample

**Definition 5.2.1** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

**Definition 5.2.2** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

$$\bar X=\frac{X_1+..+X_n}{n}=\frac1n\sum_{i=1}^nX_i$$

**Definition 5.2.3** The sample variance is the statistic defined by

$$S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$$
The sample standard deviation is the statistic defined by $S=\sqrt{S^2}$

**Theorem 5.2.4** $\sum_{i=1}^n(X_i-a)^2$ is minimized when $a=\bar x$


**Lemma 5.2.5** 

$$E\left(\sum_{i=1}^ng(X_i) \right)=nE(g(X_1))$$
$$Var\left(\sum_{i=1}^ng(X_i) \right)=nVar(g(X_1))$$

**Theorem 5.2.6** 

$$E\bar X=\mu$$
$$Var\bar X=\frac{\sigma^2}n$$
$$ES^2=\sigma^2$$

### 5.3 Sampling from the Normal Distribution

#### 5.3.1 Properties of the Sample Mean and Variance

**Theorem 5.3.1**

#### 5.3.2 The Derived Distributions: Student's t and Snedecor's F

**Definition 5.3.4**


**Theorem 5.3.8**

### 5.4 Order Statistics

### 5.5 Convergence Concepts

#### 5.5.1 Convergence in Probability

#### 5.5.2 Almost Sure Convergence

#### 5.5.3 Convergence in Distribution

**Theorem 5.5.14 (Central Limit Theorem)** Let $X_1,X_2,..$ be a sequence of iid random variables whose mgfs exist in a neighborhood of 0 (that is, $M_{X_i}(t)$ exists for $|t|<h$, for some positive h). Let $EX_i =\mu$ and $VarX_i=\sigma^2>0$. (Both $\mu$ and $\sigma^2$ are finite since the mgf exists.) Define $\bar X_n=(\frac1n)\sum_{i=1}^nX_i$. Let $G_n(x)$ denote the cdf of $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$. Then, for any x, $-\infty< x <\infty$,

$$\lim_{n\to\infty}G_n(x)=\int_{-\infty}^x\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}dy$$

that is, $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$ has a limiting standard normal distribution.

**Theorem 5.5.17 (Slutsky's Theorem)** If $X_n\to X$ in distribution and $Y_n\to a$, a constant, in probability, then

 a. $Y_nX_n\to aX$ in distribution

 b. $X_n+Y_n\to X+a$ in distribution

#### 5.5.4 The Delta Method

## 6. Principles of Data Reduction

### 6.2 The Sufficiency Principle

#### 6.2.1 Sufficient Statistics

**Definition 6.2.1** A satistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$.


**Example 6.2.3 (Binomial sufficient statistic)**

**Example 6.2.4 (Normal sufficient statistic)**

**Example 6.2.5 (Sufficient order statistics)**

**Theorem 6.2.6 (Factorization Theorem)**

**Example 6.2.8 (Uniform sufficient statistic)**

**Example 6.2.9 (Normal sufficient statistic, both parameters unknown)**

#### 6.2.2 Minimal Sufficient Statistics

**Definition 6.2.11** A sufficient statistic $T(\mathbf{X})$ is called a minimal sufficient statistic if, for any other sufficient statistic $T'(\mathbf{X})$, $T(\mathbf{x})$ is a function of $T'(\mathbf{X})$.

**Definition 6.2.13** Let $f(x|\theta)$, be the pmf or pdf of a sample $\mathbf{X}$. Suppose there exists a function $T'(\mathbf{x})$ such that, for every two sample points x and y, the ratio $\frac{f(x|\theta)}{f(y|\theta)}$ is constant as a function of $\theta$ if and only if $T(\mathbf{x})=T(\mathbf{y})$, then $T(\mathbf{x})$ is a minimal sufficient statistic for $\theta$.


#### 6.2.3 Ancillary Statistics

**Definition 6.2.16** A satistic $S(\mathbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic.

#### 6.2.4 Sufficient, Ancillary, and Complete Statistics

**Theorem 6.2.24 (Basu's Theorem)** If $T(\mathbf{X})$ is a complete and minimal sufficient statistic, then $T(\mathbf{X})$ is independent of every ancillary statistic.


### 6.3 The Likelihood Principle

### 6.4 The Equivariance Principle

>Equivariance Principle: if $Y=g(\mathbf{X}$ is a change of measurement scale such that the model for $\mathbf{Y}$ has the same formal structure as the model for $\mathbf{X}$, then an inference procedure should be both measurement equivariant and formally equivariant.

## 7. Point Estimation

#### 7.2.1 Method of Moments

#### 7.2.2 Maximum Likelihood Estimators

**Theorem 7.2.10 (Invariance property of MLEs)** If $\hat\theta$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\tau(\theta)$ is $\tau(\hat\theta)$.


#### 7.2.3 Bayes Estimators


**Example 7.2.16 (Normal Bayes estimators)**

$$E(\theta|x)=\frac{\tau^2}{\tau^2+\sigma^2}x+\frac{\sigma^2}{\sigma^2+\tau^2},$$

$$Var(\theta|x)=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}$$

#### 7.2.4 The EM Algorithm

(Expectation-Maximization)

**Theorem 7.2.20 (Monotonic EM sequence)** The sequence {$\hat\theta_{(r)}$} defined by 7.2.20 satisfies

$$L\left(\hat\theta^{(r+1)}|y\right)\ge L\left(\hat\theta^{(r)}|y\right)$$

### 7.3 Methods of Evaluating Estimators

#### 7.3.1 Mean Squared Error

#### 7.3.2 Best Unbiased Estimators

**Corollary 7.3.10 (Cramer-Rao Inequality, iid case)**

#### 7.3.3 Sufficiency and Unbiasedness

#### 7.3.4 Losst Function Optimality

