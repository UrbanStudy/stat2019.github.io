---
title: 'STAT562 Homework'
author: "Shen Qu"
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**4.4**  A pdf is defined by $f(x,y)=\begin{cases}C(x+2y)& 0<y<1,0<x<2 \\0 &otherwise\end{cases}$

 (a). Find the value of C.

$$\because \int_0^1\int_0^2f(x,y)dxdy=\int_0^1\int_0^2C(x+2y)dxdy=C\int_0^1\left[\frac{x^2}2+2yx\right]_0^2dy=C\int_0^1(2+4y)dy=\left.C(2y+2y^2)\right|_0^1=4C=1$$

$$\therefore C=\frac14$$

  ---

 (b). Find the marginal distribution of X.
 
 $$f_X(x)=\begin{cases}\int_0^1f(x,y)dy=\int_0^1\frac14(x+2y)dy=\frac14\left[xy+y^2\right]_0^1=\frac14(x+1)& 0<x<2\\ 0 & otherwise \end{cases}$$

  ---

 (c). Find the joint cdf of X and Y.
 
 let u=X, v=Y
 
 $$F_{XY}(x,y)=P(X\le x,Y\le y)=\begin{cases}0& x\le0,y\le0\\\\\int_0^x\int_0^yf(u,v)dvdu&0<y<1,0<x<2\\\\1& x\ge2,y\ge1 \end{cases}$$
 
$$=\begin{cases}0& y\le0,x\le0\\\int_0^x\int_0^y\frac14(u+2v)dvdu=\frac14\int_0^x[uv+v^2]_{v=0}^{y}du=\frac14\int_0^x(yu+y^2)du=\frac14[\frac{yu^2}2+y^2u]_{u=0}^x=\frac{x^2y}8+\frac{xy^2}4&0<y<1,0<x<2\\\int_0^2\int_0^y\frac14(u+2v)dvdu=\frac14\int_0^2[uv+v^2]_{v=0}^{y}du=\frac14\int_0^2(yu+y^2)du=\frac14[\frac{yu^2}2+y^2u]_{u=0}^2=\frac{y^2}2+\frac{y}2&0<y<1,2\le x\\\int_0^x\int_0^1\frac14(u+2v)dvdu=\frac14\int_0^x[uv+v^2]_{v=0}^{1}du=\frac14\int_0^x(u+1)du=\frac14[\frac{u^2}2+u]_{u=0}^x=\frac{x^2}8+\frac{x}{4}&1\le y,0<x<2 \\1& y\ge1,x\ge2\end{cases}$$

  ---

 (d). Find the pdf of the random variable $Z=9/(X + 1)^2$

For $Z=g(x)=9/(X + 1)^2$ is monotone when $0<x<2$, $X=g^{-1}(z)=3Z^{-\frac12}-1$, $1<z<9$, $d(g^{-1}(z))=-\frac32z^{-\frac32}$

$$f_Z(z)=\begin{cases}f_X(g^{-1}(z))|\frac{d}{dz}g^{-1}(z)|=\frac14(3z^{-\frac12}-1+1)|-\frac32z^{-\frac32}|=\frac98z^{-2}\quad1<z<9 \\0\qquad elsewhere\end{cases}$$

  ---

**4.11** Let U the number of trials needed to get the first head and V the number of trials needed to get two heads in repeated tosses of a fair coin. Are U and V independent random variables?

For $U\sim Geom(p=\frac12),u=1,2..$ and $V\sim N Bin(p=\frac12,r=2),v=2,3..$, the distribution of (U,V) is $\{(u,v):u=1,2,..;v=u+1,u+2,..\}$, which is not a cross-product set. Given $U=u$, $V>u$ is always true and U and V are not independent.


  ---

**4.16** Let X and Y be independent random variables with the same geometric distribution.

 (a). Show that U and V are independent, where U and V are defined by $U=\min(X,Y)$ and $V=X-Y$.

$$f_{U,V}(u,v)=P(U=u,V=v)$$

$$=\left\{\begin{array}{l} P(X=u+v,Y=u)=p(1-p)^{u+v-1}p(1-p)^{u-1}=p^2(1-p)^{2u+v-2} &\text{if } X>Y\\P(X=u,Y=u)=p^2(1-p)^{2(u-1)} &\text{if } X=Y \\P(X=u,Y=u-v)=p(1-p)^{u-1}p(1-p)^{u-v-1}=p^2(1-p)^{2u-v-2} &\text{if } X<Y\end{array}\right\}=(\frac{p}{1-p})^{2}(1-p)^{2u}(1-p)^{|v|},$$

$u=1,2..,v=0,\pm1,\pm2,..$

Since the joint pmf factors into a function of u and v, U and V are independent.

  ---

 (b). Find the distribution of $Z=X/(X+Y)$, where we define $Z=0$ if $X+Y=0$.

Suppose r and s are positve integers, $r<s$, and $\frac{r}s$ is in reduced form. Let $Z=\frac{X}{X+Y}=\frac{ir}{is},i=1,2..$, then $x=ir,y=i(s-r)$

$$P(Z=\frac{ir}{is})=\sum_{i=1}^{\infty}P(X=ir,Y=i(s-r))=\sum_{i=1}^{\infty}p(1-p)^{ir-1}p(1-p)^{i(s-r)-1}=(\frac{p}{1-p})^{2}\sum_{i=1}^{\infty}(1-p)^{is}$$

This is a geometric series with common ratio $0<(1-p)^{s}<1$, therefore

$$=(\frac{p}{1-p})^{2}\frac{(1-p)^{s}}{1-(1-p)^{s}}$$

  ---

 (c). Find the joint pdf of X and X + Y.

Let $U=X+Y$, then $Y=U-X$

$$P(X=x,U=x+y)=P(X=x,Y=u-x)=P(X=x)P(Y=u-x)=p(1-p)^{x-1}p(1-p)^{u-x-1}=(\frac{p}{1-p})^{2}(1-p)^{u}$$

  ---

**4.17** Let X be an exponential(1) random variable, and define Y to be the integer part of X + 1, that is $Y=i+1$ if and only if $i\le X<i+1, i=0,1,2,..$

(a) Find the distribution of Y . What well-known distribution does Y have?

For $Y=X+1$, $X=Y-1$

$$P(Y=i+1)=\int_{i}^{i+1}e^{-x}dx=\left.-e^{-x}\right|_{x=i}^{i+1}=-e^{-i-1}+e^{-i}=(1-\frac1e)(\frac1e)^i$$

Which is a Geometric distribution with $p=1-\frac1e,x=0,1,2,..$

  ---

(b) Find the conditional distribution of $X-4$ given $Y\ge5$.

For $f_X(x)=e^{-x}$, $F_X(x)=P(X\le x)=1-e^{-x}$ and $1-F_X(x)=P(X>x)=e^{-x}$

$$P(X-4\le x|Y\ge5)=P(X\le x-4|X+1\ge5)=1-P(X\ge x+4|X\ge4)$$

According to the 'memoryless' property of Exponential distribution,

$$=1-P(X\ge x|X\ge4)=1-P(X\ge x)=P(X\le x)=e^{-x},\quad x\ge4$$

  ---

**4.20** $X_1$ and $X_2$ are independent $n(0, \sigma^2)$ random variables.

 (a). Find the joint distribution of Y1 and Y2 , where $Y_1 = X_1^2 + X_2^2$ and $Y_2 =\frac{X_1}{\sqrt{Y_1}}$
 
Since $Y_2 =\frac{X_1}{\sqrt{Y_1}}=\frac{X_1}{\sqrt{X_1^2+ X_2^2}}=\pm\sqrt{1-\frac{X_2^2}{\sqrt{X_1^2+ X_2^2}}}$. Thus, $|y_2|<1$

To let the transformation between the points $(x_1,x_2)$ and $(y_1,y_2)$ is one-to-one, let

$$\left.\begin{array}\mathcal{A_0}=\{-\infty<x_1<\infty,x_2=0\}\\ \mathcal{A_1}=\{-\infty<x_1<\infty,x_2>0\} \\ \mathcal{A_2}=\{-\infty<x_1<\infty,x_2<0\} \\ \end{array}\right\}\to\mathcal{B}=\{0<y_1<\infty,|y_2|<1\}$$



$$f_{Y_1,Y_2}(y_1,y_2)=\sum_{i=1}^kf_{X_1,X_2}(h_1(y_1,y_2),\ h_2(y_1,y_2))|J_i|$$

$$h_1(y_1,y_2)=y_2\sqrt{y_1}$$
$$h_2(y_1,y_2)=\pm\sqrt{(1-y_2^2)y_1}$$

$$J_{1,2}=\begin{vmatrix}\frac{\partial h_1}{\partial y_1} & \frac{\partial h_2}{\partial y_2} \\ \frac{\partial h_1}{\partial y_1} & \frac{\partial h_2}{\partial y_2} \end{vmatrix}=\frac{\partial h_1}{\partial y_1}\frac{\partial h_2}{\partial y_2}-\frac{\partial h_2}{\partial y_1}\frac{\partial h_1}{\partial y_2}=\pm\frac12y_2y_1^{-\frac12}\cdot y_1^{\frac12}[\frac12(1-y_2^2)^{-\frac12}(-2y_2)]\mp \frac12y_1^{-\frac12}(1-y_2^2)^{\frac12}y_1^{\frac12}$$
$$=\mp\frac{y_2^2}{2(1-y_2^2)^{\frac12}}\mp \frac{1-y_2^2}{2(1-y_2^2)^{\frac12}}=\mp\frac12(1-y_2^2)^{-\frac12}$$

$$\therefore f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(h_1(y_1,y_2))\cdot f_{X_1,X_2}(h_2(y_1,y_2))(|J_1|+|J_2|)=\frac1{2\pi\sigma^2}e^{-\frac{y_2^2y_1+(1-y_2^2)y_1}{2\sigma^2}}(1-y_2^2)^{-\frac12}=\frac1{2\pi\sigma^2}e^{-\frac{y_1}{2\sigma^2}}(1-y_2^2)^{-\frac12}$$

$\quad 0<y_1<\infty,|y_2|<1$

  ---

 (b). Show that $Y_1$ and $Y_2$ are independent, and interpret this result geometrically.

The joint pdf, $f_{Y_1,Y_2}(y_1,y_2)$ factors into a function of $y_1$ and $y_2$. So $Y_1$ and $Y_2$ are independent. 


$Y_1=X_1^2+X_2^2$ is the suqared distance of hypotenuse in Pythagorean equation, which represents the square of the distance from $(X_1,X_2)$ to the origin. 

$Y_2=\frac{X_1}{\sqrt{Y_1}}$ is the ratio of the adjacent leg (the side of the triangle joining the angle to the right angle) to the hypotenuse, which represents the cosine of the angle between the positive $x_1$-axis and the line from $(X_1,X_2)$ to the origin. 

It is ture that the distance from the origin is independent of the orientation - the angle.


## HW2

**5.7** In Example 5.2. 10, a partial fraction decomposition is needed to derive the distribution of the sum of two independent Cauchy random variables. This exercise provides the details that are skipped in that example.

 (a). Find the constants A, B, C, and D that satisfy

$\frac1{1+(\frac{w}\sigma)^2} \frac1{1+(\frac{z-w}{\tau})^2}=\frac{Aw}{1+(\frac{w}\sigma)^2}+\frac{B}{1+(\frac{w}\sigma)^2}-\frac{Cw}{1+(\frac{z-w}{\tau})^2}-\frac{D}{1+(\frac{z-w}{\tau})^2}$, where A, B, C, and D may depend on z but not on w.






 (b). Using the facts that evaluate (5.2.4) and hence verify (5.2.5).
$\int\frac{t}{1+t^2}dt=\frac12\log(1+t^2)+$ constant and $\int\frac{1}{1+t^2}dt=\arctan(t)+$ constant
(Note that the integration in part (b) is quite delicate. Since the mean of a Cauchy does not exist, the integrals $\int_{-\infty}^{\infty}\frac{Aw}{1+(\frac{w}\sigma)^2}dw$ and $\int_{-\infty}^{\infty}\frac{Cw}{1+(\frac{z-w}{\tau})^2}dw$ dw do not exist.
However, the integral of the difference does exist, which is all that is needed.)

---

**5.10** Let $X_1,..,X_n$ be a random sample from a $n(\mu,\sigma^2)$ population.

 (c). Calculate $VarS^2$ a completely different (and easier) way: Use the fact that $(n-1)S^2/\sigma^2\sim\chi^2_{n-1}$

$\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{n-1}$


---

**5.13** Let $X_1,..,X_n$ be iid $n(\mu,\sigma^2)$. Find a function of $S^2$, the sample variance, say $g(S^2)$, that satisfies $Eg(S^2)=\sigma$. (Hint:Try $g(S^2)=c\sqrt{S^2}$, where c is a constant.)

---

**5.15** Establish the following recursion relations for means and variances. Let $\bar{X_n}$, and $S_n^2$ be the mean and variance, respectively, of $X_1,..,X_n$. Then suppose another observation, $X_{n+1}$ , becomes available. Show that

 (a). $\bar X_{n+1}=\frac{X_{n+1}-n\bar X_n}{n+1}$


 (b). $nS^2_{n+1}=(n-1)S_n^2+(\frac{n}{n+1})(X_{n+1}-X_n)$
 

## HW3

**5.22** Let X and Y be lid nCO, 1) random variables, and define Z = min(X, Y ) . Prove that Z2 '" XI

 ---

**5.24** Let Xl , . . . , Xn be a random sample from a population with pdf
fx(x) = {􀄋jO if 0 < x < ()
otherwise.
Let X(1) < . . . < X(n) be the order statistics. Show that X(l)j X(n) a.nd X(n) are
independent random variables.

 ---

**5.25** As a generalization of the previous exercise, let Xl , . . . , Xn be iid with pdf 

fx (x) = { a a - I:x if 0 < X < ()
otherwise.

Let X(l) < . . . < X(n) be the order statistics. Show that X(lJlX(2J> X(2J1X(3), 
X(n-lJl X(n), and X(n) are mutually independent random variables. Find the distribution of each of them.

## HW4

**5.35** Stirling's Formula (derived in Exercise 1.28), which gives an approximation for factorials, can be easily derived using the CLT.

 (a). Argue that, if Xi '" exponential ( I ) , i = 1 , 2, . . ., all independent, then for every x, 
 
 P (Xn - 1 )
I /Vn
:::; x ...... P (Z :::; x) ,

where Z is a standard normal random variable.

 (b). Show that differentiating both sides of the approximation in part (a) suggests

Vn
(xvn +n) ", - l e -C:z:vn+ n)  _1_ e -:Z:2/2 r(n) y'2;

and that x = 0 gives Stirling's Formula.

 ---

**5.39** This exercise, and the two following, will look at some o f the mathematical details of convergence.

 (a). Prove Theorem 5.5.4. (Hint: Since h is continuous, given e: > 0 we can find a 6
such that Ih(xn) - h(x)1 < e: whenever Ix" - xl < 6. Translate this into probability
statements. )

 (b). In Example 5.5.B, find a subsequence of the XiS that converges almost surely, that is, that converges p ointwise.
 
 ---

**5.41** Prove Theorem 5.5.13; that is, show that

P (IX" - J.LI > e) -t 0 for every e <=> P (X < x) -t {O f x < J.L
n- 1 1f x  J.L.

 (a). Set e: = Ix - J.LI and show that if x > J.L, then P(Xn :0:; x)  P(I Xn - J.LI :0:; e), while
if x < J.L, then P(X" :0:; x) :0:; P ( I X" - J.L I e) . Deduce the => implication.


 (b). Use the fact that {x : Ix J.LI > e:} ::: {x : x J.L < -e:} U {x : x - J.L > e} to deduce the *" implication.
 
(See Billingsley 1995, Section 25, for a detailed treatment of the above results.)
 
## HW5

**6.6** Let Xl , . . . , Xn be a random sample from a gamma(o, t3) population. Find a two dimensional sufficient statistic for (0 . t3) .

 ---

**6.9** For each of the following distributions let Xl , . . . , Xn be a random sample. Find a minimal sufficient statistic for e.

 (a). j(xIO) -00 < x < 00, -00 < 0 < 00  (normal)
 (b). j(xIB) = e-(x-6) , B < x < 00, -00 < 0 < 00  (location exponential)
 (c). j (xIO) =-00 < x < 00, -00 < e < 00 (logistic)
 (d). j (xI O)1f[1+(x-6)2] ,-00 < X < 00, -00 < B < 00 (Cauchy)
 (e). j(xIO)'2I e- lx-81 ,-00 < x < 00, -00 < B < 00 (double exponential)
 
 ---
 
**6.10** Show that the minimal sufficient statistic for the uniform(O, 0 + 1 ) , found in Example 6.2.15, is not complete.

 ---

**6.13** Suppose Xl and X2 are iid observations from the pdf j(xla) =, x > o, a >O. Show that (log XI )/(log X2) is an ancillary statistic.

## HW6

**6.30** Let Xl, . . . , Xn be a random sample from the pdf f(x ifJ.) = e-(x-!-') , where -00 < fJ. <x < 00.

(a). Show that X(l) = min; Xi is a complete sufficient statistic.

(b). Use Basu's Theorem to show that X{!) and 82 are independent.

 ---

**7.6** Let Xl , . . . , Xn be a random sample from the pdf

f(xIO) O -2 X , 0 < 0 􀀬 x < 00 .

 (a). What is a sufficient statistic for 87
 (b). Find the MLE of 8.
 (c). Find the method of moments estimator of O.

 ---

**7.10** The independent random variables X l , . . . , Xn have the common distribution

if x < 0
if O 􀃉 x 􀃉 jJ
if x > jJ,

where the parameters a and jJ are positive.

 (a). Find a tW<rdimensional sufficient statistic for (a, jJ).
 (b). Find the MLEs of a and jJ.
 (c). The length (in millimeters) of cuckoos' eggs found in hedge sparrow nests can be modeled with this distribution. For the data
 
22.0, 23.9, 20.9, 23.8, 25.0, 24.0, 21 .7, 23.8, 22.8, 23. 1, 23. 1 , 23.5, 23.0, 23.0,

find the MLEs of a and jJ.

 ---

**7.11** Let X! , . . . , X", be iid with pdf

f(xI9) = 9x9-t, 0 􀋁 X 􀁱 1 , 0 < (} < 00 .

 (a). Find the MLE of 8, and show that its variance --+ 0 as n --+ 00.

 (b). Find the method of moments estimator of (}.

## HW7

**5.59** Prove that the algorithm of Example 5.6.7 generates a beta( a, b) random variable.

 ---

**5.60** Generalize the algorithm of Example 5.6.7 to apply to any bounded pdf; that is, for an arbitrary bounded pdf f(x) on [a, b), define c = maxa:<;z:9 f (x) . Let X and Y be independent, with X '" uniform(a, b) and Y '" uniform(O, c). Let d be a number greater than b, and define a new random variable

W = {X if Y < f(X)
d if Y  f(X).

 (a). Show that pew 􀁱 w) = J: f(t)dt / [c(b - a)l for a 􀁱 w 􀁱 b.

 (b). Using part (a), explain how a random variable with pdf f(x) can be generated.

(Hint: Use a geometric argument; a picture will help.)


 ---

**7.22** This exercise will prove the assertions in Example 7.2.16, and more. Let Xl, . . . , X n be a random sample from a n(O, a2) population, and suppose that the prior distribution on 0 is n(#, 72). Here we assume that a2, #, and 72 are all known.

 (a). Find the joint pdf of X and 8. 
 
 (b). Show that m(xia2, #, 72), the marginal distribution of X, is n(#, (a2 In) + 72).

 (c). Show that 1T(Oix, a2, #, 72), the posterior distribution of 0 , i s normal with mean and variance given by (7.2.10).

 ---

**7.23** If 82 is the sample variance based on a sample of size n from a normal population, we know that (n - 1 ) 82/u2 has a X!-l distribution. The conjugate prior for u2 is the inverted gamma pdf, 10(a, {3), given by


where a and {3 are positive constants. Show that the posterior distribution of u2 is 10 (a + n;l , [(n_)s2 + 􀀺l-l ) . Find the mean of this distribution, the Bayes estimator of u2•


