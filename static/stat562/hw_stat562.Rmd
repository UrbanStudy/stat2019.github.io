---
title: 'STAT562 Homework'
author: ""
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**4.4**  A pdf is defined by 

f(x ) = c(x + 2y) if 0 <y< 1 and 0 < x < 2 , 

0 otherwise.

 (a). Find the value of C.
 (b). Find the marginal distribution of X.
 (c). Find the joint cdf of X a.nd Y.
 (d). Find the pdf of the random variable Z=9/(X + 1)^2

---

**4.11** Let U the number of trials needed to get the first head and V the number of trials needed to get two heads in repeated tosses of a fair coin. Are U and V independent random variables?


---

**4.16** Let X and Y be independent random variables with the same geometric distribution.

 (a). Show that U and V are independent, where U and V are defined by

U = min(X, Y ) an d V = X - Y.

 (b). Find the distribution of Z = X/(X + Y), where we define Z = 0 if X+Y=O.

 (c). Find the joint pdf of X and X + Y.

---

**4.17** Let X be an exponential( l ) random variable, and define Y to be the integer part of X + 1, that is

Y = i + l if and only if i S X < i + l , i = O, 1, 2, . . . .

(a) Find the distribution of Y . What well-known distribution does Y have?
(b) Find the conditional distribution of X -4 given $Y\ge5$.

---

**4.20** Xl and X2 are independent $n(0, \sigma^2)$ random variables.

 (a). Find the joint distribution of Y1 and Y2 , where
 
$Y_1 = X_1^2 + X_2^2$ and $Y_2 =\frac{X_1}{\sqrt{Y_1}}$

 (b). Show that Y1 and Y2 are independent, and interpret this result geometrically.


## HW2

**5.7** In Example 5.2. 10, a partial fraction decomposition is needed to derive the distribution of the sum of two independent Cauchy random variables. This exercise provides the details that are skipped in that example.

 (a). Find the constants A, B, C, and D that satisfy

1 1
1 + (W/U)2 1 + « z - w)/r)2
Aw B Cw
1 + (w/a)2 +
1 + (w/aF - 1 + « z 􀤃 w)/r)2

where A, B, C, and D may depend on z but not on w.

 (b). Using the facts that

D
1 + « z - w)/r)2 '
J 1 : t2 dt = 􀀦 log ( 1 + t2 ) + constant and J 􀁠
2 dt = arctan(t) + constant,
l + t

evaluate (5.2.4) and hence verify (5.2.5).

(Note that the integration in part (b) is quite delicate. Since the mean of a Cauchy does not exist, the integrals J::::' 1+(:/'0")2 dw and J􀄌oo 1+« .°:)/1')2 dw do not exist.
However, the integral of the difference does exist, which is all that is needed.)

---

**5.10** Let Xl , ... , Xn be a ra.ndom sample from a $n(\mu,\sigma2)$ population.

 (c). Calculate Var 82 a completely different (and easier) way: Use the fact that
(n 1)82/(72 '" X-l '

---

**5.13** Let Xl , . . . , X". be iid n(J1., (72). Find a function of 82, the sample variance, say g(82),
that satisfies Eg(82) (J". ( Hint: Try g(82) c..(S2, where c is a constant.)

---

**5.15** Establish the following recursion relations for means and variances. Let X.,. and 8! be the mean and variance, respectively, of Xl , . . . , Xn• Then suppose another observation, Xn+l , becomes available. Show that

 (a). X _ xn + l + n n+1 - X',,- n + 1


 (b). n8n+l - (n - 1)8.. + n + - 2 1 (Xn+1 - Xn) .
 

## HW3

**5.22** Let X and Y be lid nCO, 1) random variables, and define Z = min(X, Y ) . Prove that Z2 '" XI

---

**5.24** Let Xl , . . . , Xn be a random sample from a population with pdf
fx(x) = {􀄋jO if 0 < x < ()
otherwise.
Let X(1) < . . . < X(n) be the order statistics. Show that X(l)j X(n) a.nd X(n) are
independent random variables.

---

**5.25** As a generalization of the previous exercise, let Xl , . . . , Xn be iid with pdf 

fx (x) = { a a - I:x if 0 < X < ()
otherwise.

Let X(l) < . . . < X(n) be the order statistics. Show that X(lJlX(2J> X(2J1X(3), 
X(n-lJl X(n), and X(n) are mutually independent random variables. Find the distribution of each of them.

## HW4

**5.35** Stirling's Formula (derived in Exercise 1.28), which gives an approximation for factorials, can be easily derived using the CLT.

 (a). Argue that, if Xi '" exponential ( I ) , i = 1 , 2, . . ., all independent, then for every x, 
 
 P (Xn - 1 )
I /Vn
:::; x ...... P (Z :::; x) ,

where Z is a standard normal random variable.

 (b). Show that differentiating both sides of the approximation in part (a) suggests

Vn
(xvn +n) ", - l e -C:z:vn+ n)  _1_ e -:Z:2/2 r(n) y'2;

and that x = 0 gives Stirling's Formula.

---

**5.39** This exercise, and the two following, will look at some o f the mathematical details of convergence.

 (a). Prove Theorem 5.5.4. (Hint: Since h is continuous, given e: > 0 we can find a 6
such that Ih(xn) - h(x)1 < e: whenever Ix" - xl < 6. Translate this into probability
statements. )

 (b). In Example 5.5.B, find a subsequence of the XiS that converges almost surely, that is, that converges p ointwise.
 
---

**5.41** Prove Theorem 5.5.13; that is, show that

P (IX" - J.LI > e) -t 0 for every e <=> P (X < x) -t {O f x < J.L
n- 1 1f x  J.L.

 (a). Set e: = Ix - J.LI and show that if x > J.L, then P(Xn :0:; x)  P(I Xn - J.LI :0:; e), while
if x < J.L, then P(X" :0:; x) :0:; P ( I X" - J.L I e) . Deduce the => implication.


 (b). Use the fact that {x : Ix J.LI > e:} ::: {x : x J.L < -e:} U {x : x - J.L > e} to deduce the *" implication.
 
(See Billingsley 1995, Section 25, for a detailed treatment of the above results.)
 
## HW5

**6.6** Let Xl , . . . , Xn be a random sample from a gamma(o, t3) population. Find a two dimensional sufficient statistic for (0 . t3) .

---

**6.9** For each of the following distributions let Xl , . . . , Xn be a random sample. Find a minimal sufficient statistic for e.

 (a). j(xIO) -00 < x < 00, -00 < 0 < 00  (normal)
 (b). j(xIB) = e-(x-6) , B < x < 00, -00 < 0 < 00  (location exponential)
 (c). j (xIO) =-00 < x < 00, -00 < e < 00 (logistic)
 (d). j (xI O)1f[1+(x-6)2] ,-00 < X < 00, -00 < B < 00 (Cauchy)
 (e). j(xIO)'2I e- lx-81 ,-00 < x < 00, -00 < B < 00 (double exponential)
 
---
 
**6.10** Show that the minimal sufficient statistic for the uniform(O, 0 + 1 ) , found in Example 6.2.15, is not complete.

---

**6.13** Suppose Xl and X2 are iid observations from the pdf j(xla) =, x > o, a >O. Show that (log XI )/(log X2) is an ancillary statistic.

## HW6

**6.30** Let Xl, . . . , Xn be a random sample from the pdf f(x ifJ.) = e-(x-!-') , where -00 < fJ. <x < 00.

(a). Show that X(l) = min; Xi is a complete sufficient statistic.

(b). Use Basu's Theorem to show that X{!) and 82 are independent.

---

**7.6** Let Xl , . . . , Xn be a random sample from the pdf

f(xIO) O -2 X , 0 < 0 􀀬 x < 00 .

 (a). What is a sufficient statistic for 87
 (b). Find the MLE of 8.
 (c). Find the method of moments estimator of O.

---

**7.10** The independent random variables X l , . . . , Xn have the common distribution

if x < 0
if O 􀃉 x 􀃉 jJ
if x > jJ,

where the parameters a and jJ are positive.

 (a). Find a tW<rdimensional sufficient statistic for (a, jJ).
 (b). Find the MLEs of a and jJ.
 (c). The length (in millimeters) of cuckoos' eggs found in hedge sparrow nests can be modeled with this distribution. For the data
 
22.0, 23.9, 20.9, 23.8, 25.0, 24.0, 21 .7, 23.8, 22.8, 23. 1, 23. 1 , 23.5, 23.0, 23.0,

find the MLEs of a and jJ.

---

**7.11** Let X! , . . . , X", be iid with pdf

f(xI9) = 9x9-t, 0 􀋁 X 􀁱 1 , 0 < (} < 00 .

 (a). Find the MLE of 8, and show that its variance --+ 0 as n --+ 00.

 (b). Find the method of moments estimator of (}.

## HW7

**15.59** Prove that the algorithm of Example 5.6.7 generates a beta( a, b) random variable.

---

**5.60** Generalize the algorithm of Example 5.6.7 to apply to any bounded pdf; that is, for an arbitrary bounded pdf f(x) on [a, b), define c = maxa:<;z:9 f (x) . Let X and Y be independent, with X '" uniform(a, b) and Y '" uniform(O, c). Let d be a number greater than b, and define a new random variable

W = {X if Y < f(X)
d if Y  f(X).

 (a). Show that pew 􀁱 w) = J: f(t)dt / [c(b - a)l for a 􀁱 w 􀁱 b.

 (b). Using part (a), explain how a random variable with pdf f(x) can be generated.

(Hint: Use a geometric argument; a picture will help.)


---

**7.22** This exercise will prove the assertions in Example 7.2.16, and more. Let Xl, . . . , X n be a random sample from a n(O, a2) population, and suppose that the prior distribution on 0 is n(#, 72). Here we assume that a2, #, and 72 are all known.

 (a). Find the joint pdf of X and 8. 
 
 (b). Show that m(xia2, #, 72), the marginal distribution of X, is n(#, (a2 In) + 72).

 (c). Show that 1T(Oix, a2, #, 72), the posterior distribution of 0 , i s normal with mean and variance given by (7.2.10).

---

**7.23** If 82 is the sample variance based on a sample of size n from a normal population, we know that (n - 1 ) 82/u2 has a X!-l distribution. The conjugate prior for u2 is the inverted gamma pdf, 10(a, {3), given by


where a and {3 are positive constants. Show that the posterior distribution of u2 is 10 (a + n;l , [(n_)s2 + 􀀺l-l ) . Find the mean of this distribution, the Bayes estimator of u2•


