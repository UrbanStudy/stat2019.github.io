---
title: 'STAT551 Notes'
author: ""
date: "Fall 2018"
output: html_document
---

Applied Statistics for Engineers and Scientists

#  {.tabset .tabset-fade .tabset-pills}

## 1. Basic Statistical Concepts

### 1.2 Populations and Samples

In statistics the word population is used to denote the set of all objects or subjects relevant to the particular study that are exposed to the same treatment or method.
The members of a population are called population units.

Sampling refers to the process of selecting a number of population units and recording their characteristic(s).

### 1.6 Proportions, Averages, and Variances

#### 1.6.1 Population Proportion and Sample Proportion

The sample proportion $\hat p$ approximates but is, in general, different from
the population proportion p.

#### 1.6.2 Population Average and Sample Average

**Definition** the population average or population mean, denoted by μ, is simply the arithmetic average of all numerical values in the statistical population.

$$μ=\frac1N\sum_{i=1}^N\nu_i$$

**Definition** If the random variable X denotes the value of the variable of a randomly selected population unit, then a synonymous term for the population mean is expected value of X, or mean value of X, and is denoted by $μ_X$ or E(X).

**Definition** If a sample of size n is randomly selected from the population, and if $x_1,x_2,..,x_n$ denote the variable values corresponding to the sample units (note that a different symbol is used to denote the sample values), then the sample average or sample mean is simply 

$$\bar x=\frac1n\sum_{i=1}^nx_i$$

> The sample mean x approximates but is, in general, different from the population mean μ.

> A proportion is a special case of mean.

#### 1.6.3 Population Variance and Sample Variance

The population variance and standard deviation offer a quantification of the intrinsic variability of the population.

**Definition** of Population Variance $σ^2$, the variance of the random variable X, $σ^2_X$, or $Var(X)$.

$$\sigma^2=\frac1N\sum_{i=1}^N(\nu_i-\mu)^2=\frac1N\sum_{i=1}^N\nu_i^2-\mu^2$$

**Definition** The positive square root of the population variance is called the population standard deviation and is denoted by $σ$.

**Definition** If a sample of size n is randomly selected from the population, and if $x_1,x_2,..,x_n$ denote the variable values corresponding to the sample units, then the sample variance is

$$S^2=\frac1{n-1}\sum_{i=1}^n(x_i-\bar x)^2=\frac1{n-1}\left[\sum_{i=1}^nx_i^2-\frac1n(\sum_{i=1}^nx_i)^2\right]$$

> $S^2$ and $S$ approximate but are, in general, different from $σ^2$ and $σ$.

**Definition** the statistical parlance of degrees of freedom: Because the definition of $S^2$ involves the deviations of each observation from the sample mean, that is, $x_1−\bar x, x_2−\bar x,.., x_n−\bar x$, and because these deviations sum to zero, that is,
$$\sum_{i=1}^n(x_i-\bar x)=0$$
there are n−1 degrees of freedom, or independent quantities (deviations) that determine $S^2$. 

### 1.7 Medians, Percentiles, and Boxplots

**Definition** The 50th sample percentile is also called the sample median and is denoted by $\tilde x$; it is the value that separates the upper or largest 50% from the lower or smallest 50% of the data. The 25th, the 50th, and the 75th sample percentiles are also called sample quartiles, as they divide the sample into roughly four equal parts. We also refer to the 25th and the 75th sample percentiles as the lower sample quartile ($q_1$) and upper sample quartile ($q_3$),

**Definition 1.7-1** The sample interquartile range, or sample IQR, defined as
$IQR = q_3 − q_1$ is an estimator of the population IQR, which is a measure of variability.

**Definition 1.7-2** Let $x_{(1)}, x_{(2)},.., x_{(n)}$ denote the ordered sample values in a sample of size n. Then $x_{(i)}$, the $i$th smallest sample value, is taken to be the $100(\frac{i−0.5}n)$-th sample percentile. Sample percentiles estimate the corresponding population percentiles.

**Definition 1.7-3** Let $x_{(1)}, x_{(2)},.., x_{(n)}$ denote the order statistics. Then

1. The sample median is defined as
$$\tilde x=\begin{cases}x_{(\frac{n+1}2)} &\text{, if n is odd}\\\frac{x_{(\frac{n}2)}+x_{(\frac{n}2+1)}}2 &\text{, if n is even}\end{cases}$$

2. The sample lower quartile is defined as

q1 = Median of smaller half of the data values

where, if n is even the smaller half of the values consists of the smallest n/2 values, and if n is odd the smaller half consists of the smallest (n + 1)/2 values. Similarly, the sample upper quartile is defined as

q3 = Median of larger half of the data values

where, if n is even the larger half of the values consists of the largest n/2 values, and if n is odd the larger half consists of the largest (n+1)/2 values.

### 1.8 Comparative Studies

#### 1.8.3 Causation: Experiments and Observational Studies

Sample Effect of Level i in a One-Factor Design $\widehatα_i =\bar x_i −\bar x$

 - the sample effects estimate the k population effects $α_i =\mu_i −\mu$ where $\mu=\frac1k\sum_{i=1}^k\mu_i$

#### 1.8.4 Factorial Experiments: Main Effects and Interactions

**Definition 1.8-1** When a change in the level of factor A has different effects on the levels of factor B we say that there is interaction between the two factors. The absence of interaction is called additivity.

 - Main Row and Column Effects $α_i=μ_{i·}−\bar μ_{··}$, $β_j =\bar μ_{·j} −\bar μ_{··}$

Under additivity, the cell means μij are given in terms of their overall average, $μ_{··}$ and the main row and column effects in an additive manner:

 - Cell Means under Additivity $μ_{ij}=\bar μ_{··}+α_i+β_j$

When there is interaction between the two factors, the cell means are not given
by the additive relation (1.8.5). The discrepancy/difference between the left and
right-hand sides of this relation quantifies the interaction effects:

 - Interaction Effects $γ_{ij}=μ_{ij}−(\barμ_{··}+α_i+β_j)$

 - Sample Mean of Observations in Cell (i,j)
 
$$\bar x_{ij}=\frac1{n_{ij}}\sum_{k=1}^{n_{ij}}x_{ijk}$$

 - Sample Main Row and Column Effects $\widehatα_i=x_{i·}−\bar x_{··}$, $β_j =\bar x_{·j} −\bar x_{··}$

 - Sample Interaction Effects $\widehat{γ_{ij}}=x_{ij}−(\bar x_{··}+\widehatα_i+\widehatβ_j)$

## 2. Introduction to Probability

### 2.2 Sample Spaces, Events, and Set Operations

**Definition 2.2-1** The set of all possible outcomes of an experiment is called the sample space of the experiment and will be denoted by $S$.

Such subsets of the sample space (i.e., collections of individual outcomes) are called events. An event consisting of only one outcome is called a simple event. Events consisting of more than one outcome are called compound.

### 2.3 Experiments with Equally Likely Outcomes

#### 2.3.1 Definition and Interpretation of Probability

 - Probability of Each of N Equally Likely Outcomes
 
If the sample space of an experiment consists of N outcomes that are equally
likely to occur, then the probability of each outcome is 1/N.

 - Assignment of Probabilities in the Case of N Equally Likely Outcomes

$$P(E)=\frac{N(E)}N$$

#### 2.3.2 Counting Techniques

 - Fundamental Principle of Counting

If a task can be completed in two stages, if stage 1 has n1 outcomes, and if stage 2 has $n_2$ outcomes, regardless of the outcome in stage 1, then the task has $n_1n_2$ outcomes.

 - Generalized Fundamental Principle of Counting
 
 If a task can be completed in k stages and stage i has $n_i$ outcomes, regardless of the outcomes of the previous stages, then the task has $n_1n_2\cdots n_k$ outcomes.

**Definition 2.3-1** If the k stages of a task involve sampling one unit each, without replacement, from the same group of n units, then:
 1. If a distinction is made between the outcomes of the stages, we say the outcomes are ordered. Otherwise we say the outcomes are unordered.
 2. The ordered outcomes are called permutations of k units. The number of permutations of k units selected from a group of n units is denoted by $P_{k,n}$.
 3. The unordered outcomes are called combinations of k units. The number of combinations of k units selected from a group of n units is denoted by $\binom{n}{k}$

 - Number of Permutations of k Units Selected from n
 
$$P_{k,n}=n(n−1)\cdots(n−k+1)=\frac{n!}{(n−k)!}$$

 - Number of Permutations of n Units Among Themselves

$$P_{n,n}=n!$$

 - Number of Combinations of k Units Selected from n

$$\binom{n}k=\frac{P_{k,n}}{P_{k,k}}=\frac{n!}{k!(n−k)!}$$

Because the numbers $\binom{n}{k}$, $k=1,..,n$, are used in the binomial theorem, they are referred to as the binomial coefficients.

 - Number of Arrangements of n Units into r Groups of Sizes $n_1, n_2,..,n_r$

$$\binom{n}{n_1,n_2,..,n_r}=\frac{n!}{n_1!n_2!\cdots n_r!}$$

Because the numbers $\binom{n}{n_1,n_2,..,n_r}$, with $n_1+n_2+..+n_r=n$, are used in the
multinomial theorem, they are referred to as the multinomial coefficients.

#### 2.3.3 Probability Mass Functions and Simulations

**Definition 2.3-2** The probability mass function (PMF) of an experiment that records the value of a discrete random variable X, or simply the PMF of X, is a list of the probabilities $p(x)$ for each value x of the sample space $S_X$ of X.

This kind of sampling, which is random but not simple random, is called probability sampling, or sampling from a probability mass function. When the sample space is considered as the population from which we sample with replacement, it is called a sample space population.

### 2.4 Axioms and Properties of Probabilities

**Definition 2.4-1** For an experiment with sample space S, probability is a function that assigns a number, denoted by $P(E)$, to any event E so that the following axioms hold:

 - Axiom 1: $0\leP(E)\le1$
 - Axiom 2: $P(S) = 1$
 - Axiom 3: For any sequence of disjoint events $E_1,E_2,..$ the probability of
their union equals the sum of their probabilities, that is,

$$P\left(\bigcup_{i=1}^\infty E_i\right)=\sum_{i=1}^\infty P(E_i)$$

**Proposition 2.4-1** The three axioms imply the following properties of probability:
 1. The empty set,$\emptyset$, satisfies $P(\emptyset)=0$.
 2. For any finite collection, $E_1,..,E_m$, of disjoint events $P(E_1\cup E_2\cup\cdots\cup E_m)=P(E_1)+P(E_2)+ ..+P(E_m)$.
 3. If $A\subseteq B$ then $P(A)\le P(B)$.
 4. $P(A^c)=1−P(A)$, where $A^c$ is the complement of A.

**Proposition 2.4-2** The three axioms imply the following properties of probability:

 1. $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
 2. $P(A\cup B\cup C)=P(A)+P(B)+P(C)-P(A\cap B)-P(A\cap C)-P(B\cap C)+P(A\cap B\cap C)$

### 2.5 Conditional Probability

the conditional probability that B occurs given that A occurred and is denoted by $P(B|A)$.

 - Basic Principle for Conditional Probabilities

Given the information that an experiment with sample space $S$ resulted in the event $A$, conditional probabilities are calculated by

  + replacing the sample space $S$ by $A$, and
  + treating the outcomes of the new sample space $A$ as equally likely
 
**Definition 2.5-1** The definition of conditional probability states that for any two events A, B with $P(A)>0$,

$$P(B|A)=\frac{P(A\cap B)}{P(A)}$$

#### 2.5.1 The Multiplication Rule and Tree Diagrams

 - Multiplication Rule for Two Events
 
 $$P(A\cap B)=P(A)P(B|A)$$

 - Multiplication Rule for Three Events

$$P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B)$$

#### 2.5.2 Law of Total Probability and Bayes’ Theorem

 - Law of Total Probability

$$P(B)=P(A_1)P(B|A_1)+..+P(A_k)P(B|A_k)$$

**REMARK 2.5-1** The number of the events $A_i$ in the partition may also be (countably) infinite. Thus, if $A_i\cap A_j=\emptyset$, for all $i\neq j$, and $\bigcup_{i=1}^\infty A_i=S$, the Law of Total Probability states that

$$P(B)=\sum_{i=1}^{\infty}P(A_i)P(B|A_i)$$

holds for any event B.

 - Bayes’ Theorem

$$P(A_j|B)=\frac{P(A_j)P(B|A_j)}{\sum^k_{i=1}P(A_i)P(B|A_i)}$$

### 2.6 Independent Events

**Definition 2.6-1** Events A and B are called independent if
$$P(A\cap B) = P(A)P(B)$$

If events A and B are not independent, then they are dependent.

A similar argument implies that, if $P(B)>0$, Definition 2.6-1 is equivalent to $P(A|B) = P(A)$.

**Proposition 2.6-1**
 1. If A and B are independent, then so are $A^c$ and $B$.
 2. The empty set, $\emptyset$, and the sample space, S, are independent from any other set.
 3. Disjoint events are not independent unless the probability of one of them is zero.

It would appear that events $E_1,E_2,E_3$ are independent if $E_1$ is independent from
$E_2$ and $E_3$, and $E_2$ is independent from $E_3$ or, in mathematical notation, if

$$P(E_1\cap E_2) = P(E_1)P(E_2),\ P(E_1\cap E_3) = P(E_1)P(E_3),\ P(E_2\cap E_3) = P(E_2)P(E_3)$$

However, this pairwise independence does not imply

$$P(E_1\cap E_2\cap E_3)=P(E_1)P(E_2)P(E_3)$$

**Definition 2.6-2** Independence of three events. The events $E_1,E_2,E_3$ are (mutually) independent
if all three relations in (2.6.3) hold and (2.6.4) holds.

$$P(E_{i1}\cap E_{i2}\cap\cdots E_{ik}) =P(E_{i1})P(E_{i2})\cdots P(E_{ik})$$


#### 2.6.1 Applications to System Reliability

A series system has no redundancy in the sense that it functions only if all its components function.Aparallel system has the maximum possible redundancy since it functions if at least one of its components functions. A k-out-of-n system functions if at least k of its n components functions.

## 3. Random Variables and Their Distributions

### 3.2 Describing a Probability Distribution

#### 3.2.1 Random Variables, Revisited

**Definition 3.2-1** A random variable is a function (or rule) that associates a number with each
outcome of the sample space of a random experiment.

**Definition 3.2-2** A discrete random variable is a random variable whose sample space has a finite
or at most a countably infinite number of values.

**Definition 3.2-3** A randomvariable X is called continuous if it can take any value within a finite
or infinite interval of the real number line $(-\infty,\infty)$.

#### 3.2.2 The Cumulative Distribution Function

**Definition 3.2-4** The cumulative distribution function, or CDF, of a randomvariable X gives the
probability of events of the form $[X\le x]$, for all numbers x.

The CDF of a random variable X is typically denoted by a capital letter, most often F in this book. Thus, in mathematical notation, the CDF of X is defnined as

$$F_X(x) = P(X\le x)$$

**Proposition 3.2-1** The cumulative distribution function, F, of any (i.e., discrete or continuous) random
variable X satisfies the following basic properties:

 1. It is non-decreasing: If $a\le b$ then $F(a)\le F(b)$.
 2. $F(-\infty)=0, F(\infty)=1$.
 3. If $a<b$ then $P(a<X\le b)=F(b)−F(a)$.

**Proposition 3.2-2** Let $x_1<x_2<\cdots$denote the possible values of the discrete random variable X
arranged in an increasing order. Then

 1. F is a step function with jumps occurring only at the values x of $S_X$, while the flat regions of F correspond to regions where X takes no values. The size of the jump at each x of $S_X$ equals $p(x)=P(X=x)$.
 2. The CDF can be obtained from the PMF through the formula
 
$$F(x) =\sum_{x_i\le x}p(x_i)$$

 3. The PMF can be obtained from the CDF as $p(x_1)=F(x_1)$, and $p(x_i)=F(x_i)−F(x_i−1)$ for $i=2,3,\cdots$
 4. The probability of events of the form $[a<X\le b]$ is given in terms of the PMF as
$P(a<X\le b)=\sum_{a<x_i\le b}p(x_i)$, and in terms of the CDF as $P(a<X\le b)=F(b)−F(a)$.

In view of part (2) of Proposition 3.2-2, the CDF property $F(\infty) = 1$ (see property 2 of Proposition 3.2-1) can be restated as $\sum_{x_i\in S_X}p(x_i)=1$, (3.2.7)

that is, the values of the PMF sum to 1. Of course, (3.2.7) can be independently justified in terms of Axiom 2.4.2 of probabilities.

#### 3.2.3 The Density Function of a Continuous Distribution Exercises

**Definition 3.2-5** Uniform in [0, 1] random variable. Select a number from [0, 1] so that any two subintervals of [0, 1] of equal length are equally likely to contain the selected number, and let X denote the selected number. Then we say that X has the uniform in [0, 1] distribution and denote this by writing $X\sim U(0,1)$.

In addition to the CDF, the probability distribution of a continuous random variable can be described in terms of its probability density function.

**Definition 3.2-6** The probability density function, or PDF, of a continuous random variable X is a nonnegative function fX (thus, fX(x) ≥ 0, for all x), with the property that $P(a<X<b)$ equals the area under it and above the interval [a, b]. Thus, $P(a<X<b)=$ area under $f_X$ between a and b.

A positively skewed distribution is also called skewed to the right, and a negatively skewed distribution is also called skewed to the left.

 - Probability of an Interval in Terms of the PDF

$$P(a<X<b)=\int^b_af_X(x)dx$$

 - Total Area Under the Curve of a PDF Must Equal 1
 
 $$\int^{\infty}_{-\infty}f_X(x)dx$$

**Proposition 3.2-3** If X is a continuous random variable, $P(a<X<b)=P(a\le X\le b)=F(b)−F(a)$.

**REMARK 3.2-2** Proposition 3.2-3 is true only for the idealized version of a continuous random variable. As we have pointed out, in real life, all continuous variables are measured on a discrete scale. The PMF of the discretized random variable that is actually measured is readily approximated from the formula

$$P(x−\Delta x\le X\le x+\Delta x)\approx 2f_X(x)\Delta x$$

**Proposition 3.2-4** If X is a continuous random variable with PDF f and CDF F, then
 (a) The CDF can be obtained from the PDF through the formula

$$F(x)=\int^x_{-\infty}f(y)dy$$
 (b) The PDF can be obtained from the CDF through the formula
$$f(x)=F′(x)=\frac{d}{dx}F(x)$$

**Example 3.2-5** A randomvariable X is said to have the uniform in [A,B] distribution, denoted by $X\sim U(A,B)$, if its PDF is

$$f(x)=\begin{cases}0&\text{ if }x<A\\ \frac1{B-A}& \text{ if }A\le x\le B \\ 1&\text{ if }x>B\end{cases}$$

$$F(x)=\int^x_A\frac1{B−A}dy=\frac{x−A}{B−A}$$

$F(x)=0$ for $x<A$, $F(x)=F(B)=1$ for $x>B$.

### 3.3 Parameters of Probability Distributions

#### 3.3.1 Expected Value

 - General Definition of Expected Value
 
$$E(X)=μ_X=\sum_{x \in S_X}xp(x)$$

 - Definition of Expected Value for Continuous X

$$E(X)=μ_X=\int_{-\infty}^{\infty}xf(x)dx$$

$$\int_0^5\frac1{(x+1)\sqrt{x}}dx,\quad \int_1^{\infty}\frac1{x^2}dx,\quad \int_{-\infty}^{\infty}e^{-|x|}dx$$

```{r eval=FALSE, include=T}
f=function(x){1/((x+1)*sqrt(x))}

integrate(f, lower=0,upper=5)

f=function(x){1/x^2}

integrate(f, lower=1, upper=Inf)

f=function(x){exp(-abs(x))}

integrate(f, lower=-Inf,upper=Inf)
```

**Proposition3.3-1**

 1. If X is discrete with sample space $S_X$ and $h(x)$ is a function on $S_X$, the mean value of $Y=h(X)$ can be computed using the PMF $p_X(x)$ of X as

  - Mean Value of a Function of a Discrete Random Variable X
$$E(h(X)) =\sum_{x \in S_X}h(x)p_X(x)$$

 2. If X is continuous and $h(x)$ is a function, the expected value of $Y = h(X)$ can
be computed using the PDF $f_X(x)$ of X as

  - Mean Value of a Function of a Continuous Random Variable X
  
$$E(h(X)) =\int_{-\infty}^{\infty}h(x)f(x)dx$$
  
 3. If the function $h(x)$ is linear, that is, $h(x)=ax+b$, so $Y=aX+b$, then
 
  - Mean Value of a Linear Function of a General Random Variable X

$$E(h(X))=aE(X)+b$$

#### 3.3.2 Variance and Standard Deviation

 - General Definition of Variance of a Random Variable X

$$σ^2_X=E[(X−μ_X)^2]$$

 - Short-cut Formula for Variance of a Random Variable X

$$σ^2_X=E(X^2)-[E(X)]^2$$

$$=\sum_{x \in S_X}(x-\mu_X)^2p(x)=\sum_{x \in S_X}x^2p(x)-\mu_X^2$$

$$=\int_{-\infty}^{\infty}(x-\mu_X)^2f_X(x)dx=\int_{-\infty}^{\infty}x^2f_X(x)dx-\mu_X^2$$

 - Definition of Standard Deviation
 
 $$σ_X=\sqrt{σ^2_X}$$

**roposition3.3-2** If the variance of X is σ2Xand Y = a + bX, then

 - Variance and Standard Deviation of a Linear Transformation

$$σ^2_Y= b^2σ^2_X,\quad σ_Y = |b|σ_X$$

#### 3.3.3 Population Percentiles

**Definition 3.3-1** Let X be a continuous random variable with CDF F and α a number between 0 and 1. The 100(1−α)-th percentile (or quantile) of X is the number, denoted by $x_α$, with the property
$$F(x_α)=P(X\le x_α)=1−α$$
In particular:

 1. The 50th percentile, which corresponds to $α=0.5$ and is denoted by $x_{0.5}$, is called the median and is also denoted by $\tildeμ_X$. The defining property of $\tildeμ_X$ is $F(\tildeμ_X)=0.5$.
 2. The 25th percentile, which corresponds to $α=0.75$ and is denoted by $x_{0.75}$, is called the lower quartile and is also denoted by $Q_1$. The defining property of $Q_1$ is $F(Q_1)=0.25$.
 3. The 75th percentile, which corresponds to $α=0.25$ and is denoted by $x_{0.25}$, is called the upper quartile and is also denoted by $Q_3$. The defining property of $Q_3$ is $F(Q_3)=0.75$.

**Definition 3.3-2** The interquartile range, abbreviated by IQR, is the distance between the 25th and 75th percentile: $IQR=Q_3−Q_1$.

### 3.4 Models for Discrete Random Variables

#### 3.4.1 The Bernoulli and Binomial Distributions

**The Bernoulli Distribution** A Bernoulli trial or experiment is one whose outcome can be classified as either a success or a failure. The Bernoulli random variable X takes the value 1 if the outcome is a success, and the value 0 if it is a failure.

**The Binomial Distribution** Suppose n Bernoulli experiments, each having probability of success equal to p, are performed independently. Taken together, the n independent Bernoulli experiments constitute a binomial experiment. The binomial random variable Y is the total number of successes in the n Bernoulli trials.

If $X_i$ denotes the Bernoulli random variable associated with the ith Bernoulli trial, that is,
$$X_i =\begin{cases}1& \text{if ith experiment results in success}\\0& \text{otherwise} \end{cases} \text{for } i=1,..,n$$

then the binomial random variable Y equals

$$Y =\sum^n_{i=1}X_i$$

 - PMF of the Binomial Distribution
 
$$P(Y=y)=\binom{n}{y}p^y(1−p)^{n−y},\quad y=0,1,..,n$$

 - Mean and Variance of the Binomial Distribution
 
$$E(X) = np,\quad σ^2_X=np(1−p)$$

For $n=1$ the binomial randomvariable is just a Bernoulli randomvariable, and the above formulas reduce to the mean and variance given in (3.4.1).

#### 3.4.2 The Hypergeometric Distribution

The hypergeometric model applies to situations where a simple random sample of size n is taken from a finite population of $N$ units of which $M_1$ are labeled 1 and the rest, which are $M_2=N−M_1$, are labeled 0. The number X of units labeled 1 in the sample is a hypergeometric random variable with parameters $M_1, M_2,and\ n$

If $X_i$ is the Bernoulli random variable corresponding to the ith product item, the hypergeometric random variable X equals

$$X=\sum^n_{i=1}X_i$$

$$S_X = \{\max(0,n−M_2),..,\min(n, M_1)\}$$

where $\max(a_1, a_2)$ and $\min(a_1, a_2)$ denote the larger and the smaller, respectively, of the two numbers $a_1$ and $a_2$.

 - Probability Mass Function of the Hypergeometric Distribution

$$P(X=x)=\frac{\binom{M_1}{x}\binom{M_2}{n-x}}{\binom{M_1+M_2}{n}}$$

 - Mean and Variance of the Hypergeometric Distribution
 
$$μ_X=n\frac{M_1}{N},\quad σ^2_X=n\frac{M_1}N(1−\frac{M_1}N)\frac{N−n}{N−1}$$

**Binomial Approximation to Hypergeometric Probabilities**

 - Rule of Thumb for Using the Binomial Approximation to Hypergeometric Probabilities

$$\text{If }\frac{n}N\le0.05,\quad \text{then } P(X = x)\simeq P(Y = x)$$

#### 3.4.3 The Geometric and Negative Binomial Distributions

The Geometric Distribution A geometric experiment is one where independent Bernoulli trials, each with the same probability p of success, are performed until the occurrence of the first success. The geometric(p) randomvariable X is the total number of trials up to and including the first success in such a geometric experiment.

 - PMF of the Geometric Distribution
 
 $$P(X=x)=(1−p)^{x−1}p,\quad x=1,2,3,...$$

$$F(x) =\sum_{y\le x}P(Y = y) = p\sum^x_{y=1}(1−p)^{y−1}=p\sum^{x−1}_{y=0}(1 − p)^y$$

 - CDF of the Geometric Distribution 

 $$F(x)=1−(1−p)^x,\quad x=1,2,3,..$$

 - Mean and Variance of the Geometric Distribution

$$E(X) =\frac1p,\quad σ^2=\frac{1−p}{p^2}$$

The Negative Binomial Distribution A negative binomial experiment is one where independent Bernoulli trials, each with the same probability p of success, are performed until the occurrence of the rth success. The negative binomial(r, p) random variable Y is the total number of trials up to and including the rth success in such a
negative binomial experiment.

The sample space of the negative binomial(r, p) random variable Y is $S_Y=\{r,r+1,..\}$. For $r=1$, the negative binomial(r,p) experiment reduces to the geometric(p) experiment. In fact, if $X_1$ is the geometric(p) random variable that counts the number of trials until the first success, $X_2$ is the geometric(p) random
variable that counts the additional number of trials until the second success, and so forth, the negative binomial(r, p) random variable Y can be expressed in terms of these geometric(p) randomvariables as

$$Y=\sum^r_{i=1}X_i$$

The PMF $P(Y=y),y=r,r+1,..$, of the negative binomial(r, p) random variable Y is

 - PMF of the Negative Binomial Distribution

$$P(Y=y)=\binom{y-1}{r-1}p^r(1−p)^{y−r}$$

 - Mean and Variance of the Negative Binomial Distribution

$$E(Y) =\frac{r}p,\quad σ^2_Y=r\frac{1 − p}p^2$$

#### 3.4.4 The Poisson Distribution

The Model and Its Applications A random variable X that takes values $0,1,2,..$ is said to be a Poisson random variable with parameter $λ$, denoted by $X\sim Poisson(λ)$, if its PMF is given by

 - PMF of the Poisson Distribution

$$P(X=x)=e^{−λ}\frac{λ^x}{x!},\quad x=0,1,2,..$$

 - Mean and Variance of the Poisson Distribution
 
 $$μ_X=λ, σ^2_X=λ$$

One of the earliest uses of the Poisson distribution was in modeling the number of alpha particles emitted from a radioactive source during a given period of time. Today it has a tremendous range of applications in such diverse areas as insurance, tourism traffic engineering, demography, forestry, and astronomy. For example, the
Poisson random variable X can be
 1. the number of fish caught by an angler in an afternoon,
 2. the number of new potholes in a stretch of I80 during the winter months,
 3. the number of disabled vehicles abandoned on I95 in a year,
 4. the number of earthquakes (or other natural disasters) in a region of the United States in a month,
 5. the number of wrongly dialed telephone numbers in a given city in an hour,
 6. the number of freak accidents, such as falls in the shower, in a given time period,
 7. the number of vehicles that pass a marker on a roadway in a given time period,
 8. the number of marriages, or the number of people who reach the age of 100,
 9. the distribution of trees in a forest, and
 10. the distribution of galaxies in a given region of the sky.

As seen from these applications of the Poisson model, the random phenomena that the Poisson distribution models differ from those of the previously discussed distributions in that they are not outcomes of sampling experiments from a wellunderstood population. Consequently, the Poisson PMF is derived by arguments that are different from the ones used for deriving the PMFs of the previously discussed distributions (which use the counting techniques of Chapter 2 and the concept of independence). Instead, the Poisson PMF is derived as the limit of the binomial PMF (see Proposition 3.4-1 below) and can also be obtained as a consequence of certain postulates governing the random occurrence of events (see the following discussion about the Poisson process).

Poisson Approximation to Binomial Probabilities The enormous range of applications
of the Poisson random variable is, to a large extent, due to the following
proposition stating that it can be used as an approximation to binomial random
variables.

**Proposition 3.4-1** A binomial experiment where the number of trials n is large $(n\ge100)$, the probability
p of success in each trial is small $(p\le 0.01)$, and the product $np$ is not large $(np\le 20)$, can be modeled (to a good approximation) by a Poisson distribution with $λ=np$. In particular, if $Y\sim Bin(n, p)$, with $n\ge 100, p\le 0.01$, and $np\le 20$, then the approximation

 - Poisson Approximation to Binomial Probabilities

$$P(Y\ge k)\simeq P(X\ge k)$$

holds for all $k=0,1,2,..,n$, where $X\sim Poisson(λ=np)$.

The Poisson Process All examples of Poisson random variable pertain to the number of events occurring in a fixed time period (fish caught in an afternoon, potholes during the winter months, etc). Often, however, events are recorded at the time they occur as time unfolds. This requires that time itself become an integral part of the notation describing the data records. Letting time 0 denote the start of observations we set

$$X(t)= \text{number of events occurring in the time interval } (0, t]$$

**Definition 3.4-1** Viewed as a function of time, the number of occurrences $X(t),t\ge 0$, is called a Poisson process if the following postulates are satisfied.

 1. The probability of exactly one occurrence in a time period of length h is equal to $αh + o(h)$, for some $α>0$, where the quantity $o(h)$ satisfies $o(h)/h\rightarrow0$, as $h\rightarrow0$.
 2. The probability of more than one occurrence in a short time period of length h is equal to $o(h)$.
 3. For any set of nonoverlapping time intervals $A_i, i=1,..,n$, the events $E_i=[k_i\ \text{events occur in}\ A_i], i=1,..,n$, where the ki are any integers, are mutually independent.

The parameter α in the first postulate specifies the rate of the occurrences or, synonymously, the average number of occurrences per unit of time. Thus, the first postulate states that the rate at which events occur is constant in time. The second postulate means that the events are rare, in the sense that it is highly unlikely that two will occur simultaneously. Finally, the third postulate specifies that in disjoint time intervals events occur independently. When these postulates hold we have the following proposition.

**Proposition3.4-2** If $X(t), t\ge 0$, is a Poisson(α) process, then

 1. For each fixed $t_0$, the random variable $X(t_0)$, which counts the number of occurrences in $(0, t_0]$, has the Poisson distribution with parameter $λ = α\times t_0$. Thus,
 
$$P(X(t_0)=k)=e^{−αt_0}\frac{(αt_0)^k}{k!},\quad k=0,1,2,..$$

 2. For any two positive numbers $t_1<t_2$ the random variable $X(t_2)−X(t_1)$, which counts the number of occurrences in $(t_1, t_2]$, has the Poisson distribution with parameter $λ=α(t_2−t_1)$. Thus, the PMF of $X(t_2)−X(t_1)$ is given by (3.4.22) with $t_0$ replaced by $(t_2−t_1)$.
 3. For any two positive numbers $t_1<t_2$ the random variable $X(t_2)−X(t_1)$ is independent from $X(s)$ for all $s\le t_1$.

A noteworthy implication of part (2) of this proposition is that the zero time point of a Poisson process can be any arbitrary time point. In other words, one may start recording the events which happen after time t1, completely ignoring anything that happened up to that point, and still get a Poisson process with the same rate.

The proof of Proposition 3.4-2 is based on the fact that Poisson probabilities are obtained as limits of binomial probabilities (Proposition 3.4-1). Indeed, if the postulates of Definition 3.4-1 are satisfied, then by dividing an interval into a large number of small subintervals of equal length, the total number of occurrences in that interval can be thought of as a binomial random variable made up of the sum of Bernoulli random variables, each of which corresponds to one of the small subintervals. Since by Proposition 3.4-1 the Poisson probability mass function is obtained as the limit of binomial probability mass functions, it can be argued that the total number of occurrences is a Poisson random variable. While making this argument rigorous is not beyond the scope of this book, such a proof adds little to the understanding of the Poisson process and thus will not be presented.

### 3.5 Models for Continuous Random Variables

#### 3.5.1 The Exponential Distribution

A random variable X is said to be an exponential, or to have the exponential distribution with parameter $λ$, denoted by $X\sim Exp(λ)$, if its PDF is

$$f(x)=\begin{cases} λe^{−λx} &if\ x\ge 0\\0 &if\ x<0\end{cases}$$

$$F(x)=\begin{cases}1- e^{−λx} &if\ x\ge 0\\0 &if\ x<0\end{cases}$$

 - Mean, Variance, and Percentiles of the Exponential Distribution

$$μ=\frac1λ,\quad σ^2=\frac1{λ^2},\quad x_α=−\frac{log(α)}λ$$


 - Memoryless Property of a Random Variable

$$P(X>s+t|X>s)=P(X>t)$$

The Poisson-Exponential Connection For a Poisson process, let T1 be the time the first event occurs, and for $i=2,3,..$, let $T_i$ denote the time elapsed between the occurrence of the (i−1)-st and the ith event. For example, $T_1=3$ and $T_2=5$ means that the first occurrence of the Poisson process happened at time 3 and the second at time 8. The times $T_1,T_2,..$ are called interarrival times.

**Proposition 3.5-1** If $X(s), s\ge 0$, is a Poisson process with rate α, the interarrival times have the exponential distribution with PDF $f(t)=αe^{−αt}, t>0$.

#### 3.5.2 The Normal Distribution 150

A random variable is said to have the standard normal distribution if its PDF and CDF, which are denoted (universally) by $\phi$ and $\Phi$, respectively, are

$$\phi(z) =\frac1{\sqrt{2\pi}}e^{−z^2/2} ,\quad \Phi(z) =\int^z_\infty\phi(x)dx$$

for $-\infty<z<\infty$. A standard normal random variable is denoted by Z. Note that the PDF $φ$ is symmetric about zero

A randomvariable X is said to have the normal distribution, with parameters $μ$ and $σ$, denoted by $X\sim N(μ, σ^2)$, if its PDF and CDF are
$$f(x)=\frac1σ\phi{(\frac{x − μ}σ)},\quad F(x)=\Phi{(\frac{x − μ}σ)}$$

for $-\infty<z<\infty$. Thus,

$$f(x) =\frac1{\sqrt{2\piσ^2}}e^{−\frac{(x − μ)^2}{2σ^2}}$$

**Proposition 3.5-2** If $X\sim N(μ, σ^2)$ and a, b are any real numbers, then

$$a+bX\sim N(a+bμ, b^2σ^2)$$

The new element of this proposition is that a linear transformation of a normal random variable is also a normal random variable. That the mean value and variance of the transformed variable, $Y = a+bX$, are $a+bμ$ and $b^2σ^2$ follows from Propositions 3.3-1 and 3.3-2, respectively, so there is nothing new in these formulas.

**Corollary 3.5-1** If $X\sim N(μ, σ^2)$, then
 1. $\frac{X−μ}σ\sim N(0,1)$, and
 2. $P(a\le X\le b)=\Phi(\frac{b−μ}σ)-\Phi(\frac{a−μ}σ)$

**The Q-Q Plot** As already mentioned, most experiments resulting in the measurement of a continuous random variable provide little insight as to which probability model best describes the distribution of the measurements. Thus, several procedures have been devised to test the goodness-of-fit of a particular model to a random sample obtained from some population. Here we discuss a very simple graphical
procedure, called the Q-Q plot, as it applies for checking the goodness-of-fit of the normal distribution.

The basic idea of the Q-Q plot is to plot the sample percentiles, which are the ordered sample values, against with the corresponding percentiles of the assumed model distribution. Since sample percentiles estimate corresponding population percentiles, if the assumed model distribution is a good approximation to the true population distribution, the plotted points should fall approximately on a straight line of angle 45o that passes through the origin.

## 4. Jointly Distributed Random Variables

### 4.2 Describing Joint Probability Distributions
#### 4.2.1 The Joint and Marginal PMF

**Definition 4.2-1** The joint, or bivariate, probability mass function (PMF) of the jointly discrete random variables X and Y is defined as

$$p(x,y)=P(X=x,Y=y)$$

 - Obtaining the Marginal PMFs from the Joint PMF
 
 $$p_X(x)=\sum_{y\in S_Y}p(x,y),\quad p_Y(y)=\sum_{x\in S_X}p(x,y)$$

#### 4.2.2 The Joint and Marginal PDF

**Definition 4.2-2** The joint or bivariate density function of the jointly continuous random variables X and Y is a nonnegative function f(x,y) with the property that the
probability that (X,Y) will take a value in a region A of the x-y plane equals
the volume under the surface defined by f(x,y) and above the region A.

 - Volume Under the Entire Surface Defined by f(x,y) Is 1
 
$$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f(x,y)dxdy=1$$

 - Probability that (X,Y) Lies in the Region A

$$P((X,Y)\in A)=\iint_A f(x,y)dxdy$$.

 - Probability of (X,Y) Lying in a Rectangle 
 
 $$P(a\le X\le b,\ c\le Y\le d)=\int_a^b\int_c^df(x, y)dydx$$

 - Obtaining the Marginal PDFs from the Joint PDF
 
$$f_X(x)=\int_{-\infty}^{\infty}f(x,y)dy,\quad f_Y(y)=\int_{-\infty}^{\infty}f(x,y)dx$$

### 4.3 Conditional Distributions
#### 4.3.1 Conditional Probability Mass Functions

 - Definition of Conditional PMF of Y given X=x

$$p_{Y|X=x}(y)=P(Y=y|X=x)=\frac{p(x,y)}{p_X(x)},\quad y\in S_Y$$

 - Basic Properties of Conditional PMFs

$$p_{Y|X=x}(y)\ge0, y\in S_Y, \quad \sum_yp_{Y|X=x}(y)=1$$

 - Multiplication Rule for Joint Probabilities

$$p(x,y)=p_{Y|X=x}(y)p_X(x)$$

 - Law of Total Probability for Marginal PMFs

$$p_Y(y)=\sum_{x\in S_X}p_{Y|X=x}(y)p_X(x)$$

#### 4.3.2 Conditional Probability Density Functions

 - Definition of the Conditional PDF of Y Given X=x
 
 $$f_{Y|X=x}(y)=\frac{f(x,y)}{f_X(x)}$$
 
 - Conditional Probabilities in Terms of the Conditional PDF
 
 $$P(a<Y<b|X=x)=\int_a^bf_{Y|X=x}(y)dy$$

 - Multiplication Rule for Joint PDFs

$$f(x,y)=f_{Y|X=x}(y)f_X(x)$$

 - Law of Total Probability for Marginal PDFs
 
 $$f_Y(y)=\int_{-\infty}^{\infty} f_{Y|X=x}(y)f_X(x)dx$$

#### 4.3.3 The Regression Function

 - Regression Function for Jointly Discrete (X,Y)

$$μ_{Y|X}(x) =\sum_{y\in S_Y}yp_{Y|X=x}(y),\ x\in S_X$$

 - Regression Function for Jointly Continuous (X,Y)

$$μ_{Y|X}(x) =\int_{-\infty}^{\infty}yf_{Y|X=x}(y),\ x\in S_X$$

 - Law of Total Expectation

$$E(Y)=E[E(Y|X)]$$

 - Law of Total Expectation for Discrete Random Variables
 
 $$E(Y) =\sum_{x\in S_X}E(Y|X=x)p_X(x)$$
 
 - Law of Total Expectation for Continuous Random Variables

 $$E(Y) =\int_{-\infty}^{\infty}E(Y|X=x)f_X(x)$$

#### 4.3.4 Independence

 - Definition of Independence of Two Random Variables
 
 $$P(X\in A Y\in B)=P(X\in A)P(Y\in B)$$
 
 - Condition for Independence of Two Discrete RandomVariables
 
  $$p_{X,Y}(x,y) = p_X(x)p_Y(y)$$
 
 - Condition for Independence of Two Continuous Random Variables
 
 $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$

 - Condition for Independence of Several Discrete Random Variables
 
 $$p(x_1,x_2,..,x_n)=p_{X_1}(x_1)\cdots p_{X_n}(x_n)$$
 
 - Condition for Independence of Several Continuous Random Variables

 $$f(x_1,x_2,..,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n)$$

### 4.4 Mean Value of Functions of Random Variables
#### 4.4.1 The Basic Result

 1. Let (X,Y) be discrete with joint PMF p(x,y). The expected value of a function, h(X,Y), of (X,Y) is computed by

 - Mean Value of a Function of Discrete Random Variables
 
 $$E[h(X,Y)] =\sum_{x\in S_X}\sum_{y\in S_Y}h(x,y)p(x,y)$$

 2. Let (X,Y) be continuous with joint PDF f(x,y). The expected value of a
function, h(X,Y), of (X,Y) is computed by

 - Mean Value of a Function of Continuous Random Variables
 
 $$E[h(X,Y)] =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y)f(x,y)dxdy$$

 - Variance of a Function of Two Random Variables
 
 $$σ^2_{h(X,Y)} = E[h^2(X, Y)] − [E[h(X,Y)]]^2$$

#### 4.4.2 Expected Value of Sums

**Proposition 4.4-2** Let $X_1,..,X_n$ be any n random variables (i.e., they may be discrete or continuous,
independent or dependent), with marginal means $E(X_i) = μ_i$. Then

 - Expected Value of a Linear Combination of Random Variables
 
 $$E(a_1X_1 +..+ a_nX_n) = a_1μ_1 +..+ a_nμ_n$$

**Corollary 4.4-1** If the random variables $X_1,..,X_n$ have common mean $μ$, that is, if $E(X_1)=..=E(X_n)=μ$, then

 - Expected Value of the Average and the Total

$$E(\bar X) = μ,\quad E(T) = nμ$$

 - Expected Value of the Sample Proportion
 
 $$E(\hat p) = p $$

**Proposition 4.4-3** Suppose that N is an integer-valued random variable, and the random variables $X_i$ are independent from N and have common mean value $μ$. Then,

 - Expected Value of a Sum of a Random Number of Random Variables
 
 $$E\left(\sum_{i=1}^NXi\right)=E(N)μ$$

#### 4.4.3 The Covariance and the Variance of Sums

$$Var(X + Y) = E\{[X + Y − E(X + Y)]^2\}=Var(X)+Var(Y)+2E[(X−E(X))(Y−E(Y))]$$

The quantity $E[(X−E(X))(Y−E(Y))]$ that appears in formula (4.4.5) is called
the covariance of X and Y, and is denoted by $Cov(X,Y)$ or $σ_{X,Y}$:

 - Definition of and Short-cut Formula for the Covariance
 
 $$σ_{X,Y}=E[(X−μ_X)(Y−μ_Y)]=E(XY)−μ_Xμ_Y$$

**Proposition 4.4-4**
 1. Let $σ^2_1,σ^2_2$ denote the variances of $X_1,X_2$, respectively. Then 
  (a) If $X_1,X_2$ are independent (or just $Cov(X_1,X_2)=0$),
     $$Var(X_1+X_2) = σ^2_1 + σ^2_2,\quad Var(X_1 − X_2) = σ^2_1 + σ^2_2$$
     
  (b) If X1,X2 are dependent,
$$Var(X_1−X_2)=σ^2_1+σ^2_2−2Cov(X_1,X_2)$$

$$Var(X_1+X_2)=σ^2_1+σ^2_2+2Cov(X_1,X_2)$$

 2. Let $σ^2_1 ,..,σ^2_m$ denote the variances of $X_1,..,X_m$, respectively, and $a_1,..,a_m$ be any constants. Then
  (a) If $X_1,..,X_m$ are independent (or just Cov(X_i,X_j)=0, for all $i\neq j$),
  
$$Var(a_1X_1+..+a_mX_m) = a^2_1σ^2_1 +..+a^2_mσ^2_m$$

 (b) If $X_1,..,X_m$ are dependent,
 
$$Var(a_1X_1+..+a_mX_m) = a^2_1σ^2_1+..+a^2_mσ^2_m+\sum_i\sum_{j\neq i}a_ia_j\sigma_{ij}$$

**Corollary 4.4-2** Let $X_1,..,X_n$ be iid (i.e., a simple random sample from an infinite population) with common variance $σ^2$. Then,

 - Variance of the Average and the Sum
 
 $$Var(X)=\frac{σ^2}n,\quad Var(T) = nσ^2$$
 
 - Variance of the Sample Proportion

$$Var(\hat p) =\frac{p(1−p)}n$$

**Proposition 4.4-5** Properties of covariance

 1. $Cov(X,Y) = Cov(Y,X)$
 2. $Cov(X,X) = Var(X)$
 3. If X,Y are independent, then $Cov(X,Y)=0$.
 4. $Cov(aX+b, cY+d)=acCov(X,Y)$ for any real numbers a, b, c, and d.

### 4.5 Quantifying Dependence

#### 4.5.1 Positive and Negative Dependence

If the variables are either positively or negatively dependent, their dependence is called monotone.

#### 4.5.2 Pearson’s (or Linear) Correlation Coefficient

**Definition 4.5-1** The Pearson’s (or linear) correlation coefficient of X and Y, denoted by
$Corr(X,Y)$ or $ρ_{X,Y}$, is defined as

$$ρ_{X,Y}=Corr(X,Y)=\frac{Cov(X,Y)}{σ_Xσ_Y}$$

where $σ_X$, $σ_Y$ are the marginal standard deviations of X, Y, respectively

**Proposition 4.5-1**

 1. If a and c are either both positive or both negative, then

$$Corr(aX+b,cY+d)=ρ_{X,Y}$$

If a and c are of opposite signs, then

$$Corr(aX+b,cY+d)=-ρ_{X,Y}$$

 2. $−1\le ρ_{X,Y}\le1$, and
  (a) if X, Y are independent then $ρ_{X,Y}=0$.
  (b) $ρ_{X,Y}=1$ or −1 if and only if $Y=aX+b$ for some numbers a, b with $a\neq0$.

Pearson’s Correlation as a Measure of Linear Dependence It should be emphasized
that correlation measures only linear dependence. In particular, it is possible to have
the strongest possible dependence, that is, knowing one amounts to knowing the
other, but if the relation between X and Y is not linear, then the correlation will not
equal 1. Let X have the uniform in (0,1) distribution, and $Y = X^2,\ ρ_{X,Y}=0.968$. $Y=X^4,\ ρ_{X,Y}=0.866$.

**Definition 4.5-2** Two variables having zero correlation are called uncorrelated.

Sample Versions of the Covariance and Correlation Coefficient If $(X_1,Y_1),..,(X_n,Y_n)$ is a sample from the bivariate distribution of (X,Y), the sample covariance, denoted by $\widehat {Cov}(X,Y)$ or $S_{X,Y}$, and sample correlation coefficient, denoted by
$\widehat{Corr}(X,Y)$ or $r_{X,Y}$, are defined as

 - Sample Versions of Covariance and Correlation Coefficient

$$S_{X,Y}=\frac1{n−1}\sum^n_{i=1}(X_i−\bar X)(Y_i−\bar{Y})=\frac1{n−1}\left[ \sum^n_{i=1}X_iY_i-\frac1n(\sum^n_{i=1}X_i)(\sum^n_{i=1}Y_i)\right]$$
$$r_{X,Y}=\frac{S_{X,Y}}{S_XS_Y}$$

### 4.6 Models for Joint Distributions
#### 4.6.1 Hierarchical Models

The principle of hierarchical modeling uses the multiplication rules in order to specify the joint distribution of X and Y by first specifying the conditional distribution of Y given X=x, and then specifying the marginal distribution of X. Thus, a hierarchical model consists of
$$Y|X = x\sim F_{Y|X=x}(y),\quad X\sim F_X(x)$$
where the conditional distribution of Y given $X=x, F_{Y|X=x}(y)$, and the marginal distribution of $X,F_X(x)$, can depend on additional parameters.

**Example 4.6-1** (p.200)

$$Y|X=x\sim Bin(x,p),\quad X\sim Poisson(λ)$$
which leads to a joint PMF of (X,Y), which, for $y\le x$, is:
$$p(x,y)=p_{Y|X=x}(y)p_X(x)=\binom{x}{y}p^y(1−p)^{x−y}\frac{e^{−λ}λ^x}{x!}$$

**Example 4.6-2** The bivariate normal distribution. X and Y are said to have a bivariate normal
distribution if their joint distribution is specified according to the hierarchical model

$$Y|X=x\sim N(β_0+β_1(x−μ_X), σ^2_ε),\quad X\sim N(μ_X, σ^2_X)$$

$$f_{Y|X=x}(y)=\frac1{\sqrt{2πσ^2_ε}}e^{−\frac{(y−β_0−β_1(x−μ_X))^2}{2σ^2_ε}}$$

$$f_X(x)=\frac1{\sqrt{2πσ^2_X}}e^{−\frac{(x−μ_X)^2}{2σ^2_X}}$$

$$f_{X,Y}(x,y)=\frac1{2πσ_εσ_X}e^{−\frac{(y−β_0−β_1(x−μ_X))^2}{2σ^2_ε}−\frac{(x−μ_X)^2}{2σ^2_X}}$$

#### 4.6.2 Regression Models

In regression studies Y is called the response variable, and X is interchangeably referred to as the covariate, or the independent variable, or the predictor, or the explanatory variable.

The Simple Linear Regression Model The simple linear regression model specifies that the regression function of Y on X is linear, that is,
$$μ_{Y|X}(x)=α_1+β_1x$$
and the conditional variance of Y given X=x, denoted by $σ^2_ε$ , is the same for all values x. The latter is known as the homoscedasticity assumption. In this model, $α_1$,$β_1$, and $σ^2_ε$ are unknown parameters. The regression function (4.6.4) is often written as 

$$μ_{Y|X}(x)=β_0+β_1(x−μ_X)$$ 
where $μ_X$ is the marginal mean value of X, and $β_0$ is related to $α_1$ through $β_0=α_1+β_1μ_X$. The straight line defined by the equation is called the regression line.

**Proposition 4.6-1** The marginal expected value of Y is given by

$$E(Y)=α_1+β_1μ_X,\quad E(Y)=β_0$$

 - Mean Plus Error Form of the Simple Linear Regression Model
 
 $$Y = α_1 + β_1X + ε,\quad Y = β_0 + β_1(X − μ_X) + ε$$

**Proposition4.6-2** The intrinsic error variable, $ε$, has zero mean and is uncorrelated from the explanatory variable X:

$$E(ε)=0 ,\quad Cov(ε,X)=0$$

**Proposition 4.6-3** If the regression function of Y on X is linear (so (4.6.4) or, equivalently, (4.6.5) holds), then we have the following:
 1. The marginal variance of Y is

$$σ^2_Y= σ^2_ε + β^2_1σ^2_X$$

 2. The slope $β_1$ is related to the covariance, $σ_{X,Y}$, and the correlation, $ρ_{X,Y}$, by

$$β_1=\frac{σ_{X,Y}}{σ^2_X}=ρ_{X,Y}\frac{σ_Y}{σ_X}$$

**REMARK 4.6-1** Sample version of the regression line.

$$\widehat{β_1}=\frac{S_{X,Y}}{S^2_X},\quad \widehat{α_1}=\bar Y−\widehat{β_1}\bar X$$

The normal regression model specifies that the conditional distribution of Y given X=x is normal,

$$Y|X=x\sim N(μ_{Y|X}(x), σ^2_ε)$$
where $μ_{Y|X}(x)$ is a given function of x, typically depending on unknown parameters.

The normal simple linear regression model specifies, in addition, that the regression
function $μ_{Y|X}(x)$ in (4.6.11) is linear, that is, that (4.6.5) or, equivalently, (4.6.4)
holds. The normal simple linear regression model is also written as

$$Y=α_1+β_1x+ε\quad\text{, with } ε\sim N(0,σ^2_ε)$$

#### 4.6.3 The Bivariate Normal Distribution

$$Y|X=x\sim N(β_0+β_1(x−μ_X), σ^2_ε)$$

$$f_{X,Y}(x,y)=\frac1{2πσ_Xσ_Y\sqrt{1-ρ^2}}e^{\frac{-1}{1-ρ^2}[\frac{\tilde x^2}{2σ^2_X}−\frac{ρ\tilde x\tilde y}{σ_Xσ_Y}+\frac{\tilde y^2}{2σ^2_Y}]}$$

The two variances and the covariance are typically arranged in a symmetric matrix, called the covariance matrix:

$$\sum=\begin{pmatrix}σ^2_X &σ_{X,Y} \\σ_{X,Y} & σ^2_Y\end{pmatrix}$$

**Proposition 4.6-4** Let (X,Y) have a bivariate normal distribution with parameters $μ_X, μ_Y, σ^2_X, σ^2_Y, σ_{X,Y}$. Then we have the following:

 1. The marginal distribution of Y is also normal.
 2. If X and Y are uncorrelated then they are independent.
 3. If X and Y are independent normal random variables, their joint distribution is bivariate normal with parameters $μ_X, μ_Y, σ^2_X, σ^2_Y$, and $σ_{X,Y}=0$.
 4. Any linear combination of X and Y has a normal distribution. In particular
 
$$aX + bY\sim N(aμ_X+bμ_Y,a^2σ^2_X+b^2σ^2_Y+2abCov(X,Y))$$

#### 4.6.4 The Multinomial Distribution

When this basic experiment is repeated n times, one typically records $N_1,..,N_r$, where $N_j=$ the number of times outcome j occurred. If the r possible outcomes of the basic experiment have probabilities $p_1,..,p_r$, the joint distribution of the random variables $N_1,..,N_r$ is said to be multinomial with n trials and probabilities $p_1,..,p_r$.

Note that, by their definition, $N_1,..,N_r$ and $p_1,..,p_r$ satisfy

$$N_1+..+N_r = n,\quad p_1+..+p_r=1$$

If $N_1,..,N_r$ have the $multinomial(n, p_1,..,p_r)$ distribution, then their joint PMF is

$$P(N_1=x_1,..,N_r=x_r)=\frac{n!}{x_1!\cdots x_r!}p^{x_1}_1\cdots p^{x_r}_r$$

## 5. Some Approximation Results

### 5.2 The LLN and the Consistency of Averages

**Theorem 5.2-1** The Law of Large Numbers. Let $X_1,..,X_n$ be independent and identically distributed and let g be a function such that $−\infty<E[g(X_1)]<\infty$. Then, $\frac1n\sum_{i=1}^ng(X_i)$ converges inprobability to $E[g(X1)]$, that is, for any $ϵ>0$,
$$P\left(\left|\frac1n\sum_{i=1}^ng(X_i) − E[g(X_1)]\right|>ϵ\right)\to0\ \text{as}\ n\to \infty$$

**Lemma 5.2-1** Chebyshev’s inequality. Let the random variable Y have mean value $μ_Y$ and variance $σ^2_Y<\infty$. Then, for any $ϵ>0$,

$$P(|Y − μ_Y|>ϵ)\le\frac{σ^2_Y}{ϵ^2}$$

### 5.3 Convolutions

#### 5.3.1 What They Are and How They Are Used

**Example 5.3-1** Sum of independent Poisson random variables. If $X\sim Poisson(λ_1)$ and $Y\sim Poisson(λ_2)$ are independent random variables, show that
$$X+Y\sim Poisson(λ_1+λ_2)$$

**Example 5.3-2** Sum of independent binomial random variables. If $X\sim Bin(n_1,p)$ and $Y\sim Bin(n_2,p)$ are independent binomial random variables with common probability of success, show that

$$X+Y\sim Bin(n_1+n_2,p)$$

**Example 5.3-3** The sum of two uniforms. If $X_1$ and $X_2$ are independent random variables having the uniform in (0,1) distribution, find the distribution of $X_1 + X_2$.

 - Convolution of the PDFs $f_1$ and $f_2$
 
$$f_{X1+X2}(y)=\int_{-\infty}^{\infty}f_{X_2}(y − x_1)f_{X_1}(x_1)d_{x_1}$$

#### 5.3.2 The Distribution of X in the Normal Case

**Proposition 5.3-1** Let $X_1,X_2,..,X_n$ be independent and normally distributed random variables, $X_i\sim N(μ_i,σ^2_i)$, and let $Y=a_1X_1+..+a_nX_n$ be a linear combination of the $X_i$. Then $Y\sim N(μ_Y,σ^2_Y)$, where $μ_Y=a_1μ_1+\cdots+a_nμ_n, σ^2_Y=a^2_1σ^2_1+\cdots+a^2_nσ^2_n$

**Corollary 5.3-1** Let $X_1,X_2,..,X_n$ be $iid N(μ,σ^2)$, and let X be the sample mean. Then $\bar X\sim N(μ_{\bar X}, σ^2_{\bar X})$, where $μ_{\bar X} =μ$, $σ^2_{\bar X}=\frac{σ^2}n$

### 5.4 The Central Limit Theorem

**Theorem 5.4-1** The Central Limit Theorem. Let $X_1,X_2,..,X_n$ be iid with mean $μ$ and a finite variance $σ^2$. Then for large enough n (n ≥ 30 for our purposes),

 1. X has approximately a normal distribution with mean $μ$ and variance $\frac{σ^2}n$,
that is,

$$\bar X\sim N(μ,\frac{σ^2}n)$$

2. $T=X_1,X_2,..,X_n$ has approximately a normal distribution with mean $nμ$ and variance $nσ^2$, that is,

$$T=X_1+X_2+..+X_n\sim N(nμ, nσ^2)$$

#### 5.4.1 The DeMoivre-Laplace Theorem

**Theorem 5.4-2** DeMoivre-Laplace. If $T\sim Bin(n,p)$ then, for large enough n,
$$T\sim N(np, np(1 − p))$$

 - Sample Size Requirement for Approximating Binomial Probabilities by Normal Probabilities
 
$$np\ge5\ \text{and}\ n(1−p)\ge5$$

## 7.1 Introduction to Confidence Intervals

#### 7.1.1 Construction of Confidence Intervals

$$|\hatθ−θ|\le Z_{\fracα2}S_{\hat θ}$$
gives an interval of plausible values for the true value of θ, with degree of
plausibility, or confidence level, approximately 95%. Such an interval is called a confidence interval (CI). More generally, a (1 − α)100% error bound, can be written as a CI with confidence level (1 − α)100%, also called (1 − α) 100% CI:

$$\hatθ−Z_{\fracα2}S_{\hat θ}\leθ\le \hatθ+Z_{\fracα2}S_{\hat θ}$$

#### 7.1.2 Z Confidence Intervals

Confidence intervals that use percentiles from the standard normal distribution, like
that in relation (7.1.5), are called Z CIs, or Z intervals.

Z intervals for the mean (so $θ=μ$) are used only if the population variance is
known and either the population is normal or the sample size is at least 30. Z intervals will be primarily used for the proportion ($θ=p$).

#### 7.1.3 The T Distribution and T Confidence Intervals

When sampling from normal populations, an estimator!θ of some parameter θ often
satisfies, for all sample sizes n,
$$\frac{\hatθ− θ}{S_{\hatθ}} \sim T_{\nu} $$
where $S_{\hatθ}$ is the estimated standard error of $\hatθ$ , and $T_{\nu}$ stands for the T distribution with $\nu$ degrees of freedom.
