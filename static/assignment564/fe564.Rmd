---
title: 'Shen Qu, 918881147'
author: "STAT564 Fall 2018 Final Exam"
date: "12/05/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---

```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

### Problem 1
Exercise 3.25 (Just report the T matrix, β vector and c vector along with their dimensions, and the rank of T matrix for each part).$y=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+ε$
 (a) $H_0:\ β_1=β_2=β_3=β_4=β$

$$y=β_0+β(x_1+x_2+x_3+x_4)+ε$$

$$
\mathbf{T}=\begin{bmatrix} 0 & 1 & -1 & 0 & 0 \\ 0 & 0 & 1 & -1 & 0 \\ 0 & 0 & 0 & 1 & -1  \\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}_{4\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \\ 0  \\ \beta \end{bmatrix}_{4\times1} rank(T)=4
$$

 (b) $H_0:\ β_1=β_2,\ β_3=β_4$
 
$$y=β_0+β_1(x_1+x_2)+β_3(x_3+x_4)+ε$$
 
 $$\mathbf{T}=\begin{bmatrix} 0 & 1 & -1 & 0 & 0 \\ 0 & 0 & 0 & 1 & -1 \end{bmatrix}_{2\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}_{2\times1} rank(T)=2$$
 
 (c) $H_0:\ β_1-2β_2=4β_3,\ β_1+2β_2=0$
 
$$β_1=-2β_2=2β_3 \\
y=β_0+β_1(x_1-\frac12x_2+\frac12x_3)+β_4 x_4+ε$$


$$\mathbf{T}=\begin{bmatrix} 0 & 1 & -2 & -4 & 0 \\ 0 & 1 & 2 & 0 & 0 \end{bmatrix}_{2\times5} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{bmatrix}_{5\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}_{2\times1} rank(T)=2$$

### Problem 2: 

Exercise 3.27 Show that $Var(\mathbf{\hat y})=\sigma^2\mathbf{H}$

$$\because \mathbf{\hat y=X\hatβ},\  \mathbf{\hatβ}=\mathbf{(X'X)^{-1} X'Y}$$

$$\therefore Var(\mathbf{\hat y})=Var(\mathbf{X\hatβ})=Var(\mathbf{X(X'X)^{-1} X'Y})=Var(\mathbf{HY})=HVar(\mathbf{Y})H'$$

$$\because \mathbf{y=Xβ+ε},\quad  Var(ε)=\sigma^2\mathbf{I}$$

$$\therefore Var(\mathbf{\hat y})=H\cdot Var(\mathbf{Xβ+ε})\cdot H'=H\cdot Var(\mathbf{ε})\cdot H'=H\cdot \sigma^2\mathbf{I}\cdot H'=\sigma^2\mathbf{H}$$

$$\because \sigma\ \text{is a constant, H is symmatric and idenogent.}$$
$$\therefore H\cdot \sigma^2\mathbf{I}\cdot H'=\sigma^2\mathbf{HH'}=\sigma^2\mathbf{HH}=\sigma^2\mathbf{H}$$

### Problem 3: 

The $(X'X)^{(-1)}$ for the $y=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+ε$ is given below. If MSE = 0.513 and n=9, compute the 

(a) $Var(\hatβ_1)$

$$Var(\mathbf{\hat\beta_1})=MSE\times C_{22}=0.513\times4.624=2.372112$$

(b) $se(\hatβ_3)$

$$se(\mathbf{\hat\beta_3})=\sqrt{MSE\times C_{44}}=\sqrt{0.513\times0.031}=0.1261071$$

(c) $Cov(\hatβ_1,\hatβ_3)$

$$Cov(\mathbf{\hat\beta_1,\hat\beta_3})=MSE\times C_{24}=0.513\times(-0.346)=-0.177498$$

(d) $Cor(\hatβ_1\hat,β_3)$

$$Cor(\mathbf{\hat\beta_1,\hat\beta_3})=\frac{Cov(\mathbf{\hat\beta_1,\hat\beta_3})}{se(\mathbf{\hat\beta_1})se(\mathbf{\hat\beta_3})}=\frac{-0.177498}{1.540166\times0.1261071}=-0.913874$$

(e) Based on what you found in the previous part(s), explain the relationship between $\hatβ_1$ and $\hatβ_3$.

The value of $Cor(\mathbf{\hat\beta_1,\hat\beta_3})$ is close to negative 1. There is a strong negative relationship between $\hatβ_1$ and $\hatβ_3$.

(f) Without computing variance for all the estimators, explain which estimator is the least consistent.

$C_{11}=11.423$ has the lagest value. $\hatβ_0$ has the the lagest variance and the least consistent among the estimators.

(g) Without computing covariances for all the pairs of estiamtors, list the pair(s) of estimators that are negatively correlated. Provide a reason.

According to the $(X'X)^{(-1)}$,

$C_{12},\ C_{13},\ C_{15},\ C_{23},\ C_{24},\ C_{45}$ are negative. 

The negatively correlated pairs of parameters are

$\hatβ_0$ and $\hatβ_1$, $\hatβ_0$ and $\hatβ_2$, $\hatβ_0$ and $\hatβ_4$, $\hatβ_1$ and $\hatβ_2$, $\hatβ_1$ and $\hatβ_3$, $\hatβ_3$ and $\hatβ_4$.

### Problem 4: 

a. the fitted model

$$\hat y=-1.808372+0.003598x_2+0.193960x_7-0.004816x_8$$

b. ANOVA


According to the ANOVA table, the P-values is much smaller than 0.05. The regression is significant on 95% confident level.

c. The t test.

Each parameter is significant differnct with zero on 95% confident level. x2 has weak positive affect on response variable. x7 has strong positive affect but not as significant as other variables. x8 has weak negative affect.

d. R-square and adjusted R-square

$$R^2=0.7863,\quad  R_{adjusted}^2=0.7596$$

e. The partial F test statistic

$$F_0=\frac{(SSE_{x_7reduced}-SSE_{full})/r}{MSE}=\frac{(83.938-69.870)/1}{2.911}=4.832704$$

This partial F satistic is the square of the t sattistic for x7 in full modle.

3.2 The correlation

$$Cor(y_i,\hat y_i)=\frac{Cov(y_i,\hat y_i)}{\sqrt{Var(y_i)Var(\hat y_i)}}
=\sqrt{\frac{Var(y_i,\hat y_i)}{Var(y_i)Var(\hat y_i)}}=\sqrt{\frac{\sum(y_i-\hat y_i)^2}{\sum(y_i)^2\sum(\hat y_i)^2}}=\sqrt{0.7863}=0.8867395$$

$$R^2=1-\frac{SSE}{SST}=1-\frac{69.870}{326.964}=0.7863$$

Therefore, square of simple corelation coefficient between $y_i$ and $\hat y_i$ equal R-squared.

3.3 The confidence intervals

a. The 95% CI on $\beta_7$ is

$$\hat\beta_7\pm t_{\frac{\alpha}2,24}se(\hat\beta_7)=(0.011855322,\ 0.376065098)$$

b. For x2=2300, x7=56.0, x8=2100, the 95% CI on the mean numbers of games won by a team is

$$\hat y\pm t_{\frac{\alpha}2,24}se(\hat y)=(6.436203,\ 7.996645)$$

```{R, collapse=TRUE}
# Problem 3
B<- data.frame(
   b0=c(11.423,-4.349,-0.790, 0.486,-0.196),
   b1=c(-4.349, 4.624,-3.057,-0.346, 0.024),
   b2=c(-0.790,-3.057, 3.978, 0.135, 0.063),
   b3=c( 0.486,-0.346, 0.135, 0.031,-0.005),
   b4=c(-0.196, 0.024, 0.063,-0.005, 0.011))
# Var($\hat b_1$)
0.513*B[2,2]
# se(b1)
sqrt(0.513*B[2,2])
# se(b3)
sqrt(0.513*B[4,4])
# cov(b1,b3)
0.513*B[2,4]
# cor(b1,b3)
(0.513*B[2,4])/((sqrt(0.513*B[2,2]))*(sqrt(0.513*B[4,4])))
```

```{r, echo=TRUE, message = FALSE, collapse=TRUE}
# Problem 4
library(tidyverse)
library(broom)
# Import Data
table_b1 <- read_table2("TableB.1.txt")
# Observe the data frame
head(table_b1)
# build the full model
model_2_7_8 <- lm(y ~ x2+x7+x8, data=table_b1)
# build the reduced model on x7
model_2_8 <- lm(y ~ x2+x8, data=table_b1)

# 3.1a/c/d t statistics for each hypotheses.
model_2_7_8%>% summary()
# 3.1b ANOVA for full model
anova(model_2_7_8)
# 3.1e ANOVA for reduced model on x7
anova(model_2_8)
# calculate partial F
anova(model_2_7_8,model_2_8)
(83.938-69.870)/1/2.911

# 3.2 calculate cor(yi,\hat yi)
augment(model_2_7_8)
cor(select(augment(model_2_7_8),y,.fitted))
# another way
cor(table_b1[,1],predict(model_2_7_8, table_b1[,c(3,8,9)]))
# calculate r
sqrt(0.7863)

# 3.3a calculate CI on b7
confint(model_2_7_8, level = 0.95)
# 3.3b calculate CI on the mean response variable
model_2_7_8 %>% predict(newdata = data.frame(x2=2300,x7=56.0,x8=2100), interval = "confidence", level=0.95)
```

### other codes

```{r, eval=FALSE, include = FALSE, collapse=TRUE}

library(texreg)
# Pretty print regression results on screen
lm(mpg ~ wt, data=my_df) %>% screenreg
texreg::screenreg(l=list(model_2_7_8))

# visualizng
library(GGally)
ggpairs(data=table_b1[c(1,3,8,9)])

# Correlation
cor(table_b1)

# Half correlation matrix:
library(corrr)
mtcars %>% correlate() %>% shave() %>% fashion()
# Visulize correlation matrix:
mtcars %>% correlate() %>% shave() %>% rplot()

# Scatterplot Matrix
mtcars[1:6] %>% plot
# Better looking version
library(ggfortify)
model_2_7_8 %>% autoplot()

# Confidence interval of coefficients
lm(mpg ~ wt + cyl, data=mtcars) %>% confint()

# Hypothesis testing of nested models
lm_mpg_wt <- lm(mpg ~ wt, data=mtcars)
lm_mpg_wt.cyl <- lm(mpg ~ wt + cyl, data=mtcars)
anova(lm_mpg_wt, lm_mpg_wt.cyl)

# convert mpg to kilometers per liter
mtcars %>% mutate(kmpl = mpg * 0.425144) %>% select(mpg, kmpl) %>% filter(mpg > 20)
nrow()
mtcars %>% group_by(am) %>% 
  summarize(n=n(),
            mean_mpg=mean(mpg),
            sd_mpg=sd(mpg),
            min_mpg=min(mpg),
            max_mpg=max(mpg)
mtcars %>% arrange(desc(mpg))

# mean & sd
mtcars %>% summarize(am_mean=mean(am), am_sd=sd(am))
# Frequencies by categories
mtcars %>% group_by(am) %>% tally

# Assume we want to combine LA + SD to Southern CA and Bay Area and Sacramento
## as Northern CA
(californiatod <- californiatod %>% 
  mutate(transit_level=case_when(
    transit>0.4~"high",
    transit>0.2~"medium",
    TRUE ~ "low")))


## General linear F test
fit_R <- lm(mpg ~ wt, data=mtcars)
fit_F <- lm(mpg ~ wt + cyl, data=mtcars)
anova(fit_R, fit_F)

SSE_R <- resid(fit_R)^2 %>% sum
SSE_F <- resid(fit_F)^2 %>% sum
df_R <- df.residual(fit_R)
df_F <- df.residual(fit_F)
F_val <- ((SSE_R - SSE_F)/(df_R - df_F))/(SSE_F/df_F)

# Look up the critical F value for alpha=0.05
alpha <- 0.05
qf(alpha, (df_R - df_F), df_F, lower.tail=F)
# Alternatively, find the p-value corresponding to our F_val
pf(F_val, (df_R - df_F), df_F, lower.tail=F)
n <- nrow(mtcars)          # number of observations
k <- length(coef(fit_R))   # number of coefficients
## Calculate R2 and adjusted R2 manually
TSS <- sd(mtcars$mpg)^2 * (n - 1)
# OR
TSS <- var(mtcars$mpg) * (n - 1)
(R2_R <- 1 - SSE_R/TSS)
(R2_R_adj <- 1 - (SSE_R/(n - k))/(TSS/(n - 1)))

# Interaction Terms
huxreg(
  lm(houseval ~ transit, data=californiatod),
  lm(houseval ~ transit * railtype, data=californiatod),
  lm(houseval ~ transit * region, data=californiatod),
  lm(houseval ~ transit * CA, data=californiatod))

# redefine the region variables with a new reference category (4 for SD)
catod2 <- californiatod %>% mutate(region = relevel(as.factor(region), ref = 4))
lm(houseval ~ region, data=catod2)  %>%  summary

# Partial F test:
catod3 <- californiatod %>% mutate(region = ifelse(region =="LA" | region == "SD", "LA_SD", region))
lm(houseval ~ region, data=catod3)  %>% summary
anova(lm(houseval ~ region, data=catod3), lm(houseval ~ region, data=californiatod))

# Hypothesis testing of linear combination of coefficients
car::lht(model_2_7_8, "x2 = x7")
# Partial F test:H0:β2+β2=0
car::lht(lm(hours ~ married*women, data=chores), "women + married:women = 0")

# linear combination of coefficients
# The point estimate is β2^+β3^ In this case, our linear combination involves the sum rather than the difference between two coefficients, and the formula for estimating the standard error of the sum of two coefficients is:
# $\sqrt{\hat{\sigma^2_{\hat{\beta_2}}} + \hat{\sigma^2_{\hat{\beta_3}}} + 2\hat{cov}_{\hat{\beta_2}\hat{\beta_3}}}$
fit1 <- lm(hours ~ married*women, data=chores)
beta2 <- coef(fit1)["women"]
beta3 <- coef(fit1)["married:women"]
betas_vcov <- vcov(fit1)
se <- sqrt(betas_vcov["women", "women"] + betas_vcov["married:women", "married:women"] + 2 * betas_vcov["women", "married:women"])
(t_stat <- (beta2 + beta3)/se)

## Degrees of Freedom
dof <- fit1$df.residual

## compare t_stat to critical t-value
(t_crit <- qt(0.025, df=dof, lower.tail = F))
## OR find the corresponding p-value
(p_val <- 2 * pt(t_stat, lower.tail = F, df=dof))

# Partial F test on the nonlinear term
anova(lm(houseval ~ density, data=californiatod),lm(houseval ~ density + I(density^2), data=californiatod))
#To be on the safe side, enclose your tranformation in an I() function. This is not necessary for log transformation.

library(olsrr)
# leverage (hat)
leverage <- ols_leverage(lm_sfr)
ols_rsdlev_plot(lm_sfr)
# Cook's distance
ols_cooksd_chart(lm_sfr)
# DFFITS
ols_dffits_plot(lm_sfr)
# DFBETAS
ols_dfbetas_panel(lm_sfr)
# Heteroskedasticity
ols_rvsp_plot(lm_sfr)
ols_rsd_qqplot(lm_sfr)
# hypothesis test of normality of residuals
ols_norm_test(lm_sfr
# Test of Heteroskedasticity with Breusch-Pagan Test
ols_bp_test(lm_sfr)
#Heteroskedasticity-Consistent Standard Errors
# standard variance-covariance matrix
vcov0 <- vcov(lm_sfr)
vcov(model_2_7_8)
# convert to correlation
vcov0
# Heteroskedasticity-Consistent variance covariance matrix
require(car)
vcov_hc3 <- hccm(lm_sfr, type="hc3")
# In presence of Heteroskedasticity, vcov_hc3 is larger than vcov0, to redo hypothesis tests
# with the Heteroskedasticity-Consistent variance covariance matrix
if (!require(lmtest)) install.packages("lmtest") & library(lmtest)
coeftest(lm_sfr, vcov_hc3)
# All possible subset
sfrmodel <- lm(TOTALVAL ~ BLDGSQFT + YEARBUILT + GIS_ACRES + dpioneer + dfwy + dpark + dmax + dbikehq, data = taxlot_sfr)
(sfrmodel_all_subset <- ols_all_subset(sfrmodel))
# Best Subset Regression
ols_best_subset(model_2_7_8)
# Multicollinary with VIF
ols_vif_tol(lm_sfr)
## Stepwise Forward Regression
# based on p-value
(sfrmodel_stepfwd.p <- ols_step_forward(sfrmodel))
# based on AIC
(sfrmodel_stepfwd.aic <- ols_stepaic_forward(sfrmodel))
## Stepwise Backward Regression
# based on p-value
(sfrmodel_stepbwd.p <- ols_step_backward(sfrmodel))
# based on AIC
(sfrmodel_stepbwd.aic <- ols_stepaic_backward(sfrmodel))
## Step AIC regression
# Build regression model from a set of candidate predictor variables by entering and removing predictors based on Akaike Information Criteria, in a stepwise manner until there is no variable left to enter or remove any more. The model should include all the candidate predictor variables.
(sfrmodel_stepboth.aic <- ols_stepaic_both(sfrmodel))

# Cross Validation: CV assesses how the results of a model will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.
library(modelr)
library(purrr)
(taxlot_sfr_kcv <- taxlot_sfr %>% 
  modelr::crossv_kfold() %>% 
  mutate(model=map(train, ~lm(TOTALVAL~BLDGSQFT+YEARBUILT+GIS_ACRES+dpioneer+dfwy, data=.x)),
         rmse=map2_dbl(model, test, modelr::rmse),
         rsquare=map2_dbl(model, test, modelr::rsquare)))
taxlot_sfr_kcv %>% 
  summarise_at(c("rmse", "rsquare"), funs(mean))

## DID omitted
## Discrete Outcome: Count/Poisson Regression
require(MASS)
require(huxtable)
fit_lm <- lm(carb ~ mpg + qsec, data=mtcars)
fit_glm <- glm(carb ~ mpg + qsec, data=mtcars, family="poisson")
huxreg(OLS=fit_lm, Poisson=fit_glm)

fit_lm <- lm(am ~ qsec + hp, data=mtcars)
fit_glm <- glm(am ~ qsec + hp, data=mtcars, family=binomial("logit"))
huxreg(OLS=fit_lm, logit=fit_glm)

# log Likelihood
logLik(fit_glm)
fit_glm0 <- update(fit_glm, .~1)
logLik(fit_glm0)
## 'log Lik.' -21.61487 (df=1)
# pseudo R2
1 - logLik(fit_glm)/logLik(fit_glm0)
## 'log Lik.' 0.381052 (df=3)
# Interpretation of coefficients
# odds ratio
(odds <- exp(coef(fit_glm)))
#prob
odds/(1 + odds)



huxtable::huxreg(model_2_7_8, statistics = NULL)





```
