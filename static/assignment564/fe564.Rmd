---
title: 'Shen Qu, 918881147'
author: "STAT564 Fall 2018 Final Exam"
date: "12/05/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---

```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

## {.tabset}

### Question 1 (Show main steps of your work to get full points)

The $(X'X)^{-1}$ for the $y=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+β_5 x_5+β_6 x_6+ε$ is given below.

```{R eval=FALSE, collapse=TRUE, include=FALSE}
# Problem 3
B<- data.frame(
   b0=c(3.694,	-2.166,	0.046,	-0.300,	-0.022,	-0.100,	0.023),
   b1=c(-2.166,	2.230,	-0.075,	0.166,	0.012,	-0.079,	-0.015),
   b2=c(0.046,	-0.075,	0.067,	-0.010,	-0.035,	-0.009,	-0.027),
   b3=c(-0.300,	0.166,	-0.010,	0.083,	-0.028,	-0.010,	-0.010),
   b4=c(-0.0216,	0.012,	-0.035,	-0.028,	0.069,	-0.003,	-0.003),
   b5=c(-0.100,	-0.079,	-0.099,	-0.010,	-0.003,	0.058,	0.015),
   b6=c(0.023,	-0.015,	-0.027,	-0.010,	-0.003,	0.015,	0.063)
   )
# Var($\hat b_4$)
1.395*B[5,5]
# se(b4)
sqrt(1.395*B[5,5])
# se(b2)
sqrt(1.395*B[3,3])
# cov(b2,b4)
1.395*B[3,5]
# cor(b2,b4)
(1.395*B[3,5])/((sqrt(1.395*B[3,3]))*(sqrt(1.395*B[5,5])))
```

(a) If MSE = 1.395 and n = 38 , compute the (Keep 4 or more decimal places, DO NOT round in the intermediate steps)

(i)   se(β ̂_4)

$$se(\mathbf{\hat\beta_4})=\sqrt{MSE\times C_{55}}=\sqrt{1.395\times0.069}=0.3102499$$

(ii)   Cov(β ̂_2,β ̂_4)

$$Cov(\mathbf{\hat\beta_2,\hat\beta_4})=MSE\times C_{35}=1.395\times(-0.035)=-0.048825$$

(iii)   Cor(β ̂_2,β ̂_4 )

$$se(\mathbf{\hat\beta_2})=\sqrt{MSE\times C_{33}}=\sqrt{1.395\times0.067}=0.3057205$$

$$Cor(\mathbf{\hat\beta_2,\hat\beta_4})=\frac{Cov(\mathbf{\hat\beta_2,\hat\beta_4})}{se(\mathbf{\hat\beta_2})se(\mathbf{\hat\beta_4})}=\frac{-0.048825}{0.3057205\times0.3102499}=-0.5147615$$

(iv) Without computing anything, explain which estimator is the most consistent.

$C_{66}=0.058$ has the smallest value. $\hatβ_5$ has the the least variance and the most consistent among the estimators.

(v) Without computing anything , list the pair(s) of estimators that are positively correlated. Provide a reason.

According to the $(X'X)^{(-1)}$,

$C_{13},\ C_{17},\ C_{24},\ C_{25},\ C_{67}$ are positive. 

The positively correlated pairs of parameters are

$\hatβ_0$ and $\hatβ_2$, 
$\hatβ_0$ and $\hatβ_6$, 
$\hatβ_1$ and $\hatβ_3$, 
$\hatβ_1$ and $\hatβ_4$, 
$\hatβ_5$ and $\hatβ_6$.


(b) Consider the following hypothesis: $H_0:  β_1=2β_3,β_2=β_3,β_5=0$

(i)  Report the T matrix, β vector and c vector along with their dimensions, and the rank of T matrix for testing the above hypothesis.

$$
\mathbf{T}=\begin{bmatrix} 0 & 1 & 0 & -2 & 0 & 0& 0 \\ 0 & 0 & 1 & -1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 \end{bmatrix}_{3\times7} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \\ \beta_5 \\ \beta_6 \end{bmatrix}_{7\times1}
\mathbf{C}=\begin{bmatrix} 0 \\ 0 \\ 0\end{bmatrix}_{3\times1} rank(T)=3
$$

(ii) Report the values of numerator and denominator degrees of freedom for the corresponding F test.
For F test, the numerator is MSR while denominator is MSE, thus

In this hypothesis,$y=β_0+2β_3x_1+β_3x_2+β_3x_3+β_4x_4+0x_5+β_6x_6+ε=β_0+β_3(2x_1+x_2+x_3)+β_4x_4+β_6x_6+ε$


The value of numerator is $r=df_{Reduced}-df_{Full}=n-(3+1)-[n-(6+1)]=3$

The denominator degrees of freedom is $df_{Full}=n-(k+1)=38-(6+1)=31$

(c) Show the following equation is an alternative form of the sum of squares of regreesion or model (SSR).

$$SSR=\sum_{i=1}^n(\hat y_i-\bar y)^2=\sum_{i=1}^n(\hat y_i^2-2\hat y_i\bar y+\bar y^2)=\sum_{i=1}^n\hat y_i^2-2\bar y\sum_{i=1}^n\hat y_i+\sum_{i=1}^n\bar y^2$$

$$=\sum_{i=1}^n\hat y_i^2-2\bar yn\frac{\sum_{i=1}^n\hat y_i}n+n\bar y^2=\sum_{i=1}^n\hat y_i^2-2\bar yn\bar y+n\bar y^2=\sum_{i=1}^n\hat y_i^2-n\bar y^2$$

### Question 2 (Use software to analyze the given data)

The data in the WaterFlow file are simulated data on peak rate of flow (in cfs) of water from six watersheds following storm episodes. The predictors are:

x1 : Area of watershed (mi2) 
x2 : Area impervious to water (mi2)  
x3 : Average slope of watershed (percent)   
x4 : Longest stream flow in watershed (1000s of feet)  
x5 : surface absorbency index, (0= complete absorbency, 100=no absorbency)  
x6 : estimated soil storage capacity (inches of water)   
x7 : Infiltration rate of water into soil (inches/hour)  
x8 : Rainfall (inches)   
x9 : Time period during which rainfall exceeded ¼ inch/hour  

(a) Create the matrix of scatterplots and compute the correlation matrix for all the variables. Copy and paste them here.

```{r echo=FALSE, fig.height=6, fig.width=8}
library(tidyverse)

table_wf <- read_table2("WaterFlow.txt")
library(GGally)
ggpairs(data=table_wf[c(1:10)])

```

****

(b) Based on scatterplots and correlation, explain which predictors are significantly related to (most likely to contribute to the variation in) the response variable.

Based on scatterplots and correlation, X2(0.666),X7(0.668),X1(0.781),X4(0.866) have medium to strong positive linear relationship to the response variable (Correlation coefficient is more than 0.6). X5(-0.62) have medium negative linear relationship to the response variable.

****

(c) Fit the full model.

$$\hat y=292.561-203.144X_1+ 1055.782X_2-49.24X_3+209.762X_4-10.197X_5-24.558X_6+142.778X_7+511.713X_8-301.872X_9$$

****

(i) Explain whether the overall model is significant at 5% significance level.

The fitted model is statistically significant at 5% significance level (p-value=0.0000). But most of the coefficients are not significent. This model is not the best fitted model.

****

(ii) Explain whether assumptions of random errors and model are satisfied. If there is a violation of those, then suggest reasonable methods to correct them.

- Residual Diagnostics: Use plots to examine residuals to validate OLS assumptions

There is some violation of assumptions about the errors: 

On the residual plot, there is a funnel pattern.

On the outlier and leverage plot, there are two outlier.

On the qq plot, most of points follow approximately straight line but have some positive skew. 

- Suggestion: Transform and other diagnostics.

I suggest using natural log of response to make a variance-stabilizing transformations. 

Other diagnostics of heteroskedasticity, variable selection, measures of influence also should be considered.

****

(iii) How much of the sum of squares is explained by rainfall, given that all the other regression coefficients are in the model?

Accroding to the  F test, the partial sum of squares explained by rainfall is 2209825, given that all the other regression coefficients are in the model.

****

(iv) Explain whether there is a problem of multicollinearity.

- Collinearity diagnostics: 

The model does have serious problems of multicollinearity. The VIF of variables X4(105.754708), X1(101.859709), X3(31.446394
), X7(20.53505) are larger than 10. 

It will be important to solve multicollinearity. However, X7, X1, and X4 have medium to strong positive linear relationship to the response variable. It is also dangerous to remove these variables. We should have more diagnostics and comparisons.

****

(v) Interpret the estimated coefficient of rainfall predictor of the full model using question context.

Coefficient of 511.713 suggests the peak rate of flow increases by 511.713 cubic feet per second when the rainfall increases by 1 inch and other variables are constants. 

****

(d) Create a new variable using natural log of response. Then fit the full model using this new variable as response.

$$\hat y=3.402256-0.013532X_1-1.023664X_2+0.177966X_3+0.108788X_4-0.009622X_5-0.389474X_6+4.233475X_7+0.63007X_8-0.462276X_9$$

(1) Explain whether the overall model is significant at 5% significance level.

The fitted model is statistically significant at 5% significance level (p-value=0000). But most of the coefficients are not significent. This model is not the best fitted model.

****

(2) Explain whether there is a problem of multicollinearity.

The variance-stabilizing transformations does not change the problem of multicollinearit.

The model still has serious problems of multicollinearity. The VIF of variables X4(105.754708), X1(101.859709), X3(31.446394
), X7(20.53505) are larger than 10. 

It will be important to solve multicollinearity. However, X7, X1, and X4 have medium to strong positive linear relationship to the response variable. It is also dangerous to remove these variables. We should have more diagnostics and comparisons.

****

(3) If you wanted to simplify this full model, explain which predictor you would eliminate first.

If


- Residual Diagnostics: 

Includes plots to examine residuals to validate OLS assumptions

There is no violation of assumptions about the errors (no pattern on residual plots and points follow approximately straight line on the qq plot). 

- Variable selection: 

Differnt variable selection procedures such as all possible regression, best subset regression, stepwise regression, stepwise forward regression and stepwise backward regression

- Heteroskedasticity: 

Tests for heteroskedasticity include bartlett test, breusch pagan test, score test and f test

- Measures of influence: 

Use different plots to detect and identify influential observations


- Collinearity diagnostics: 

VIF, Tolerance and condition indices to detect collinearity and plots for assessing mode fit and contributions of variables

The general approaches for dealing with multicollinearity include collecting additional data, model respecification (redefine the regressors, variable elimination), estimation methods (Ridge Regression, Principal-Component Regression)

"Variable elimination is often a highly effective technique. However, it may not provide a satisfactory solution if the regressors dropped from the model have significant explanatory power relative to
the response y. That is, eliminating regressors to reduce multicollinearity may damage the predictive power of the model." (p.304)



Predictor X1 is the number of rooms while X4 is the number of bedrooms in a house. A high correlation is expected between these two variables. 

 and hence predictor x6 is the first to remove. However, according to the variable names of x1 and x4, predictor x6 may contain same information contains in x7. The bedrooms are rooms in a house. 

Further, according to the correlation coefficients, x7 is less correlated with y than x6 is correlated. 

It is better to remove x7 first and check whether the multicollinearity is solved. If it was not solved, then definitely, x6 has to be removed first.



x1 : Area of watershed (mi2)  
x8 : Rainfall (inches)   

x9 : Time period during which rainfall exceeded ¼ inch/hour  
x4 : Longest stream flow in watershed (1000s of feet)  

x3 : Average slope of watershed (percent)  


x2 : Area impervious to water (mi2)  
x5 : surface absorbency index, (0= complete absorbency, 100=no absorbency)  
x7 : Infiltration rate of water into soil (inches/hour)  
x6 : estimated soil storage capacity (inches of water)  

****

(4) Use the forward selection method to find the best model (use α=0.15) and report the final fitted model with estimated coefficients here.

Stepwise Forward Regression based on p values (use α=0.15) 

Stepwise AIC Forwardd Regression

Full model

eliminated model

****

(5) Use the backward elimination method to find the best model (use α=0.05) and report the final fitted model with estimated coefficients here.

Stepwise Backward Regression based on p values (use α=0.05) 

Stepwise AIC Backward Regression

Full model

eliminated model

****

(6) Use best subsets method (6 models from each size) to find the best model for these data and report the final fitted model with estimated coefficients here.

Full model

eliminated model

***

(7) If the final models in the previous 3 methods are different, compare their model adequacy and suggest one best model.

Both models do not have a problem of multicollinearity (VIF <10), and violation of assumptions about the errors (no pattern on residual plots and points follow approximately straight line on the qq plot). 

The model with 4 predictors has a slightly higher (about by 2%) adjusted R square compared to the model with only x1 and x2. Further, x5 and x7 predictors are not statistically significant at 10% significance level (p values are 0.11479 and 0.10356, respectively). There is no significant pattern on the plot of studentized residuals versus predicted values from the model with only x1 and x2. The partial regression plots do not show nonlinear patterns and hence first-order terms are good enough. 

Finally, the model with 2 predictors is simpler that model with 4 predictors. Therefore, the best model will be


- Residual Diagnostics: 

Includes plots to examine residuals to validate OLS assumptions

There is no violation of assumptions about the errors (no pattern on residual plots and points follow approximately straight line on the qq plot). 

Residual QQ Plot
Residual Normality Test
Residual vs Fitted Values Plot
Residual Histogram

- Variable selection: 

Differnt variable selection procedures such as all possible regression, best subset regression, stepwise regression, stepwise forward regression and stepwise backward regression

- Heteroskedasticity: 

Tests for heteroskedasticity include bartlett test, breusch pagan test, score test and f test

Bartlett Test
Breusch Pagan Test
Score Test
F Test

- Measures of influence: 

Use different plots to detect and identify influential observations

Cook’s D Bar Plot
Cook’s D Chart
DFBETAs Panel
DFFITs Plot
Studentized Residual Plot
Standardized Residual Chart
Studentized Residuals vs Leverage Plot
Deleted Studentized Residual vs Fitted Values Plot
Hadi Plot
Potential Residual Plot

****

(8) Provide complete ANOVA table for the best model. Provide partial sum of squares, estimated coefficients, standard errors, p-values, 95% Bonferroni joint confidence intervals for the coefficients of the best model. Provide in a tabular form clearly.


****

(9) How much variation in the response is explained by the best model after taking number of data and regression coefficients in to account?



****

(10) Report the PRESS statistic of the best model.

About 72.93% of variation in predicting the sale price of houses in Erie, Pennsylvania.

****

(e) Report the complete code along with output here.



### 

#### (a) The matrix of scatterplots and the correlation matrix

```{r, eval=T, include = F}
# Load the package 
library(olsrr)
library(car)
# Import Data
table_wf <- read_table2("WaterFlow.txt")

```

#### (c) The fitted full model

```{r, eval=T, include = T}
# build the model
model_wf_full <- lm(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9, data=table_wf)
ols_regress(model_wf_full)
model_wf_full%>% summary()
Anova(model_wf_full)

```

#### (c) ii Residual diagnostics

```{r, eval=T, include = T}
#Model Fit Assessment
ols_plot_diagnostics(model_wf_full)

# Part & Partial Correlations
ols_test_correlation(model_wf_full) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_full) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

```

#### (c) iii The partial regression and nonlinear diagnostics

```{r, eval=T, include = T}
#Lack of Fit F Test
ols_pure_error_anova(lm(y~X8, data = table_wf))

# Variable Contributions
ols_plot_added_variable(model_wf_full)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_full)
```

#### (c) iv Collinearity diagnostics

```{r, eval=T, include = T}
# for full model
ols_coll_diag(model_wf_full)

```

#### (d)  The fitted log model

```{r, eval=T, include = T}
# build full log model
model_wf_full_log <- lm(log(y) ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9, data=table_wf)
ols_regress(model_wf_full_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_full_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_full_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_full_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

```

#### (d) (2) Collinearity diagnostics

```{r, eval=T, include = T}
# for log model
ols_coll_diag(model_wf_full_log)

# remove X4
ols_vif_tol(lm(log(y) ~ X1 + X2 + X3 + X5 + X6 + X7 + X8 + X9, data=table_wf))

# remove X1
ols_vif_tol(lm(log(y) ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9, data=table_wf))

# remove X3
ols_vif_tol(lm(log(y) ~ X1 + X2 + X4 + X5 + X6 + X7 + X8 + X9, data=table_wf))

# remove X7
ols_vif_tol(lm(log(y) ~ X1 + X2 + X3 + X4 + X5 + X6 + X8 + X9, data=table_wf))

# remove X5
ols_vif_tol(lm(log(y) ~ X1 + X2 + X3 + X4 + X6 + X7 + X8 + X9, data=table_wf))


# build X4 eliminated log model
model_wf_rm4_log <- lm(log(y) ~ X1 + X2 + X3 + X5 + X6 + X7 + X8 + X9, data=table_wf)
ols_regress(model_wf_rm4_log)
# build X1 eliminated log model
model_wf_rm1_log <- lm(log(y) ~ X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9, data=table_wf)
ols_regress(model_wf_rm1_log)

library(huxtable)
huxreg(model_wf_full, model_wf_full_log, model_wf_rm4_log, model_wf_rm1_log)

```

#### (d) (3) Variable selection

```{r, eval=T, include = T}
#Lack of Fit F Test

ols_pure_error_anova(lm(y~X1, data = table_wf))
ols_pure_error_anova(lm(y~X4, data = table_wf))

alias(lm(y ~ as.factor(X3) + as.factor(X4) + as.factor(X5) + as.factor(X6) + as.factor(X7), data=table_wf))

alias(lm(y ~ as.factor(X1) + as.factor(X8) , data=table_wf))

alias(lm(y ~ as.factor(X4) + as.factor(X9) , data=table_wf))

alias(lm(y ~ as.factor(X3) + as.factor(X6) + as.factor(X7) + as.factor(X8) + as.factor(X9) , data=table_wf))

```


#### (d) (4) Forward selection 

Stepwise Forward Regression for full model

```{r, eval=T, include = T}
# Stepwise Forward Regression based on p values (use α=0.15) #
ols_step_forward_p(model_wf_full_log, penter = 0.15)

# Stepwise AIC Forward Regression #
ols_step_forward_aic(model_wf_full_log)

```

Stepwise Forward Regression for X4 eliminated model

```{r, eval=T, include = T}
# Stepwise Forward Regression based on p values (use α=0.15) #
ols_step_forward_p(model_wf_rm4_log, penter = 0.15)
# Stepwise AIC Forward Regression #
ols_step_forward_aic(model_wf_rm4_log)
```

Stepwise Forward Regression for X1 eliminated model

```{r, eval=T, include = T}
# Stepwise Forward Regression based on p values (use α=0.15) #
ols_step_forward_p(model_wf_rm1_log, penter = 0.15)
# Stepwise AIC Forward Regression #
ols_step_forward_aic(model_wf_rm1_log)
```

#### (d) (5) Backward selection 

Stepwise Backward Regression for full model

```{r, eval=T, include = T}
# Stepwise Backward Regression based on p values (use α=0.05) #
ols_step_backward_p(model_wf_full_log, penter = 0.05)
# Stepwise AIC Backward Regression #
ols_step_backward_aic(model_wf_full_log)
```

Stepwise Backward Regression for X4 eliminated model

```{r, eval=T, include = T}
# Stepwise Backward Regression based on p values (use α=0.05) #
ols_step_backward_p(model_wf_rm4_log, penter = 0.05)
# Stepwise AIC Backward Regression #
ols_step_backward_aic(model_wf_rm4_log)
```

Stepwise Backward Regression for X1 eliminated model

```{r, eval=T, include = T}
# Stepwise Backward Regression based on p values (use α=0.05) #
ols_step_backward_p(model_wf_rm1_log, penter = 0.05)
# Stepwise AIC Backward Regression #
ols_step_backward_aic(model_wf_rm1_log)

```

#### (d) (6) Best Subset Regression

```{r echo=TRUE}
# For full model #
k <- ols_step_best_subset(model_wf_full_log)
k
plot(k)

# For X4 eliminated model #
k <- ols_step_best_subset(model_wf_rm4_log)
k
plot(k)

# For X1 eliminated model #
k <- ols_step_best_subset(model_wf_rm1_log)
k
plot(k)

```

#### (d) (7) Models Comparison

- Additional Regression

```{r echo=TRUE}

# All Possible Regression for full model #
k <- ols_step_all_possible(model_wf_full_log)
k
plot(k)

# All Possible Regression for X4 eliminated model #
k <- ols_step_all_possible(model_wf_rm4_log)
k
plot(k)

# All Possible Regression for X1 eliminated model #
k <- ols_step_all_possible(model_wf_rm1_log)
k
plot(k)

# Stepwise Regression based on p values for full model#
k <- ols_step_both_p(model_wf_full_log)
k
plot(k)

# Stepwise AIC Regression for full model#
k<- ols_step_both_aic(model_wf_full_log)
k
plot(k)

# Stepwise Regression based on p values for X4 eliminated model#
k <- ols_step_both_p(model_wf_rm4_log)
k
plot(k)

# Stepwise AIC Regression for X4 eliminated model#
k<- ols_step_both_aic(model_wf_rm4_log)
k
plot(k)

# Stepwise Regression based on p values for X1 eliminated model#
k <- ols_step_both_p(model_wf_rm1_log)
k
plot(k)

# Stepwise AIC Regression for X1 eliminated model#
k<- ols_step_both_aic(model_wf_rm1_log)
k
plot(k)

```

- Model 437896

```{r, eval=T, include = T}
# build model 437896
model_wf_437896_log <- lm(log(y) ~ X4 + X3 + X7 + X8 + X9 + X6, data=table_wf)
ols_regress(model_wf_437896_log)

# Collinearity Diagnostics #
ols_coll_diag(model_wf_437896_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_437896_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_437896_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_437896_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

# Variable Contributions
ols_plot_added_variable(model_wf_437896_log)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_437896_log)

```

- Model 437

```{r, eval=T, include = T}
# build model 437
model_wf_437_log <- lm(log(y) ~ X4 + X3 + X7, data=table_wf)
ols_regress(model_wf_437_log)

# Collinearity Diagnostics #
ols_coll_diag(model_wf_437_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_437_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_437_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_437_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

# Variable Contributions
ols_plot_added_variable(model_wf_437_log)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_437_log)

```

- Model 137689

```{r, eval=T, include = T}
# build model 137689
model_wf_137689_log <- lm(log(y) ~ X1 + X3 + X7 + X6 + X8 + X9, data=table_wf)
ols_regress(model_wf_137689_log)

# Collinearity Diagnostics #
ols_coll_diag(model_wf_137689_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_137689_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_137689_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_137689_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

# Variable Contributions
ols_plot_added_variable(model_wf_137689_log)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_137689_log)

```

- Other Models

```{r, eval=T, include = T}

# build X1*X8 eliminated log model
model_wf_18rm4_log <- lm(log(y) ~ X1*X8 + X3 + X6 + X7 + X9, data=table_wf)
summary(model_wf_18rm4_log)

# build X1*X8 eliminated log model
table_wf_resi <- table_wf%>% mutate(x1t8=X1*X8)
model_wf_1time8_log <- lm(log(y) ~ x1t8 + X3 + X6 + X7+ X9 , data=table_wf_resi)
summary(model_wf_1time8_log)

# build X1*X4 eliminated log model
table_wf_resi <- table_wf%>% mutate(x1t4=X1*X4)
model_wf_1time4_log <- lm(log(y) ~ x1t4 + X3 + X6 + X7+ X8+ X9, data=table_wf_resi)
summary(model_wf_1time4_log)

# build X1/X4 eliminated log model
table_wf_resi <- table_wf%>% mutate(x14=X1/X4)
model_wf_1per4_log <- lm(log(y) ~ x14 + X3 + X6 + X7+ X8+ X9, data=table_wf_resi)
summary(model_wf_1per4_log)

# build X4*X3 eliminated log model
model_wf_43rm1_log <- lm(log(y) ~ X9 + X4*X3 + X6 + X7 + X8 , data=table_wf)
summary(model_wf_43rm1_log)

# build X4*X9 eliminated log model
model_wf_49rm1_log <- lm(log(y) ~ X3 + X4*X9 + X6 + X7 + X8 , data=table_wf)
summary(model_wf_49rm1_log)

# build X4*X9 eliminated log model
model_wf_48rm1_log <- lm(log(y) ~ X3 + X4*X8 + X6 + X7 + X9 , data=table_wf)
summary(model_wf_48rm1_log)

# build X4*X9 eliminated log model
model_wf_47rm1_log <- lm(log(y) ~ X3 + X4*X7 + X6 + X9 + X8 , data=table_wf)
summary(model_wf_47rm1_log)

# build X4/X9 eliminated log model
table_wf_resi <- table_wf%>% mutate(x4p9=X4/X9)
model_wf_4per9_log <- lm(log(y) ~ X3 + x4p9 + X6 + X7 + X8 , data=table_wf_resi)
summary(model_wf_4per9_log)

# build X3/X4vX8*X9 eliminated log model
model_wf_34v89_log <- lm(log(y) ~ X3*X4 + X8*X9 + X6 + X7, data=table_wf_resi)
summary(model_wf_34v89_log)

# build X3/X4vX8*X9 eliminated log model
model_wf_34v89v67_log <- lm(log(y) ~ X3*X4 + X8*X9 + X6*X7, data=table_wf_resi)
summary(model_wf_34v89v67_log)

# build X8/X9vX4*X3 eliminated log model
table_wf_resi <- table_wf%>% mutate(x8p9=X8/X9)
model_wf_8per9v43_log <- lm(log(y) ~ x8p9 + X4*X3 + X6 + X7, data=table_wf_resi)
summary(model_wf_8per9v43_log)

# build X6/7vX8/X9vX4X3 eliminated log model
table_wf_resi <- table_wf%>% mutate(x4t3=X4*X3,x8p9=X8/X9,x6p7=X6/X7)
model_wf_6p7v8p9v4t3_log <- lm(log(y) ~ x8p9 + x4t3 + x6p7, data=table_wf_resi)
summary(model_wf_6p7v8p9v4t3_log)

library(huxtable)
huxreg(model_wf_8per9v43_log, model_wf_43rm1_log, model_wf_6p7v8p9v4t3_log, model_wf_34v89_log, model_wf_34v89v67_log)

```


#### Final models

```{r, eval=T, include = T}

# Check model 43rm1
summary(model_wf_43rm1_log)
Anova(model_wf_43rm1_log)

# Collinearity Diagnostics #
ols_coll_diag(model_wf_43rm1_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_43rm1_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_43rm1_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_43rm1_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

# Variable Contributions
ols_plot_added_variable(model_wf_43rm1_log)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_43rm1_log)

##################################################
# Check model 8per9v43
summary(model_wf_8per9v43_log)
Anova(model_wf_8per9v43_log)

# Collinearity Diagnostics #
ols_coll_diag(model_wf_8per9v43_log)

#Model Fit Assessment
ols_plot_diagnostics(model_wf_8per9v43_log)

# Part & Partial Correlations
ols_test_correlation(model_wf_8per9v43_log) # Correlation between observed residuals and expected residuals under normality.

# Residual Normality Test
ols_test_normality(model_wf_8per9v43_log) # Test for detecting violation of normality assumption. #If p-value is bigger, then no problem of non-normality #

# Variable Contributions
ols_plot_added_variable(model_wf_8per9v43_log)

# Residual Plus Component Plot
ols_plot_comp_plus_resid(model_wf_8per9v43_log)


# Check PRESS Statistic
ols_press(model_wf_full)
ols_press(model_wf_full_log)
ols_press(model_wf_437896_log)
ols_press(model_wf_437_log)
ols_press(model_wf_137689_log)
ols_press(model_wf_43rm1_log)
ols_press(model_wf_8per9v43_log)
# prediction power
ols_pred_rsq(model_wf_437896_log)
ols_pred_rsq(model_wf_137689_log)
ols_pred_rsq(model_wf_43rm1_log)
ols_pred_rsq(model_wf_8per9v43_log)
```



####

```{r, eval=F, include = T}

library(texreg)
# Pretty print regression results on screen
lm(mpg ~ wt, data=my_df) %>% screenreg
texreg::screenreg(l=list(model_2_7_8))

# visualizng
library(GGally)
ggpairs(data=table_b1[c(1,3,8,9)])

# Correlation
cor(table_b1)

# Half correlation matrix:
library(corrr)
mtcars %>% correlate() %>% shave() %>% fashion()
# Visulize correlation matrix:
mtcars %>% correlate() %>% shave() %>% rplot()

# Scatterplot Matrix
mtcars[1:6] %>% plot
# Better looking version
library(ggfortify)
model_2_7_8 %>% autoplot()

# Confidence interval of coefficients
lm(mpg ~ wt + cyl, data=mtcars) %>% confint()

# Hypothesis testing of nested models
lm_mpg_wt <- lm(mpg ~ wt, data=mtcars)
lm_mpg_wt.cyl <- lm(mpg ~ wt + cyl, data=mtcars)
anova(lm_mpg_wt, lm_mpg_wt.cyl)

# convert mpg to kilometers per liter
mtcars %>% mutate(kmpl = mpg * 0.425144) %>% select(mpg, kmpl) %>% filter(mpg > 20)
nrow()
mtcars %>% group_by(am) %>% 
  summarize(n=n(),
            mean_mpg=mean(mpg),
            sd_mpg=sd(mpg),
            min_mpg=min(mpg),
            max_mpg=max(mpg)
mtcars %>% arrange(desc(mpg))

# mean & sd
mtcars %>% summarize(am_mean=mean(am), am_sd=sd(am))
# Frequencies by categories
mtcars %>% group_by(am) %>% tally

# Assume we want to combine LA + SD to Southern CA and Bay Area and Sacramento
## as Northern CA
(californiatod <- californiatod %>% 
  mutate(transit_level=case_when(
    transit>0.4~"high",
    transit>0.2~"medium",
    TRUE ~ "low")))


## General linear F test
fit_R <- lm(mpg ~ wt, data=mtcars)
fit_F <- lm(mpg ~ wt + cyl, data=mtcars)
anova(fit_R, fit_F)

SSE_R <- resid(fit_R)^2 %>% sum
SSE_F <- resid(fit_F)^2 %>% sum
df_R <- df.residual(fit_R)
df_F <- df.residual(fit_F)
F_val <- ((SSE_R - SSE_F)/(df_R - df_F))/(SSE_F/df_F)

# Look up the critical F value for alpha=0.05
alpha <- 0.05
qf(alpha, (df_R - df_F), df_F, lower.tail=F)
# Alternatively, find the p-value corresponding to our F_val
pf(F_val, (df_R - df_F), df_F, lower.tail=F)
n <- nrow(mtcars)          # number of observations
k <- length(coef(fit_R))   # number of coefficients
## Calculate R2 and adjusted R2 manually
TSS <- sd(mtcars$mpg)^2 * (n - 1)
# OR
TSS <- var(mtcars$mpg) * (n - 1)
(R2_R <- 1 - SSE_R/TSS)
(R2_R_adj <- 1 - (SSE_R/(n - k))/(TSS/(n - 1)))

# Interaction Terms
huxreg(
  lm(houseval ~ transit, data=californiatod),
  lm(houseval ~ transit * railtype, data=californiatod),
  lm(houseval ~ transit * region, data=californiatod),
  lm(houseval ~ transit * CA, data=californiatod))

# redefine the region variables with a new reference category (4 for SD)
catod2 <- californiatod %>% mutate(region = relevel(as.factor(region), ref = 4))
lm(houseval ~ region, data=catod2)  %>%  summary

# Partial F test:
catod3 <- californiatod %>% mutate(region = ifelse(region =="LA" | region == "SD", "LA_SD", region))
lm(houseval ~ region, data=catod3)  %>% summary
anova(lm(houseval ~ region, data=catod3), lm(houseval ~ region, data=californiatod))

# Hypothesis testing of linear combination of coefficients
car::lht(model_2_7_8, "x2 = x7")
# Partial F test:H0:β2+β2=0
car::lht(lm(hours ~ married*women, data=chores), "women + married:women = 0")

# linear combination of coefficients
# The point estimate is β2^+β3^ In this case, our linear combination involves the sum rather than the difference between two coefficients, and the formula for estimating the standard error of the sum of two coefficients is:
# $\sqrt{\hat{\sigma^2_{\hat{\beta_2}}} + \hat{\sigma^2_{\hat{\beta_3}}} + 2\hat{cov}_{\hat{\beta_2}\hat{\beta_3}}}$
fit1 <- lm(hours ~ married*women, data=chores)
beta2 <- coef(fit1)["women"]
beta3 <- coef(fit1)["married:women"]
betas_vcov <- vcov(fit1)
se <- sqrt(betas_vcov["women", "women"] + betas_vcov["married:women", "married:women"] + 2 * betas_vcov["women", "married:women"])
(t_stat <- (beta2 + beta3)/se)

## Degrees of Freedom
dof <- fit1$df.residual

## compare t_stat to critical t-value
(t_crit <- qt(0.025, df=dof, lower.tail = F))
## OR find the corresponding p-value
(p_val <- 2 * pt(t_stat, lower.tail = F, df=dof))

# Partial F test on the nonlinear term
anova(lm(houseval ~ density, data=californiatod),lm(houseval ~ density + I(density^2), data=californiatod))
#To be on the safe side, enclose your tranformation in an I() function. This is not necessary for log transformation.

library(olsrr)
# leverage (hat)
leverage <- ols_leverage(lm_sfr)
ols_rsdlev_plot(lm_sfr)
# Cook's distance
ols_cooksd_chart(lm_sfr)
# DFFITS
ols_dffits_plot(lm_sfr)
# DFBETAS
ols_dfbetas_panel(lm_sfr)
# Heteroskedasticity
ols_rvsp_plot(lm_sfr)
ols_rsd_qqplot(lm_sfr)
# hypothesis test of normality of residuals
ols_norm_test(lm_sfr
# Test of Heteroskedasticity with Breusch-Pagan Test
ols_bp_test(lm_sfr)
#Heteroskedasticity-Consistent Standard Errors
# standard variance-covariance matrix
vcov0 <- vcov(lm_sfr)
vcov(model_2_7_8)
# convert to correlation
vcov0
# Heteroskedasticity-Consistent variance covariance matrix
require(car)
vcov_hc3 <- hccm(lm_sfr, type="hc3")
# In presence of Heteroskedasticity, vcov_hc3 is larger than vcov0, to redo hypothesis tests
# with the Heteroskedasticity-Consistent variance covariance matrix
if (!require(lmtest)) install.packages("lmtest") & library(lmtest)
coeftest(lm_sfr, vcov_hc3)
# All possible subset
sfrmodel <- lm(TOTALVAL ~ BLDGSQFT + YEARBUILT + GIS_ACRES + dpioneer + dfwy + dpark + dmax + dbikehq, data = taxlot_sfr)
(sfrmodel_all_subset <- ols_all_subset(sfrmodel))
# Best Subset Regression
ols_best_subset(model_2_7_8)
# Multicollinary with VIF
ols_vif_tol(lm_sfr)
## Stepwise Forward Regression
# based on p-value
(sfrmodel_stepfwd.p <- ols_step_forward(sfrmodel))
# based on AIC
(sfrmodel_stepfwd.aic <- ols_stepaic_forward(sfrmodel))
## Stepwise Backward Regression
# based on p-value
(sfrmodel_stepbwd.p <- ols_step_backward(sfrmodel))
# based on AIC
(sfrmodel_stepbwd.aic <- ols_stepaic_backward(sfrmodel))
## Step AIC regression
# Build regression model from a set of candidate predictor variables by entering and removing predictors based on Akaike Information Criteria, in a stepwise manner until there is no variable left to enter or remove any more. The model should include all the candidate predictor variables.
(sfrmodel_stepboth.aic <- ols_stepaic_both(sfrmodel))

# Cross Validation: CV assesses how the results of a model will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.
library(modelr)
library(purrr)
(taxlot_sfr_kcv <- taxlot_sfr %>% 
  modelr::crossv_kfold() %>% 
  mutate(model=map(train, ~lm(TOTALVAL~BLDGSQFT+YEARBUILT+GIS_ACRES+dpioneer+dfwy, data=.x)),
         rmse=map2_dbl(model, test, modelr::rmse),
         rsquare=map2_dbl(model, test, modelr::rsquare)))
taxlot_sfr_kcv %>% 
  summarise_at(c("rmse", "rsquare"), funs(mean))

## DID omitted
## Discrete Outcome: Count/Poisson Regression
require(MASS)
require(huxtable)
fit_lm <- lm(carb ~ mpg + qsec, data=mtcars)
fit_glm <- glm(carb ~ mpg + qsec, data=mtcars, family="poisson")
huxreg(OLS=fit_lm, Poisson=fit_glm)

fit_lm <- lm(am ~ qsec + hp, data=mtcars)
fit_glm <- glm(am ~ qsec + hp, data=mtcars, family=binomial("logit"))
huxreg(OLS=fit_lm, logit=fit_glm)

# log Likelihood
logLik(fit_glm)
fit_glm0 <- update(fit_glm, .~1)
logLik(fit_glm0)
## 'log Lik.' -21.61487 (df=1)
# pseudo R2
1 - logLik(fit_glm)/logLik(fit_glm0)
## 'log Lik.' 0.381052 (df=3)
# Interpretation of coefficients
# odds ratio
(odds <- exp(coef(fit_glm)))
#prob
odds/(1 + odds)

huxtable::huxreg(model_2_7_8, statistics = NULL)

library(leaps) # Load the package #
model_wf_subset <- regsubsets(log(y) ~X2 + X3 +X4 + X5 + X6 + X7 + X8 + X9, data=table_wf, nbest=10 ) # nbest is the number of models from each size #
summary(model_wf_subset) # Hard to read output from this #

## plot adjusted R square for each model ##
plot(model_wf_subset, scale='adjr2')
## can use Cp, r2 or bic for scale ##
plot(model_wf_subset, scale='bic')
plot(model_wf_subset, scale='Cp')

shapiro.test(rstudent(model_wf_reduce_log)) #If p-value is bigger, then no problem of non-normality #
shapiro.test(rstudent(model_wf_reduce_log))

table_wf_resi <- table_wf %>% mutate(student_residual=rstudent(model_wf_reduce_log))
ggpairs(data=table_wf_resi[c(10,3,4,6,7,8,9,11)])

table_wf_resi <- table_wf %>% mutate(student_residual=rstudent(model_wf_reduce_log))
ggpairs(data=table_wf_resi[c(10,3,4,7,11)])


Anova(model_wf_final)
vif(model_wf_final)

confint(model_wf_final, level=0.05/1) # Bonferroni joint confidence interval #

plot(model_wf_final, pch=16, col="blue")
#Create Partial Regression plots #
avPlots(model_wf_final)

confint(model_wf_437, level=0.05/1) # Bonferroni joint confidence interval #

plot(model_wf_437, pch=16, col="blue")
#Create Partial Regression plots #
avPlots(model_wf_437)


deviation <- table_wf$y-mean(table_wf$y)

# Predit_Power=1-(PRESS.stat/SST)
1-((MPV::PRESS(model_wf_final))/(deviation%*%deviation)) # Compute SST by multiplying two vectors #
# prediction power of full
1-((MPV::PRESS(model_wf_reduce_log))/(var(table_wf$y)*(nrow(table_wf)-1)))
# prediction power of 437
1-((MPV::PRESS(model_wf_437))/(var(table_wf$y)*(nrow(table_wf)-1)))
# prediction power of backward
1-((MPV::PRESS(model_wf_final))/(var(table_wf$y)*(nrow(table_wf)-1)))
1-((ols_press(model_wf_437896_log))/(var(log(table_wf$y))*(nrow(table_wf)-1)))
```
