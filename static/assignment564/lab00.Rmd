---
title: 'STAT564 Lab 1'
author: "Shen Qu"
date: "10/08/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---



```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```


# {.tabset .tabset-fade .tabset-pills}

## Lab 0

### EXAMPLE

These data appeared in the Wall Street Journal. The advertisement were selected by an annual survey conducted by Video Board Tests, Inc., a New York ad-testing company, based on interviews with 20,000 adults who were asked to name the most outstanding TV commercial they had seen, noticed, and liked. The retained impressions were based on a survey of 4,000 adults, in which regular product users were asked to cite a commercial they had seen for that product category in the past week. There are 21 observations on three variables. 

**Variables:**
Firm The name of the firm
Spend TV advertising budget, 1983 ($ millions)
MilImp Millions of retained impressions per week

These data is saved in text file (.txt) called **TVads** which will be imported to R as shown in th efollowing code. The path to the place the text file is saved must be changed accordingly. 
```{r, echo=TRUE}
TVads <- read.table("TVads.txt", header= TRUE) 
head(TVads) # This will print only the first 6 rows of data file 
TVads # This will print the complete data file in the Console Window 

# The following plot function creates the scatterplot shown in Figure 1 below
plot(TVads$Spend, TVads$MilImp, pch=16, xlab = "TV advertising budget, 1983 ($ millions) ", ylab = "Millions of retained impressions per week", main="Figure 1: Scatterplot of Millions of retained impressions per week 
     and TV advertising budget", cex.main=0.9)
```



Simple linear regression analysis in R 
```{r, echo=TRUE}
model <- lm(MilImp ~ Spend, data = TVads) # The lm function is used to fit simple linear regression model
model# This displays only the values of estimated regression coefficients
```

```{r, echo=TRUE}
summary(model) # The summary function is used to get more information about the fitted regression model #
```
The output shows estimated regression coefficients, their standard errors (deviations), test statistics & p-value for testing significance. Also, there is R squared, degrees of freedom and F test statistic for testing significance of fitted model.
You can use the abline function as shown below to plot the fitted regression model on the scatterplot.
```{r, echo=TRUE}
plot(TVads$Spend, TVads$MilImp, pch=16, xlab = "TV advertising budget, 1983 ($ millions) ", ylab = "Millions of retained impressions per week", main="Figure 2: Fitted rgeression model oevrlayed on the scatterplot of 
     Millions of retained impressions per week and TV advertising budget", cex.main=0.9) # Create the scatterplot
abline(model, col = "red", lwd= 3) # Draw the fitted line #
```


The 95% CI for parameter estimates can be obtained using the following function: 
```{r, echo=TRUE}
confint(model, level = 0.95)
```

The 95% confidence interval for slope of the regression line is (0.1599, 0.5664).
If you want to get the predicted value of response for each value of predictor, then use the **predict** function as shown below.
```{r, echo=TRUE}
predict(model, TVads)
```

This same function can be used to get prediction interval for the each value of predictor as shown below. 

```{r, echo=TRUE}
predict(model, TVads, interval = "prediction", level = 0.95)
```

You may want to compute prediction values of response for a given set of new values of predictor. In that case, first, create a new data file with the new value of predictor. Then use the predictor function to predict response as shown below. Here, our new values of the Spend variable are 50 and 100.
```{r, echo=TRUE}
NewData <- data.frame(Spend = c(50, 100)) # Creates a new data frame with only new values of predictor variable
predict(model, NewData, interval = "prediction", level = 0.95) # Use the new data frame in the Predictor function instead of old data set
```

The above output shows that there is $42.32 million of average retained impressions per week when TV advertisement budget is $ 50 million.
To check the model diagnostics using residual plots, use the code below.
```{r, echo=TRUE}
plot(model) # This plots the diagnostic graphs for the fitted regression model. Fit enter key 4 times to see the plots shown below #
```

When you want to do multiple linear regression in R, you can add other predictors to the lm function each separated by a plus sign.

### The Solution by tidyverse
```{r, eval=FALSE}
install.packages("tidyverse")
```

To use R packages you have installed, include this line in your script or in the Console:

```{r, echo=TRUE}
library("tidyverse")
# Note here the quotation marks are optional and
# it is the same as
library(tidyverse)
```

#### Import Data

1. Download the data file [here](TVads.txt) .
2. What format is the data file in?
3. Import the data in the file into R as a data frame:

```{r, echo=TRUE}
my_df <- read_table2("TVads.txt")
my_df
```

Of course, the step of import data into your statistic software vary a little by software and by the data format. 

More information as of how to import data into R can be found at [Import Data]()

#### Descriptive Statistics

#### Visualization

1. Visualize a single variable
```{r, echo=TRUE}
ggplot(my_df, aes(x=MilImp)) + geom_histogram()
```

```{r, echo=TRUE}
ggplot(my_df, aes(x=Spend)) + geom_histogram()
```

2. Visualize a pair of numeric variables
```{r, echo=TRUE}
ggplot(my_df, aes(x=Spend, y=MilImp)) + geom_point()
```

#### Correlation

```{r, echo=TRUE}
cor(my_df$MilImp, my_df$Spend)
```

#### Regression
In R, run linear regressions with `lm` (short for **l**inear **m**odel):

```{r, echo=TRUE}
lm(MilImp ~ Spend, data=my_df)
```

#### More detailed regression results

1. Pass the results from `lm()` to `summary()` for more detailed information:
```{r, echo=TRUE}
lm(MilImp ~ Spend, data=my_df) %>%
  summary
```

For better formatting of the results (pretty print), we can use the `texreg` package:
```{r, echo=TRUE}
## Install and load texreg package
install.packages("texreg")
library(texreg)

# Pretty print regression results on screen
lm(MilImp ~ Spend, data=my_df) %>%
  screenreg

# Save regression results to a html file
lm(MilImp ~ Spend, data=my_df) %>%
  htmlreg(file="lm_MilImp-Spend.html")
```

#### Visualize regression results

```{r, echo=TRUE}
ggplot(my_df, aes(x=Spend, y=MilImp)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

#### Diagnostic plots of regression

We will use the `ggfortify` package to generate the diagnostic plots for regression
```{r, echo=TRUE}
## Install and load ggfortify package
install.packages("ggfortify")
library(ggfortify)

lm(MilImp ~ Spend, data=my_df) %>%
  autoplot()

# Since we need the regression results in many places, 
# it would be easier to save it into a R variable
MilImp_lm <- lm(MilImp ~ Spend, data=my_df)

# Save the diagnostic plots as a png file
ggsave("MilImp_lm_diag.png")

```



## lab 1

A hospital is implementing a program to improve service quality and productivity. As part of this program the hospital management is attempting to measure and evaluate patient satisfaction. Table B.17 contains some of the data that have been collected on a random sample of 25 recently discharged patients.
Variables:
Satisfaction  - a subjective measure on an increasing scale
Age – age of patient in years
Severity – an index measuring the severity of the patient’s illness
Surgical-Medical - an indicator of whether the patient is a surgical or medical patient (0 = surgical, 1 = medical)
Anxiety - an index measuring the patient’s anxiety level
The patient satisfaction is thought to be related to the patient age.



 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b17 <- read_xlsx("TableB.17.xlsx")

# Observe the data frame
head(table_b17)

```

### (a) Create a scatterplot of satisfaction and age variables. Copy and paste it here. 


```{r}
# visualizng Satisfaction and Age
table_b17 %>% ggplot(aes(Age,Satisfaction))+geom_point()+geom_smooth(method="lm", level=0.95)
```

(b) Describe the relationship between patient satisfaction and patient age based on the scatterplot.

 > The plot shows a decreasing approximately linear pattern.

(c) Fit a simple linear regression model to the data. Write the fitted regression model with estimated coefficients. Edit the following math equation to write your fitted model (replace y and x with appropriate variable names).

```{r}
# build the model
model_satis_age <- lm(Satisfaction ~ Age, data=table_b17)
model_satis_age
```

 > The model is $Satisfaction=130.221-1.249Age$

(d) The coefficient of determination, denoted by R^2, (Multiple R-squared in R) provides the percentage of total variation in response variable explained by the fitted model. It can be any value from 0 to 100% and a higher value is better.
What is the percentage of total variation in the patient satisfaction explained by the fitted model for these data?

```{r}
model_satis_age %>% summary()
```

 > According to the analysis-of-variance table, Multiple R-squared is 0.7205. The fitted model for these data can explain 72.05% of total variation in the patient satisfaction. 
 
(e) Report the standard error of estimated intercept.

 > the standard error of estimated intercept is 7.7775.

(f) Using the p-value reported along with regression coefficients in your software output, explain whether the true slope of the fitted model is significant at 5% significance level. Report the p-value as well.

 > The table shows that p-value is less than 1.52e-08. The regression model is significant at 95% significance level.

(g) Report a 95% confidence interval for the true intercept of the model. Based on confidence interval, explain whether the true intercept is different from zero.

 > The equation for the 95% CI for the estimated β1 is defined as:

$$\beta_1 \pm t_{\alpha / 2, n - 2} \left(\frac{\sqrt{MSE}}{\sqrt{\sum (x_i - \bar{x})^2}}\right)$$

```{r}
model_satis_age %>% confint(level=0.95)
```

 > The fitted β0 is 130.2209. The 95% confidence interval for the intercept of the regression line is (114.131844, 146.3100181).  
The confidence interval for β0 does not contain 0, it can be concluded the true intercept is different from zero.

(h) Construct the analysis of variance (ANOVA) table using software and test for significance of regression model. Report the p-value.

```{r}
anova(model_satis_age)
```

  > The table shows that p-value is less than 1.52e-08. The regression model is significant at 95% significance level.

(i) Compute the point estimate of the mean patient satisfaction when the patient is 65 years old.

> The confidence interval around the mean response, denoted $\mu_y$, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.
 
```{r}
model_satis_age %>% predict(newdata = data.frame(Age=65), interval = "confidence", level=0.95)
```

 > From the output, the fitted satisfaction is 49.03367 when the patient's age is 65 years old. 

(j) Compute 95% confidence interval for the mean (average) patient satisfaction when the patient is 65 years old. (Confidence interval is for mean or average response) 
 
 > According the result from (i), the confidence interval of (42.86397, 55.20336) signifies the range in which the true population parameter lies at a 95% level of confidence.
 
(k) Compute the 95% prediction interval for the patient satisfaction when the patient is 65 years old. (Prediction interval is for a new value of response and not for mean response)

```{r}
model_satis_age %>% predict(newdata = data.frame(Age=65), interval = "prediction", level=0.95)
```

 > the 95% prediction interval is (26.11012, 71.95721) for the patient satisfaction when the patient is 65 years old

(l)  Compare the two intervals in the previous two questions and explain why they are same or different.

 > The results show the prediction interval is wider than confidence interval due to the additional term in the standard error of prediction. Prediction and confidence intervals are similar in that they are both predicting a response, however, they differ in what is being represented and interpreted.The best predictor of a random variable (assuming the variable is normally distributed) is the mean $\mu$. The best predictor for an observation from a sample of x data points, x1,x2,⋯,xn and error $\epsilon$ is $\bar x$.
The prediction interval depends on both the error from the fitted model and the error associated with future observations.

(m) Use the estimated slope and its standard error from software output to test whether the true slope is different from -1 (negative 1). Provide all the steps of test.

```{r}
t0= (-1.2490+1)/0.1471
2*pt(q = t0, df = nrow(table_b17)-2 , lower.tail = TRUE) # Gives the P(T < t0) from the t distribution. Enter the value for df of the t distribution. Use FALSE instead of TRUE to compute P(T >t0). #
```

 > The result shows that the p_value is 0.1040118, which is bigger than 0.05. Therefore, we can reject the H1 and the ture slope might be same with -1.
 

## lab 2

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_plant <- read_table2("Plant.txt")
# Observe the data frame
head(table_plant)
# visualizng Satisfaction and Age
table_plant %>% ggplot(aes(SolarRadiation,PlantBiomass))+geom_point()+geom_smooth(method="lm", level=0.95)
# build the model
model_solar_mass <- lm(PlantBiomass ~ SolarRadiation, data=table_plant)
model_solar_mass%>% summary()
confint(model_solar_mass, level = 0.99)
anova(model_solar_mass)
model_solar_mass_origin <- lm(PlantBiomass ~ SolarRadiation+0, data=table_plant)
model_solar_mass_origin%>% summary()
anova(model_solar_mass_origin)
```

## lab 3

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_wine <- read_csv("TableB11.csv")
# Observe the data frame
head(table_wine)
summary(table_wine)
# visualizng Satisfaction and Age
plot(table_wine,pch=16)

# matrix of correlation coefficients
round(cor(table_wine),digits = 3)
table_wine %>% corrr::correlate() %>% corrr::fashion()
table_wine %>% corrr::correlate() %>% corrr::rplot()

# build the model
model_wine_quality <- lm(Quality ~ Clarity + Aroma + Body + Flavor + Oakiness + Region, data=table_wine)
model_wine_quality%>% summary()
anova(model_wine_quality)

car::Anova(model_wine_quality, type="II")


# two kinds of confident intervals
confint(model_wine_quality, level = 0.95)

confint(model_wine_quality, level = 1-(0.05/7))

# Scheffe joint confidence interval
B.hat_wine <- coefficients(model_wine_quality)
B.StdErr_wine <- coef(summary(model_wine_quality))[,2]
df1_wine <- length(B.hat_wine)
df2_wine <- nrow(table_wine)-df1_wine
F_wine <- qf(0.05,df1_wine,df2_wine)

cbind((B.hat_wine-sqrt(2*F_wine)*B.StdErr_wine),(B.hat_wine+sqrt(2*F_wine)*B.StdErr_wine))

# Prediction
predict(model_wine_quality,data.frame(Clarity=0.9, Aroma=4.7, Body=6.1, Flavor=5.2, Oakiness=3, Region=2),interval = "confidence", level=0.99)

predict(model_wine_quality,data.frame(Clarity=0.9, Aroma=4.7, Body=6.1, Flavor=5.2, Oakiness=3, Region=2),interval = "prediction", level=0.99)

# Obtain the Design Matrix (X) for the regression model #
# Include only the predictors in the model and no response #
x_wine <- model.matrix(~ Clarity + Aroma + Body + Flavor + Oakiness + Region, data =
table_wine)
head(x_wine) # Display the design matrix #
xpx.inv_wine <- solve(t(x_wine)%*%x_wine) # Computes Inverse of (X'X) matrix #

1.395*xpx.inv_wine # cov(\beta)= MSE * (X'X)^-1 #
vcov(model_wine_quality) # another way getting cov(\beta)



# Check confidence and prediction interval of response #
x0_wine <- c(1,0.9,4.7, 6.1, 5.2, 3, 2)
predict_x0_wine <- t(x0_wine)%*%B.hat_wine
predict.stderr_CI_wine <- sqrt(1.395*t(x0_wine)%*%xpx.inv_wine%*%x0_wine)
predict.CI_wine <- c((predict_x0_wine - qt(0.995, 38-7)*predict.stderr_CI_wine), (predict_x0_wine + qt(0.995, 38-
7)*predict.stderr_CI_wine))
predict.CI_wine
predict.stderr_PI_wine <- sqrt(1.395*(1+(t(x0_wine)%*%xpx.inv_wine%*%x0_wine)))
predict.PI <- c((predict_x0_wine - qt(0.995, 38-7)*predict.stderr_PI_wine), (predict_x0_wine + qt(0.995, 38-
7)*predict.stderr_PI_wine))
predict.PI

# Test significance of linear function of regression coefficients #
# The linearHypothesis function defined in the car package can be used for this #

T_b4_wine <- c(0,0,0,0,1,0,0) # Define T matrix for testing B4=0 versus B4 not= 0 #
T_b4_wine <- c(rep(0,4),1,0,0) # Define T matrix for testing B4=0 versus B4 not= 0 #
T_b4_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix =T_b4_wine , rhs=0)
## another way
car::lht(model_wine_quality, "Flavor=0 ")

# The rhs option specifies value on the right hand side of hypotheses #
T_2b2_b3_wine <- c(0, 0, 2, -1, 0, 0, 0) # Define T matrix for testing 2B2=B3 versus 2B2 not= B3 #
T_2b2_b3_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix = T_2b2_b3_wine, rhs=0) 

## another way
car::lht(model_wine_quality, "2*Aroma=Body ")

#Test T2=0 versus T2 not= 0 #
# Define T matrix for testing the above hypotheses simultaneously #
# Use rbind function to bind two rows, The rep function is used to create replications of the same value #
T_b4.2b2_b3_wine <- rbind(c(rep(0,4), 1, 0, 0), c(0, 0, 2, -1, 0, 0, 0))
T_b4.2b2_b3_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix = T_b4.2b2_b3_wine, rhs=c(0,0)) # Test the above two hypotheses simultaneously #
## another way
car::lht(model_wine_quality, c("Flavor=0", "2*Aroma=Body"))

# Check for multicollinearity #
# Use the vif function defined in the car package #
car::vif(mod = model_wine_quality)

## another way
#if (!require(olsrr))  install.packages("olsrr") & require(olsrr)
library(olsrr)
ols_vif_tol(model_wine_quality)

# Fit reduced multiple linear regression model with only flavor and oakiness predictors #
model_wine_flavor_oakiness <- lm(Quality ~ Flavor + Oakiness , data = table_wine)
# The anova function gives Sequential (Type I) Sum of Squares of Regression #
anova(model_wine_flavor_oakiness)
# SS for Model (or Regression) = Total of Sum Sq values for Predictors #
# df for Model (or Regression) = Total of Df values for Predictors #

```


## class Nov 13th

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b6 <- read_table2("TableB6.txt")
# Observe the data frame
head(table_b6)

# build the model
model_NbOCl3 <- lm(y ~. , data=table_b6)
model_NbOCl3_1_2_4 <- lm(y ~ x1+x2+x4 , data=table_b6)
model_NbOCl3_1_3_4 <- lm(y ~ x1+x3+x4 , data=table_b6)

model_NbOCl3%>% summary()
model_NbOCl3_1_2_4%>% summary()
model_NbOCl3_1_3_4%>% summary()
# visualizng Satisfaction and Age
library(ggfortify)
model_NbOCl3 %>% autoplot()

anova(model_NbOCl3)
anova(model_NbOCl3_1_2_4)
anova(model_NbOCl3_1_3_4)
anova(model_NbOCl3_1_2_4, model_NbOCl3_1_3_4, model_NbOCl3)

car::vif(model_NbOCl3)
car::vif(model_NbOCl3_1_2_4)
car::vif(model_NbOCl3_1_3_4)

library(olsrr)
ols_vif_tol(model_NbOCl3)
ols_vif_tol(model_NbOCl3_1_2_4)
ols_vif_tol(model_NbOCl3_1_3_4)

texreg::screenreg(l=list(model_NbOCl3, model_NbOCl3_1_2_4, model_NbOCl3_1_3_4))
huxtable::huxreg(model_NbOCl3, model_NbOCl3_1_2_4, model_NbOCl3_1_3_4, statistics = NULL)

```


## lab 4

```{r, eval=FALSE, include = FALSE, collapse=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b4 <- read_table2("TableB4.txt")
# Observe the data frame
head(table_b4)
summary(table_b4)
# visualizng Satisfaction and Age
plot(table_b4,pch=16)

# matrix of correlation coefficients
cor(table_b4)
table_b4 %>% corrr::correlate() %>% corrr::fashion()
table_b4 %>% corrr::correlate() %>% corrr::rplot()

# build the model
model_b4_full <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data=table_b4)
model_b4_full%>% summary()
anova(model_b4_full)
vcov(model_b4_full)
car::vif(model_b4_full)
ols_vif_tol(model_b4_full)
plot(model_b4_full,pch=16,col="blue")

car::avPlots(model_b4_full)

yhat_b4_full <- model_b4_full$fit # Save predicted y to an object #
t_b4_full <- rstudent(model_b4_full) # Save studentized residuals to an object #
# Another way to check non-normality of studentized residuals #
hist(t_b4_full)
qqnorm(t_b4_full, pch=15)
shapiro.test(t_b4_full) #If p-value is bigger, then no problem of non-normality #
plot(yhat_b4_full,t_b4_full, pch=16) # Studentized residuals versus predicted y #
plot(table_b4$x1,t_b4_full, pch=16) # Studentized residuals versus x1 predictor #
plot(table_b4$x2,t_b4_full, pch=16) # Studentized residuals versus x2 predictor #
plot(table_b4$x3,t_b4_full, pch=16) # Studentized residuals versus x3 predictor #
plot(table_b4$x4,t_b4_full, pch=16) # Studentized residuals versus x4 predictor #
plot(table_b4$x5,t_b4_full, pch=16) # Studentized residuals versus x5 predictor #
plot(table_b4$x6,t_b4_full, pch=16) # Studentized residuals versus x6 predictor #
plot(table_b4$x7,t_b4_full, pch=16) # Studentized residuals versus x7 predictor #
plot(table_b4$x8,t_b4_full, pch=16) # Studentized residuals versus x8 predictor #
plot(table_b4$x9,t_b4_full, pch=16) # Studentized residuals versus x9 predictor #

ols_plot_resid_stud(model_b4_full)
ols_press(model_b4_full)

```

