---
title: 'STAT564 Lab 1'
author: "Shen Qu"
date: "10/08/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---



```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```


# {.tabset .tabset-fade .tabset-pills}

## lab 1

A hospital is implementing a program to improve service quality and productivity. As part of this program the hospital management is attempting to measure and evaluate patient satisfaction. Table B.17 contains some of the data that have been collected on a random sample of 25 recently discharged patients.
Variables:
Satisfaction  - a subjective measure on an increasing scale
Age – age of patient in years
Severity – an index measuring the severity of the patient’s illness
Surgical-Medical - an indicator of whether the patient is a surgical or medical patient (0 = surgical, 1 = medical)
Anxiety - an index measuring the patient’s anxiety level
The patient satisfaction is thought to be related to the patient age.



 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b17 <- read_xlsx("TableB.17.xlsx")

# Observe the data frame
head(table_b17)

```

### (a) Create a scatterplot of satisfaction and age variables. Copy and paste it here. 


```{r}
# visualizng Satisfaction and Age
table_b17 %>% ggplot(aes(Age,Satisfaction))+geom_point()+geom_smooth(method="lm", level=0.95)
```

(b) Describe the relationship between patient satisfaction and patient age based on the scatterplot.

 > The plot shows a decreasing approximately linear pattern.

(c) Fit a simple linear regression model to the data. Write the fitted regression model with estimated coefficients. Edit the following math equation to write your fitted model (replace y and x with appropriate variable names).

```{r}
# build the model
model_satis_age <- lm(Satisfaction ~ Age, data=table_b17)
model_satis_age
```

 > The model is $Satisfaction=130.221-1.249Age$

(d) The coefficient of determination, denoted by R^2, (Multiple R-squared in R) provides the percentage of total variation in response variable explained by the fitted model. It can be any value from 0 to 100% and a higher value is better.
What is the percentage of total variation in the patient satisfaction explained by the fitted model for these data?

```{r}
model_satis_age %>% summary()
```

 > According to the analysis-of-variance table, Multiple R-squared is 0.7205. The fitted model for these data can explain 72.05% of total variation in the patient satisfaction. 
 
(e) Report the standard error of estimated intercept.

 > the standard error of estimated intercept is 7.7775.

(f) Using the p-value reported along with regression coefficients in your software output, explain whether the true slope of the fitted model is significant at 5% significance level. Report the p-value as well.

 > The table shows that p-value is less than 1.52e-08. The regression model is significant at 95% significance level.

(g) Report a 95% confidence interval for the true intercept of the model. Based on confidence interval, explain whether the true intercept is different from zero.

 > The equation for the 95% CI for the estimated β1 is defined as:

$$\beta_1 \pm t_{\alpha / 2, n - 2} \left(\frac{\sqrt{MSE}}{\sqrt{\sum (x_i - \bar{x})^2}}\right)$$

```{r}
model_satis_age %>% confint(level=0.95)
```

 > The fitted β0 is 130.2209. The 95% confidence interval for the intercept of the regression line is (114.131844, 146.3100181).  
The confidence interval for β0 does not contain 0, it can be concluded the true intercept is different from zero.

(h) Construct the analysis of variance (ANOVA) table using software and test for significance of regression model. Report the p-value.

```{r}
anova(model_satis_age)
```

  > The table shows that p-value is less than 1.52e-08. The regression model is significant at 95% significance level.

(i) Compute the point estimate of the mean patient satisfaction when the patient is 65 years old.

> The confidence interval around the mean response, denoted $\mu_y$, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.
 
```{r}
model_satis_age %>% predict(newdata = data.frame(Age=65), interval = "confidence", level=0.95)
```

 > From the output, the fitted satisfaction is 49.03367 when the patient's age is 65 years old. 

(j) Compute 95% confidence interval for the mean (average) patient satisfaction when the patient is 65 years old. (Confidence interval is for mean or average response) 
 
 > According the result from (i), the confidence interval of (42.86397, 55.20336) signifies the range in which the true population parameter lies at a 95% level of confidence.
 
(k) Compute the 95% prediction interval for the patient satisfaction when the patient is 65 years old. (Prediction interval is for a new value of response and not for mean response)

```{r}
model_satis_age %>% predict(newdata = data.frame(Age=65), interval = "prediction", level=0.95)
```

 > the 95% prediction interval is (26.11012, 71.95721) for the patient satisfaction when the patient is 65 years old

(l)  Compare the two intervals in the previous two questions and explain why they are same or different.

 > The results show the prediction interval is wider than confidence interval due to the additional term in the standard error of prediction. Prediction and confidence intervals are similar in that they are both predicting a response, however, they differ in what is being represented and interpreted.The best predictor of a random variable (assuming the variable is normally distributed) is the mean $\mu$. The best predictor for an observation from a sample of x data points, x1,x2,⋯,xn and error $\epsilon$ is $\bar x$.
The prediction interval depends on both the error from the fitted model and the error associated with future observations.

(m) Use the estimated slope and its standard error from software output to test whether the true slope is different from -1 (negative 1). Provide all the steps of test.

```{r}
t0= (-1.2490+1)/0.1471
2*pt(q = t0, df = nrow(table_b17)-2 , lower.tail = TRUE) # Gives the P(T < t0) from the t distribution. Enter the value for df of the t distribution. Use FALSE instead of TRUE to compute P(T >t0). #
```

 > The result shows that the p_value is 0.1040118, which is bigger than 0.05. Therefore, we can reject the H1 and the ture slope might be same with -1.
 

## lab 2

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_plant <- read_table2("Plant.txt")
# Observe the data frame
head(table_plant)
# visualizng Satisfaction and Age
table_plant %>% ggplot(aes(SolarRadiation,PlantBiomass))+geom_point()+geom_smooth(method="lm", level=0.95)
# build the model
model_solar_mass <- lm(PlantBiomass ~ SolarRadiation, data=table_plant)
model_solar_mass%>% summary()
confint(model_solar_mass, level = 0.99)
anova(model_solar_mass)
model_solar_mass_origin <- lm(PlantBiomass ~ SolarRadiation+0, data=table_plant)
model_solar_mass_origin%>% summary()
anova(model_solar_mass_origin)
```

## lab 3

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_wine <- read_csv("TableB11.csv")
# Observe the data frame
head(table_wine)
summary(table_wine)
# visualizng Satisfaction and Age
plot(table_wine,pch=16)

# matrix of correlation coefficients
round(cor(table_wine),digits = 3)

# build the model
model_wine_quality <- lm(Quality ~ Clarity + Aroma + Body + Flavor + Oakiness + Region, data=table_wine)
model_wine_quality%>% summary()
anova(model_wine_quality)

car::Anova(model_wine_quality, type="II")


# two kinds of confident intervals
confint(model_wine_quality, level = 0.95)

confint(model_wine_quality, level = 1-(0.05/7))

# Scheffe joint confidence interval
B.hat_wine <- coefficients(model_wine_quality)
B.StdErr_wine <- coef(summary(model_wine_quality))[,2]
df1_wine <- length(B.hat_wine)
df2_wine <- nrow(table_wine)-df1_wine
F_wine <- qf(0.05,df1_wine,df2_wine)

cbind((B.hat_wine-sqrt(2*F_wine)*B.StdErr_wine),(B.hat_wine+sqrt(2*F_wine)*B.StdErr_wine))

# Prediction
predict(model_wine_quality,data.frame(Clarity=0.9, Aroma=4.7, Body=6.1, Flavor=5.2, Oakiness=3, Region=2),interval = "confidence", level=0.99)

predict(model_wine_quality,data.frame(Clarity=0.9, Aroma=4.7, Body=6.1, Flavor=5.2, Oakiness=3, Region=2),interval = "prediction", level=0.99)

# Obtain the Design Matrix (X) for the regression model #
# Include only the predictors in the model and no response #
x_wine <- model.matrix(~ Clarity + Aroma + Body + Flavor + Oakiness + Region, data =
table_wine)
head(x_wine) # Display the design matrix #
xpx.inv_wine <- solve(t(x_wine)%*%x_wine) # Computes Inverse of (X'X) matrix #
xpx.inv_wine
round(xpx.inv_wine, digits=4) # round with 4 decimal places #

vcov(model_wine_quality)

table_wine %>% corrr::correlate() %>% corrr::fashion()

# Check confidence and prediction interval of response #
x0_wine <- c(1,0.9,4.7, 6.1, 5.2, 3, 2)
predict_x0_wine <- t(x0_wine)%*%B.hat_wine
predict.stderr_CI_wine <- sqrt(1.395*t(x0_wine)%*%xpx.inv_wine%*%x0_wine)
predict.CI_wine <- c((predict_x0_wine - qt(0.995, 38-7)*predict.stderr_CI_wine), (predict_x0_wine + qt(0.995, 38-
7)*predict.stderr_CI_wine))
predict.CI_wine
predict.stderr_PI_wine <- sqrt(1.395*(1+(t(x0_wine)%*%xpx.inv_wine%*%x0_wine)))
predict.PI <- c((predict_x0_wine - qt(0.995, 38-7)*predict.stderr_PI_wine), (predict_x0_wine + qt(0.995, 38-
7)*predict.stderr_PI_wine))
predict.PI

# Test significance of linear function of regression coefficients #
# The linearHypothesis function defined in the car package can be used for this #

T_b4_wine <- c(0,0,0,0,1,0,0) # Define T matrix for testing B4=0 versus B4 not= 0 #
T_b4_wine <- c(rep(0,4),1,0,0) # Define T matrix for testing B4=0 versus B4 not= 0 #
T_b4_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix =T_b4_wine , rhs=0)
# The rhs option specifies value on the right hand side of hypotheses #
T_2b2_b3_wine <- c(0, 0, 2, -1, 0, 0, 0) # Define T matrix for testing 2B2=B3 versus 2B2 not= B3 #
T_2b2_b3_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix = T_2b2_b3_wine, rhs=0) #Test T2=0 versus T2 not= 0 #
# Define T matrix for testing the above hypotheses simultaneously #
# Use rbind function to bind two rows, The rep function is used to create replications of the same value #
T_b4.2b2_b3_wine <- rbind(c(rep(0,4), 1, 0, 0), c(0, 0, 2, -1, 0, 0, 0))
T_b4.2b2_b3_wine
car::linearHypothesis(model = model_wine_quality, hypothesis.matrix = T_b4.2b2_b3_wine, rhs=c(0,0)) # Test the above two hypotheses simultaneously #
# Check for multicollinearity #
# Use the vif function defined in the car package #
car::vif(mod = model_wine_quality)
# Fit reduced multiple linear regression model with only flavor and oakiness predictors #
model_wine_flavor_oakiness <- lm(Quality ~ Flavor + Oakiness , data = table_wine)
# The anova function gives Sequential (Type I) Sum of Squares of Regression #
anova(model_wine_flavor_oakiness)
# SS for Model (or Regression) = Total of Sum Sq values for Predictors #
# df for Model (or Regression) = Total of Df values for Predictors #

```
