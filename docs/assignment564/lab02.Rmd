---
title: 'STAT564 Homework 1'
author: "Shen Qu"
date: "10/15/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---

```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

## Problem 1: 

Exercise 2.3 (Page 58): Table B.2 presents data collected during a solar energy project at Georgia Tech.
y: Total heat flux (kwatts)  
x1 : Insolation (watts/m2)   
x2 : Position of focal point in east direction (inches)   
x3 : Position of focal point in south direction (inches)   
x4 : Position of focal point in north direction (inches)   
x5 : Time of day

 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_b2 <- read_xlsx("Table B.2.xlsx")

# Observe the data frame
head(table_b2)

# visualizng y and x4
table_b2 %>% ggplot(aes(x4,y))+geom_point()+geom_smooth(method="lm", level=0.99)
```

 > The plot shows a decreasing approximately linear pattern.

a. Fit a simple linear regression model relating total heat flux y (kilowatts) to the radial deflection of the deflected rays $x_4$ (milliradians).

```{r}
# build the model
model_y_x4 <- lm(y ~ x4, data=table_b2)
model_y_x4
```

 > The model is $y=607.1-21.4x_4$

b. Construct the analysis-of-variance table and test for significance of regression.

```{r}
anova(model_y_x4)
```

 > The table shows that p-value is less than 5.935e-09. The regression model is significant.

c. Find a 99% CI on the slope.

 > The equation for the 99% CI for the estimated β1 is defined as:

$$\beta_1 \pm t_{\alpha / 2, n - 2} \left(\frac{\sqrt{MSE}}{\sqrt{\sum (x_i - \bar{x})^2}}\right)$$

```{r}
model_y_x4 %>% confint(level=0.99)
```

 > The fitted β1 is -21.402. The 99% confidence interval for slope of the regression line is (-28.50995, -14.29497).
If the confidence interval for β1 contains 0, it can be concluded there is no significant evidence of a linear relationship between predictor x4 and response y in the population.


d. Calculate $R^2$.

```{r}
model_y_x4 %>% summary()
```

 > According to the table, Multiple R-squared is 0.7205, Adjusted R-squared is 0.7102.

e. Find a 95% CI on the mean heat flux when the radial deflection is 16.5 milliradians.

> The confidence interval around the mean response, denoted $\mu_y$, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.
 
```{r}
model_y_x4 %>% predict(newdata = data.frame(x4=16.5), interval = "confidence", level=0.95)
```

 > From the output, the fitted heat flux is 253.9627 kwatts when the radial deflection is 16.5 milliradians. The confidence interval of (249.1468, 258.7787) signifies the range in which the true population parameter lies at a 95% level of confidence.

## Problem 2: 

Exercise 2.4 (Page 58): Table B.3 presents data on the gasoline mileage performance of 32 different automobiles.  
y : Miles/gallon   
x1 : Displacement (cubic in.)   
x2 : Horsepower (ft-lb)   
x3 : Torqne (ft-lb)   
x4 : Compression ratio   
x5 : Rear axle ratio Source : Motor Trend , 1975  
x6 : Carburetor (barrels)  
x7 : No. of transmission speeds   
x8 : Overall length (in.)   
x9 : Width (in.)   
x10 : Weight (lb)   
x11 : Type of transmission (A automatic; M manual)  

```{r, echo=TRUE}

# Import Data
table_b3 <- read_xlsx("Table B.3.xlsx")

# Observe the data frame
head(table_b3)

# visualizng y and x1
table_b3 %>% ggplot(aes(x1,y))+geom_point()+geom_smooth(method="lm")
```

 > The plot shows a decreasing approximately linear pattern.

a. Fit a simple linear regression model relating gasoline mileage y (miles per gallon) to engine displacement x1 (cubic inches).


```{r}
# build the model
model_y_x1 <- lm(y ~ x1, data=table_b3)
model_y_x1
```

b. Construct the analysis-of-variance table and test for significance of regression.


```{r}
anova(model_y_x1)
```

 > The table shows that p-value is 3.82e-11. The regression model is significant.

c. What percent of the total variability in gasoline mileage is accounted for by the linear relationship with engine displacement?

 > According to the Multiple R-squared value, there are 77.2% variability in gasoline mileage is accounted for by the linear relationship with engine displacement.

d. Find a 95% CI on the mean gasoline mileage if the engine displacement is 275 in.^3

 > The confidence interval around the mean response, denoted $\mu_y$, when the predictor value is $x_k$ is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.

```{r}

model_y_x1 %>% predict(newdata = data.frame(x1=275), interval = "confidence", level=0.95)

```

 > From the output, the fitted gasoline mileage when the engine displacement is 275 in.^3 is 20.68466 gasoline mileage. The confidence interval of (19.57343, 21.79589) signifies the range in which the true population parameter lies at a 95% level of confidence.

e. Suppose that we wish to predict the gasoline mileage obtained from a car with a 275-in.3 engine. Give a point estimate of mileage. Find a 95% prediction interval on the mileage.

 > The prediction interval is rather similar to the confidence interval in calculation, but as mentioned earlier, there are significant differences. The prediction interval equation is defined as:

$$\hat{y}_h \pm t_{\alpha / 2, n - 2} \sqrt{MSE \left(1 + \frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum (x_i - \bar{x})^2} \right)}$$

 > Where $\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\alpha/2,n-2}$ is the t-value with n−2 degrees of freedom, while $\sqrt{MSE \left(1+\frac{1}{n} + \frac{(x_k - \bar{x})^2}{\sum(x_i - \bar{x}^2)} \right)}$ is the standard error of the fit.

```{r}

model_y_x1 %>% predict(newdata = data.frame(x1=275), interval = "prediction", level=0.95)

```

 > When the engine displacement is 275 in.^3, the 95% prediction interval on the mean gasoline mileage is (14.32311, 27.04622).

f. Compare the two intervals obtained in parts d and e. Explain the difference between them. Which one is wider, and why?

 > The fitted value is the same, but the prediction interval is wider than confidence interval due to the additional term in the standard error of prediction. Prediction and confidence intervals are similar in that they are both predicting a response, however, they differ in what is being represented and interpreted.The best predictor of a random variable (assuming the variable is normally distributed) is the mean $\mu$. The best predictor for an observation from a sample of x data points, x1,x2,⋯,xn and error $\epsilon$ is $\bar x$.
The prediction interval depends on both the error from the fitted model and the error associated with future observations.

## Problem 3: 

Exercise 2.12 (Page 60): The number of pounds of steam used per month at a plant is thought to be related to the average monthly ambient temperature. The past year’s usages and temperatures follow.

```{r, echo=TRUE}

# Import Data
table_12 <- read_xlsx("Problem 2.12.xlsx")

# Observe the data frame
glimpse(table_12)

# visualizng `Usage/l000` and Temperature
table_12 %>% ggplot(aes(Temperature,`Usage/l000`,col=as.factor(Month)))+geom_point( )+geom_smooth(method="lm")
```

 > The plot shows a increasing approximately linear pattern.

a. Fit a simple linear regression model to the data.

```{r}
# build the model
model_use_tem <- lm(`Usage/l000` ~ Temperature, data=table_12)

model_use_tem %>% summary()
```

 > The model is `Usage/l000`=-6.33209+9.20847*Temperature

b. Test for significance of regression.

> According to the analysis-of-variance table (a.), the table shows that p-value is less than 2.2e-16. The regression model is significant.

c. Plant management believes that an increase in average ambient temperature of 1 degree will increase average monthly steam consumption by 10,000lb. Do the data support this statement?

 > We made the null hypothesis H0: $\hat\beta_1=10$ the altertative hypothesis H1: $\hat\beta_1\ne10$. Accroding the method of t test (2.3.1)
 
$$t_0=\frac{\hat\beta_1-\beta_{10}}{\sqrt\frac{M SE}{S_{xx}}}=\frac{\hat\beta_1-\beta_{10}}{se(\hat\beta_1)}$$
 
```{r} 
2*pt(q = (9.20847-10)/0.03382, df = nrow(table_12)-2 , lower.tail = TRUE)
```
 > The result shows that the p_value is 4.593537e-10, which is far less than 0.05. Therefore, we can reject the H0 and the true slope is not equal 10. The data don't support the statement.

d. Construct a 99% prediction interval on steam usage in a month with average ambient temperature of 58°.


```{r}

model_use_tem %>% predict(newdata = data.frame(Temperature=58), interval = "prediction", level=0.99)

```

 > When the Temperature is 58°, the 99% prediction interval on the steam usage is (521.2237, 534.2944).

## Problem 4: 

Exercise 2.25 (Page 65): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$, with $E(\epsilon)=0,\ Var(\epsilon)=\sigma^2$, and $\epsilon$ uncorrelated.  

 > According the definition of covariance,  
 
$$Cov(X,Y)=\sigma_{XY}=E[(X-\bar X)(Y-\bar Y)]=\frac1n\sum_{i=1}^n(x_i-\bar X)(y_i-\bar Y)$$

 > According the properties of covariance,  

$$Cov(X,a)=0,\quad Cov(X,X)=Var(X)\equiv\sigma^2(X)\equiv\sigma_X^2$$

$$Cov(X,Y)=Cov(Y,X),\quad Cov(aX,bY)=ab\ Cov(X,Y)$$

$$Cov(X+a,Y+b)=Cov(X,Y)$$

$$And\quad Cov(a_1X_1+a_2X_2,\ b_1Y_1+b_2Y_2)=a_1b_1Cov(X_1,Y_1)+a_1b_2Cov(X_1,Y_2)+a_2b_1Cov(X_2,Y_1)+a_2b_2Cov(X_2,Y_2)$$

a. Show that $Cov(\hat\beta_0, \hatβ_1)=−\overline x\sigma^2 S_{xx}$.

 > Accroding the properties of the Least-Squares Estimators and the Fitted Regression Model (2.2.2)

$$\hat\beta_0=\bar y-\hat\beta_1\bar x,\quad Var(\hat\beta_1)=\frac{\sigma^2}{S_{xx}}$$



$$Cov(\hat\beta_0, \hatβ_1)=Cov(\bar y-\hat\beta_1\bar x,\ \hatβ_1)=-\bar x\ Cov(\hatβ_1,\hatβ_1)=-\bar x\ Var(\hat\beta_1)=−\bar x\frac{\sigma^2}{S_{xx}}$$


b. Show that $Cov(\bar y,\hat\beta_1) = 0$.

For a simple linear regression model $y=\beta_0+\beta_1x + \epsilon$, with $E(\epsilon)=0,\ Var(\epsilon)=\sigma^2$, and $\epsilon$ uncorrelated, $\bar y$ is a constant.

$$\therefore\quad Cov(\bar y,\hat\beta_1) = 0$$

## Problem 5: 
Exercise 2.26 (Page 65): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$, with $E(\epsilon)=0,\ Var(\epsilon)=\sigma^2$, and $\epsilon$ uncorrelated.

a. Show that $E(MS_R)=\sigma^2+\beta_1^2S_{xx}$

$$E(MSR)=\frac{E(SSR)}1=E(\hat\beta_1^2S_{xx})=S_{xx}\{Var(\hat\beta_1)+[E(\hat\beta_1)]^2\}=S_{xx}(\frac{\sigma^2}{S_{xx}}+\beta_1^2)=\sigma^2+\beta_1^2S_{xx}$$


b. Show that $E(MS_{Res})=\sigma^2$.

 > According the appendix C.3.2,  MSE=SSE/(n-2) is an unbiased estimator for $\sigma^2$

$$E(MSE)=E(\frac{SSE}{n-2})=\frac{\sigma^2}{n-2}E(\chi_{n-2}^2)=\sigma^2$$

## Problem 6: 

Exercise 2.27 (Page 65): Suppose that we have fit the straight-line regression model $\hat y=\hat\beta_0+\hat\beta_1x$ but the response is affected by a second variable $x_2$ such that the true regression function is

$$E(y)=\beta_0+\beta_1x_1+\beta_2x_2$$

a. Is the least-squares estimator of the slope in the original simple linear regression model unbiased?

$$E(SSR)=$$

$$E(\hat\beta_1)=E(\frac{\sum(x_i-\bar x)y_i}{S_{xx}})=\frac{\sum(x_{i1}-\bar x)}{Sxx}E(y_i)=\frac{\sum(x_{i1}-\bar x)}{Sxx}(\beta_0+\beta_1x_{i1}+\beta_2x_{i2})=\beta_1+\frac{\sum(x_{i1}-\bar x)x_{i2}}{S_{xx}}$$

b. Show the bias in $\hat\beta_1$

$$\beta_1-E(\hat\beta_1)=\frac{-\sum(x_{i1}-\bar x)x_{i2}}{Sxx}$$

## Problem 7: 
Exercise 2.32 (Page 66): Consider the simple linear regression model $y=\beta_0+\beta_1x + \epsilon$ where the intercept $\beta_0$ is known.

a. Find the least-squares estimator of $\beta_1$ for this model. Does this answer seem reasonable?

$$S(\beta_0,\beta_1)=\sum(y_i-\beta_0-\beta_1x_i)^2$$

$$-2\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)^2=0$$

$$\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^n(y_i-\beta_0)x_i$$

$$\hat\beta_1=\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}$$

b. What is the variance of the slope ($\hat\beta_1$) for the least-squares estimator found in part a?

$$Var(\hat\beta_1)=\frac1{\sum_{i=1}^nx_i^2}Var(\sum_{i=1}^ny_ix_i)=\frac1{(\sum_{i=1}^nx_i^2)^2}(\sum_{i=1}^nx_i^2)\sigma^2=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$$

c. Find a 100(1−α) percent CI for $\beta_1$. Is this interval narrower than the estimator for the case where both slope and intercept are unknown?

$$\frac{\hat\beta_1-\beta_1}{\sqrt\frac{MSE}{\sum x_i^2}}\sim t_{n-2}$$

$$\hat\beta_1\pm t_{\frac\alpha2,n-2}\sqrt\frac{MSE}{\sum x_i^2} $$

is narrower than when both are unknown