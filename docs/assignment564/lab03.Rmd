---
title: 'STAT564 Homework 2'
author: "Shen Qu"
date: "10/31/2018"
output: 
  html_document:
    toc: false
    toc_float: false
---

```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

### Problem 1
The weight and systolic blood pressure of 26 randomly selected a group of adults are given in Weight vs BP text file

 <!--https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals-->

```{r, echo=TRUE}
# Load the package 
library(readxl)
library(tidyverse)
library(broom)
# Import Data
table_weight_bp <- read_table2("Weight vs BP.txt")
# Observe the data frame
head(table_weight_bp)
# visualizng Satisfaction and Age
table_weight_bp %>% ggplot(aes(Weight,SystolicBP))+geom_point()+geom_smooth(method="lm", level=0.95)
# build the model
model_wei_bp <- lm(SystolicBP ~ Weight, data=table_weight_bp)
model_wei_bp%>% summary()
confint(model_wei_bp, level = 0.99)
anova(model_wei_bp)
model_wei_bp_origin <- lm(SystolicBP ~ Weight+0, data=table_weight_bp)
confint(model_wei_bp_origin, level = 0.99)
model_wei_bp_origin%>% summary()
anova(model_wei_bp_origin)
```
(a) Fit a simple linear regression model to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

* The simple linear model is $\hat{systolic blood pressure}=69.1044+0.4194Weight$

(b) Test for significance of true intercept of this model at 5% significance and provide the conclusion along with p-value.

* The p-value is $1.71\times e^{-05}$, which is much smaller than 0.05. We can know the true intercept is 69.10437 on 95% confidence level.

(c) Report a 99% confidence interval for true intercept of this model.

* The 99% confidence interval for true intercept of this model is (32.9955223 105.2132233).

(d) Fit a simple linear regression model through origin to predict blood pressure using weight. Provide only the fitted equation with correct variable names here.

* The model through origin is $\hat{systolic blood pressure}=0.7916Weight$

(e) Compare quality of two models and recommend only one model. Provide valid reasons for your recommendation.

* Firstly, The true intercept is different with zero on 99% confident level by the confidence interval. There are strong relationship between weight and blood pressure.
* Secondly, The estimated intercept (69.104) is confident on 95% confident level by p-value.
* Thirdly, The R-suqare value of model with intercept is 0.5983, means only 59.83% observations can be explained by this model. Meanwhile, the R-suqare value of model with zero intercept is 0.9929, which can explain more observation
* Fourthly, the sum square of total variation with intercept is $2693.6+1808.6=4502.2$, while the value with zero intercept is $551834+3968=555802$. 

* Finally, Either the blood presure or weight has a reasonable range. The blood presure for zero weight is meaningless. We should not extrapolation the trend. the assumption of through origin with zero blood presure for zero weight is unreasonable.
I would recommend the model with intercept or find a other models with higher R-square value.  


### Problem 2: 
Show your work to get full points.
Consider the simple linear regression model:
$$y_i=β_0+β_1 x_i+ε_i\quad    for  i=1,2,…,n$$

which can be written for each observation as
$$
\begin{matrix}
  y_1=\beta_0+\beta_1x_1+\varepsilon_1 \\
  y_2=\beta_0+\beta_1x_2+\varepsilon_2 \\
  \vdots\quad\quad  \vdots\quad\quad   \vdots\quad\quad  \vdots  \\
  y_n=\beta_0+\beta_1x_n+\varepsilon_n 
 \end{matrix}
$$
 (a) Write this model in matrix form. Provide all the vectors and design matrix along with dimensions. Use the above format so that you can write only 4 rows for each vector and matrix. For example, Y vector is written as
$$
\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1} 
\mathbf{X}=\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2} 
\mathbf{β}=\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}_{2\times1}+
\mathbf{ε}=\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots  \\ \varepsilon_n \end{bmatrix}_{n\times1}
$$
 (b) Compute $\mathbf{X'X}$ (show all the main steps)

$$
\mathbf{X'X}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots  \\ 1 & x_{n} \end{bmatrix}_{n\times2}=
\begin{bmatrix} n & x_{1}+x_{2}+\cdots+x_{n} \\ x_{1}+x_{2}+\cdots+x_{n} & x_1^2+x_2^2+\cdots+x_n^2  \end{bmatrix}_{2\times2}
$$

 (c) Compute $\mathbf{(X'X)^{-1}}$ (show all the main steps)
Hint: The inverse of a nonsingular is obtained by 1) replacing every element of the matrix with its cofactor, 2) transposing the resulting matrix, 3) dividing by the determinant of the origin matrix.

$$
\mathbf{|X'X|}^{-1}=\frac1{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix} x_1^2+x_2^2+\cdots+x_n^2  & -(x_{1}+x_{2}+\cdots+x_{n}) \\ -(x_{1}+x_{2}+\cdots+x_{n}) & n \end{bmatrix}_{2\times2}
$$

$$=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n\sum_{i=1}^nx_i^2-(n\bar x_i)^2}=\frac{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}{n(\sum_{i=1}^nx_i^2-n\bar x_i^2)}=\frac1{nS_{xx}}{\begin{bmatrix} \sum_{i=1}^nx_i^2 & -(\sum_{i=1}^nx_i) \\ -(\sum_{i=1}^nx_i) & n \end{bmatrix}_{2\times2}}$$

(d) Compute X'Y (show all the main steps)

$$
\mathbf{X'Y}=\begin{bmatrix} 1 &  1 &\cdots &1 \\   x_{1} & x_{2} & \cdots & x_{n} \end{bmatrix}_{2\times{n}} 
\begin{bmatrix} y_1 \\ y_2 \\ \vdots  \\ y_n \end{bmatrix}_{n\times1}=\begin{bmatrix} y_1 + y_2 +\cdots + y_n \\ x_{1}y_1 + x_{2}y_2 + \cdots + x_{n}y_n \end{bmatrix}_{2\times{1}}=\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}}
$$

(e) Compute (X'X)^(-1) X'Y (show all the main steps).
The final answer is the expression for the least squares estimator of β
$$
\mathbf{\hatβ}=\mathbf{(X'X)^{-1} X'Y}=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i \\ -\sum_{i=1}^nx_i & n \end{bmatrix}_{2\times2}\begin{bmatrix} \sum_{i=1}^ny_i \\ \sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times{1}} $$

$$=\frac1{nS_{xx}}\begin{bmatrix} \sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i \\ -\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i \end{bmatrix}_{2\times1}=\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i\sum_{i=1}^ny_i + n\sum_{i=1}^nx_{i}y_i}{nS_{xx}} \end{bmatrix}_{2\times1}$$

in the multiple linear regression model when k=1. It reduces to the same expression you learned for the slope and intercept for simple linear regression model.

(f) Also, we know that if $Var(ε_i )=σ^2\ and\ Cov(ε_i,ε_j )=0 for i≠j$, 

$$Var(\mathbf{\hatβ})=σ^2(\mathbf{X'X})^{-1}=\sigma^2\begin{bmatrix} \frac{\sum_{i=1}^nx_i^2}{nS_{xx}} & \frac{-\sum_{i=1}^nx_i}{nS_{xx}} \\ \frac{-\sum_{i=1}^nx_i)}{nS_{xx}} & \frac1{S_{xx}} \end{bmatrix}_{2\times2}=\sigma^2\begin{bmatrix} C_{11} &  C_{12} \\  C_{21} &  C_{22} \end{bmatrix}_{2\times2}$$

From this result, show that 
(i)  $Var(\hatβ_0)= σ^2 [1/n+ x^2/S_{xx}]$  

$$Var(\mathbf{\hatβ_0})=σ^2C_{11}=\frac{\sigma^2\sum_{i=1}^nx_i^2}{nS_{xx}}=\sigma^2\frac{\sum_{i=1}^nx_i^2-n\bar x_i^2+n\bar x_i^2}{nS_{xx}}=\sigma^2(\frac{1}{n}+\frac{n\bar x_i^2}{nS_{xx}})=\sigma^2(\frac{1}{n}+\frac{\bar x_i^2}{S_{xx}})$$

(ii)  $Var(\hatβ_1)=σ^2/S_{xx}$

$$Var(\mathbf{\hatβ_1})=σ^2C_{22}=\frac{\sigma^2}{S_{xx}}=\sigma^2\frac{n}{n\sum_{i=1}^nx_i^2-n^2\bar x_i^2}=\frac{\sigma^2}{\sum_{i=1}^nx_i^2-n\bar x_i^2}=\frac{\sigma^2}{S_{xx}}$$

(iii)  $Cov(β_0,β_1)=-(\bar x σ^2)/S_{xx}$

$$Cov(\mathbf{\hatβ_0,\hatβ_1})=σ^2C_{12}=\frac{\sigma^2(-\sum_{i=1}^nx_i)}{nS_{xx}}=\frac{-\sigma^2(n\bar x_i)}{nS_{xx}}=\frac{-\sigma^2\bar x_i}{S_{xx}}$$