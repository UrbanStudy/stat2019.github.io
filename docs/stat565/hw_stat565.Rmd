---
title: 'STAT565 Homework'
author: ""
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW0

### Problem 1 (2.1)

Compute the values of the missing quantities.

|Variable| N | Mean| SE Mean|Std. Dev.|Variance|Minimum| Maximum
|  ----  |---| ----|  ----  |   ----  |  ----  |  ---- | ----
|Y       | 9 |19.96|   ?    |3.12     |       ?|  15.94| 27.16

$$\text{SE mean}=\frac{Std.Dev.}{\sqrt{n}}=\frac{3.12}{\sqrt9}=1.04$$

$$Var(Y)=S^2=3.12^2=9.7344$$

### Problem 2 (2.17/19)

The viscosity of a liquid detergent is supposed to average 800 centistokes at 25‚àòC. A random sample of 16 batches of detergent is collected, and the average viscosity is 812. Suppose we know that the standard deviation of viscosity is œÉ = 25 centistokes.

 (a) State the hypotheses that should be tested.
 Let $\mu=800, \bar y=812, n=16, œÉ=25$
 
 $$H_0: \mu=800\qquad H_A: \mu\neq800$$

 ---
 
 (b) Test these hypotheses using ùõº = 0.05. What are your conclusions?
 
Assuming the population is normal, the sample size is less than 30 but the variance is known. The hypothesis may be tested using a direct application of the normal distribution.
 
 $$se(\hat\mu)=\frac{œÉ}{\sqrt{n}}=\frac{25}{\sqrt{16}}=6.25$$
 
 $$Z_0=\frac{\bar y-\mu}{se(\hat\mu)}=\frac{812-800}{6.25}=1.92 $$
 
 Since $Z_{\frac{Œ±}2} =Z_{0.025}=1.96$, do not reject.
 
 ---
 
 (c) What is the P-value for the test?
 
 The P-value is below:
 
```{r}

2*(1-pnorm(812, mean = 800, sd = 25/4, lower.tail = TRUE, log.p = FALSE))

2*pnorm(-1.92)

```

 ---
 
 (d) Find a 95 percent confidence interval on the mean.

$$\bar y-Z_{\frac{Œ±}2}se(\mu)\le\mu\le\bar y+Z_{\frac{Œ±}2}se(\mu)\implies 812-1.96\times6.25\le\mu\le812+1.96\times6.25$$

$$\implies 799.75\le\mu\le824.25$$


### Problem 3

The simple linear regression model: $y_i=Œ≤_0+Œ≤_1 x_i+Œµ$, $E(Œµ_i)=0$, $Var(Œµ_i)=\sigma^2$, for $i=1,2,‚Ä¶,n$; $Cov(Œµ_i,Œµ_j)=0$ for $i‚â†j$, $x_i=\begin{cases}0& \text{for Male for }i=1,2,‚Ä¶,\frac{n}2\\1& \text{for Female for}\frac{n}2+1,\frac{n}2+2,..n\end{cases}$

Use the least squares method to find the estimated value of $Œ≤_1$ and simplify your answer as much as possible. Show all the steps of your work.

For $Œµ_i\sim iid N(0, \sigma^2), i=1,2,‚Ä¶,n$, 

$S(\beta_0,\beta_1)=\sum_{i=1}^n\varepsilon_i=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2$

$$\left.\frac{\partial S}{\partial\beta_0}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-1)=0$$
$$\left.\frac{\partial S}{\partial\beta_1}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-x_i)=0$$

- Normal Equations

$$\sum_{i=1}^ny_i=n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i$$
$$\sum_{i=1}^nx_iy_i=\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2$$


For a indicator variable $x_i$, 

$\sum_{i=1}^nx_i=\sum_{i=1}^nx_i^2=\sum_{i=1}^{\frac{n}2}0+\sum_{i=\frac{n}2+1}^n1=\frac{n}2$,

$\sum_{i=1}^nx_iy_i=\sum_{i=1}^{\frac{n}2}0y_i+\sum_{i=\frac{n}2+1}^n1y_i=\sum_{i=\frac{n}2+1}^ny_i$, and 

$E[\bar y]=E[\frac{\sum_{i=1}^ny_i}n]=\mu$, $n\bar y=\sum_{i=1}^ny_i$, then the normal equations are


$$\bar y= \hat\beta_0+\frac{\hat\beta_1}2;\quad \sum_{i=\frac{n}2+1}^ny_i=\frac{n}2(\hat\beta_0+\hat\beta_1)$$

$$\therefore\frac2n\sum_{i=\frac{n}2+1}^ny_i=(\bar y-\frac{\hat\beta_1}2)+\hat{\beta_1}\implies\hat{\beta_1}=\frac4n\sum_{i=\frac{n}2+1}^ny_i-2\bar y$$


### Problem 4 (2.30/32)

Front housings for cell phones are manufactured in an injection molding process. The time the part is allowed to cool in the mold before removal is thought to influence the occurrence of a particularly troublesome cosmetic defect, flow lines, in the finished housing. After manufacturing, the housings are inspected visually and assigned a score between 1 and 10 based on their appearance, with 10 corresponding to a perfect part and 1 corresponding to a completely defective part. An experiment was conducted using two cool-down times, 10 and 20 seconds, and 20 housings were evaluated at each level of cool-down time. All 40 observations in this experiment were run in random order. The data are as follows.

```{r include=FALSE}

library(readxl)
library(tidyverse)

table_cellphone <- read_xlsx("Exercsie2_30.xlsx")

```
 (a) Is there evidence to support the claim that the longer cool-down time results in fewer appearance defects? Use ùõº = 0.05.

We use the paired t-test because the observations from the factor levels are ‚Äúpaired‚Äù on each experimental unit.

In order to test the claim that the longer cool-down time results in fewer appearance defects on average, the hypotheses are $H_0:\mu_{20-10}\le0;\quad H_1:\mu_{20-10}>0$.

```{r echo=T}

t.test(table_cellphone$TwentySeconds, table_cellphone$TenSeconds, paired = T,var.equal=T, alternative="greater", mu = 0, conf.level= 0.95)

```


P-value is less than significance level (5%) and null hypothesis ($H_0$) is rejected. Therefore, we can be 95% sure the housings with 20 seconds of cool-down times have a higher means of scores than 10 seconds at least 2.176087.

 ---

 (b) What is the P-value for the test conducted in part (a)?

p-value = $1.077\times10^{-05}$

 - -

 (c) Find a 95 percent confidence interval on the difference in means. Provide a practical interpretation of this interval.
 
In order to test the 95 percent confidence interval on the difference in means, the hypotheses are $H_0:\mu_{20-10}=0;\quad H_1:\mu_{20-10}\neq0$.

```{r echo=T}

t.test(score~cool_down_time, data=gather(table_cellphone,key=cool_down_time,value=score), paired = T,var.equal=T, alternative="two.sided", mu = 0, conf.level= 0.95)

```

P-value is less than significance level (5%) and the 95%  confidence bound is negative (-4.32887 -1.97113). Therefore, the null hypothesis ($H_0$) is rejected. We can be 95% confidence that the difference in average scores of housing with 10 seconds and 20 seconds of cool-down times is between -4.32887 and -1.97113. In other words, we can conclude that the scores of housings with 10-second and 20-second cool-down times are significantly different at 5% significance level.








## HW1

### Problem 1 

In a greenhouse experiment, 5 types of light were used: natural, white, green, blue, and yellow. Eight tobacco plants, each 10 mm tall, were subjected to each type of light with the following means and variances of stem growths (in mm).

|  Color | Natural | White | Yellow | Blue | Green |
|   ---- |    ---  |  ---  |  ----  | ---- | ----  |
|  Mean  | 4.09    |4.30   |3.55    |3.01  |3.19   |
|Variance| 0.027   |0.083  |0.066   |0.030 |0.073  |

A partial ANOVA table was found to be

|Source of Variance|Sum of Squares(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|
| ---  | ---  | --- | --- | --- |
|Treatment(Between or Model)|9.95904 |4 |2.48976|44.688|
|Error(Within)              |1.950   |35|0.05571429|   
|Total                      |11.90904      |39|

  (a) Complete the ANOVA table assuming the one-way fixed effects ANOVA model. Show main steps of your work. Keep 3 decimal places in $SS_{Trt}$

For $a=5,n=8$, $df_T=N-1=5\times8-1=39$, $df_{Trt}=5-1=4$, $df_E=N-a=35$

$\bar y_{..}=\frac{1}a\sum^a_{i=1}\bar y_{i.}=\frac{1}5(4.09+4.30+3.55+3.01+3.19)=3.628$

$SS_{Trt}=n\sum^a_{i=1}(\bar y_{i.}‚àí\bar y_{..})^2=9.95904$; $SS_{T}=SS_{Trt}+SS_{E}=11.90904$;

$MS_{Trt}=\frac{SS_{Trt}}{a-1}=\frac{9.95904}4=2.48976$; $MS_{E}=\frac{SS_{E}}{N-a}=\frac{1.950}{35}=0.05571429$;

$F_{0}=\frac{MS_{Trt}}{MS_{E}}=\frac{2.48976}{0.05571429}=44.688$

```{r eval=FALSE, collapse=TRUE, include=FALSE}
bar_y_all <- (4.09+4.30+3.55+3.01+3.19)/5
bar_y_all
ss_trt <- 8*((4.09-bar_y_all)^2+(4.30-bar_y_all)^2+(3.55-bar_y_all)^2+(3.01-bar_y_all)^2+(3.19-bar_y_all)^2)
ss_trt
(ss_trt/4)/(1.950/35)
```

 --- 

  (b) Compute the standard error of the difference of two observed treatment means. (that is, compute $se(\bar y _{i.}-\bar y_{j.})$) Show main steps of your work.

$$se(\bar y _{i.}-\bar y_{j.})=\sqrt{\frac{2MSE}n}=\sqrt{\frac{2\times0.05571429}8}=0.1180194$$

```{r eval=FALSE, collapse=TRUE, include=FALSE}

sqrt(2*1.950/{8*35})

```
 ---

### Problem 2 (3.37/45)

Exercise 3.37. Assume n_i is the number of observations (replications) for the $i^th$ treatment in the one-way ANVOA. Show main steps of work.
Show that the variance of the linear combination $\sum^{a}_{i=1}c_iy_{i.}$ is $œÉ^2\sum^{a}_{i=1}n_ic_i^2$

By the assumption for linear combination,$c_i$ is contrast constants. This is an unbalanced CRD, $\mu_i$ is not random. For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$ and are uncorrelated, $Var(Œµ_{ij})=\sigma^2$. Thus,

$$Var\left[\sum^{a}_{i=1}c_iy_{i.}\right]=\sum^{a}_{i=1}\left[c_i^2Var(\sum^{n_i}_{j=1}y_{ij})\right]=\sum^{a}_{i=1}\left[c_i^2Var(\sum^{n_i}_{i=1}(\mu_i+Œµ_{ij}))\right]=\sum^{a}_{i=1}\left[c_i^2\sum^{n_i}_{j=1}Var(Œµ_{ij})\right]=\sum^{a}_{i=1}\left[c_i^2n_i\sigma^2\right]=œÉ^2\sum^{a}_{i=1}n_ic_i^2$$

 ---

### Problem 3

Let $y_{ij}=\mu+\tau_{i}+\varepsilon_{ij}, Œµ_{ij}\sim iid\ N(0,œÉ^2)$,for $i=1,2,‚Ä¶,a;\ j=1,2,‚Ä¶,n$;$\sum_{i=1}^a\tau_i=0$, $\bar y_{i.}=\frac1n \sum_{j=1}^ny_{ij}$, $\bar y_{i.}\sim iid\ N(\mu+\tau_i,\frac{\sigma^2}n)$

  (a) Using ONLY the above given information and properties of expected value of a function of random variables, derive (DO NOT use any other distribution properties)

For a fixed effects model, $\mu$ is a constant and $\tau_i$ is a constant of the $i^{th}$ treatment, then

$y_{ij}-\bar y_{i.}=\mu+\tau_{i}+\varepsilon_{ij}-\frac1n \sum_{j=1}^n(\mu+\tau_{i}+\varepsilon_{ij})=\mu+\tau_{i}+\varepsilon_{ij}-\mu-\tau_{i}-\frac1n \sum_{j=1}^n\varepsilon_{ij}=\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}$

For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$, $E[\varepsilon_{ij}]=0$

$$E(y_{ij}-\bar y_{i.})=E[\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}]=E[\varepsilon_{ij}]-\frac1n \sum_{j=1}^nE[\varepsilon_{ij}]=0$$

$Var(y_{ij}-\bar y_{i.})=Var[\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}]=Var[\varepsilon_{ij}]+\frac1{n^2}\sum_{j=1}^nVar[\varepsilon_{ij}]-\frac2nCov[\varepsilon_{ij},\sum_{j=1}^n\varepsilon_{ij}]$

For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$, $Var[\varepsilon_{ij}]=Cov[\varepsilon_{ij},\varepsilon_{ij}]=\sigma^2$, $Cov[\varepsilon_{ij},\varepsilon_{ik}]=0,k\neq j$ then

$-\frac2nCov[\varepsilon_{ij},\sum_{j=1}^n\varepsilon_{ij}]=-\frac2nCov[\varepsilon_{ij},(\varepsilon_{i1}+..+\varepsilon_{ij}..+\varepsilon_{in})]=-\frac2n(Cov[\varepsilon_{ij},\varepsilon_{i1}]+..+Cov[\varepsilon_{ij},\varepsilon_{ij}]..+Cov[\varepsilon_{ij},\varepsilon_{in}])=-\frac2n\sigma^2$

$$Var(y_{ij}-\bar y_{i.})=\sigma^2+\frac{\sigma^2}n-\frac2n\sigma^2=\frac{n-1}n\sigma^2$$
 
 ---

  (b) Using your answers in part (a) and shortcut formula for variance ($Var(X)=E(X^2)-E(X)^2$), find $E[(y_{ij}-\bar y_{i.})^2]$

For $Var[y_{ij}-\bar y_{i.}]=E[(y_{ij}-\bar y_{i.})^2]-E[y_{ij}-\bar y_{i.}]^2$, then

$$E[(y_{ij}-\bar y_{i.})^2]=Var[y_{ij}-\bar y_{i.}]+E[y_{ij}-\bar y_{i.}]^2=\frac{n-1}n\sigma^2$$

 ---

  (c) Let $a=3$ treatments in the above model and the following linear functions of $\mu_i=\mu+\tau_i$ are tested. $\mu_1=\mu_2$, $\frac{\mu_1+\mu_2}{2}=\mu_3$.

   (i) Prove that each of these is a contrast and they are orthogonal contrasts.

For $a=3,\mu_1=\mu_2$, let $c_1=1,c_2=-1,c_3=0$. For $\frac{\mu_1+\mu_2}2=\mu_3$, let $d_1=1,d_2=1,d_3=-2$

$$\sum_{i=1}^3c_i=1-1+0=0;\quad\sum_{i=1}^3d_i=1+1-2=0$$

$$\sum_{i=1}^3c_id_i=1*1+(-1)*1+(-2)*0=0$$

Therefore, they are orthogonal contrasts.

 ---

   (ii) Write Sum of Squares for each contrast $SS_c$ in terms of $y_{1.},y_{2.},y_{3.}$.
    
$$SS_c=\frac{C^2}{\frac1n\sum_{i=1}^ac_i^2}=\frac{(\sum_{i=1}^ac_i\bar y_{i.})^2}{\frac1n\sum_{i=1}^ac_i^2}=\begin{cases}\frac{(\bar y_{1.}-\bar y_{2.})^2}{\frac{1+1}n}=\frac{n(\bar y_{1.}-\bar y_{2.})^2}{2}
&for\ \mu_1=\mu_2\\
\frac{(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{\frac{1+1+4}n}=\frac{n(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{6}
&for\ \frac{\mu_1+\mu_2}{2}=\mu_3
\end{cases}$$
    
  ---
    
   (iii) Add the two $SS_c$ you found in part (ii) and simplify (expand and bring similar terms together).

$SS_c=\frac{n(\bar y_{1.}-\bar y_{2.})^2}{2}+\frac{n(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{6}=\frac{n}6(4\bar y_{1.}^2+4\bar y_{2.}^2+4\bar y_{3.}^2-4\bar y_{1.}\bar y_{2.}-4\bar y_{2.}\bar y_{3.}-4\bar y_{1.}\bar y_{3.})$

Then expand and simplify the $SS_{Trt}$ given below to prove that it is equivalent to the sum of two $SS_c$ you found earlier.

$SS_{Trt}=n\sum_{i=1}^3(\bar y_{i.}-\bar y_{..})^2=\frac{n}9\sum_{i=1}^3(3\bar y_{i.}-\bar y_{1.}-\bar y_{2.}-\bar y_{3.})^2=\frac{n}9[(2\bar y_{1.}-\bar y_{2.}-\bar y_{3.})^2+(2\bar y_{2.}-\bar y_{1.}-\bar y_{3.})^2+(2\bar y_{3.}-\bar y_{1.}-\bar y_{2.})^2]$
$=\frac{n}9[6\bar y_{1.}^2+6\bar y_{2.}^2+6\bar y_{3.}^2-6\bar y_{1.}\bar y_{2.}-6\bar y_{2.}\bar y_{3.}-6\bar y_{1.}\bar y_{3.}]$

$$\therefore SS_c=SS_{Trt}=\frac{2n}3(\bar y_{1.}^2+\bar y_{2.}^2+\bar y_{3.}^2-\bar y_{1.}\bar y_{2.}-\bar y_{2.}\bar y_{3.}-\bar y_{1.}\bar y_{3.})$$

### Problem 4 (3.26/30)

Use software to do all the computations, and attach code and output at the end of the question.
If your answers rely on graphs, please include them along with the answer appropriately
Report p-value along with your conclusions
Data are given in Exercise3_26 Excel file .

Three brands of batteries are under study. It is suspected that the lives (in weeks) of the three brands are different. Five randomly selected batteries of each brand are tested with the following results:

```{r include=FALSE}
library(readxl)
library(tidyverse)
require(ggplot2)
library(car)
library(olsrr)

library(vcd)
library(vcdExtra)
library(multcompView)
library(agricolae) #LSD.test
library(asbio) #pairw.anova
library(pwr) 
```

  (a) Are the lives of these brands of batteries different?
  
The relevant overall hypotheses are $H_0: \mu_1=\mu_2=\mu_3, H_1:$ at lesat two of $\mu_i$ are different.

The ANOVA result shows that p-value of F-test ($6.141\times10^{-6}$) is small enough to reject $H_0$. We can conclude there is a significant differce in treatment means or these brands have an effect on the life of battery on 0.05 significent level.

```{r echo=T, collapse=TRUE}
table_battery <- read_xlsx("Exercise3_26.xlsx")
model_battery <- aov(Life~Brand, data=table_battery)
summary(model_battery)
```

 ---
 
  (b) Analyze the residuals from this experiment.

 - The histogram shows a normality of of residuals.
 - The QQ plot of residuals doesn't show obvious problem of normality of residuals.
 - The residuals versus fitted values and studentized residuals show no violation of zero mean and constant variance assumptions.
 - There is not outlier and leverage data points. 
 - The residuals versus observation number shows the residuals are independent with each other.
 - The Bartlett‚Äôs test shows variances across samples are not different. 

```{r echo=T,collapse=TRUE}
ols_plot_diagnostics(lm(Life~Brand, data=table_battery))
bartlett.test(Life~Brand, data=table_battery)
```
  
 ---
 
  (c) Construct a 95 percent confidence interval estimate on the mean life of battery brand 2. Construct a 99 percent confidence interval estimate on the mean difference between the lives of battery brands 2 and 3.

The 95% confidence interval on the mean life of battery brand 2 is:

$$\bar y_{i.}\pm t_{\frac{\alpha}2,N-a}\sqrt\frac{MS_E}n=79.4\pm t_{\frac{0.05}2,18-3}\sqrt\frac{15.6}5$$
  
The 99% confidence interval on the mean difference between the lives of battery brands 2 and 3 is:

$$\bar y_{i.}-\bar y_{j.}\pm t_{\frac{\alpha}2,N-a}\sqrt\frac{2MS_E}n=79.4-100.4\pm t_{\frac{0.01}2,18-3}\sqrt\frac{2*15.6}5$$

Therefore, $75.551\le\mu_2\le83.249$, $-28.631\le\mu_2-\mu_3\le-13.369$
  
```{r echo=T,collapse=TRUE}
(LSD.test(model_battery, trt="Brand",alpha = 0.05))
pairw.anova(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
bonf.cont(table_battery$Life,as.factor(table_battery$Brand), lvl = c("Brand2", "Brand3"), conf.level = 0.99, MSE = NULL, df.err = NULL, comps = 1)
``` 

 ---
 
  (d) Which brand would you select for use? If the manufacturer will replace without charge any battery that fails in less than 85 weeks, what percentage would the company expect to replace?

Chose brand 3 for longest life. Mean life of this brand in 100.4 weeks, and the variance of life is estimated by 15.60 (MSE). Assuming normality, then the probability of failure before 85 weeks is:

$$Œ¶\left(\frac{85-100.4}{\sqrt{15.60}}\right)=Œ¶(-0.390)=0.00005$$

That is, about 5 out of 100,000 batteries will fail before 85 week.

### Problem 5 (3.22/26)

The response time in milliseconds was determined for three different types of circuits that could be used in an automatic valve shutoff mechanism. The results from a completely randomized experiment are shown in the following table:

  (a) Test the hypothesis that the three circuit types have the same response time. Use ùõº = 0.01.
  
The relevant overall hypotheses are $H_0: \mu_1=\mu_2=\mu_3, H_1:$ at lesat two of $\mu_i$ are different.

The ANOVA result shows that p-value of F-test ($0.000402$) is small enough to reject $H_0$. We can conclude there is a significant differce in treatment means or these circuits types have an effect on the response time on 0.01 significent level.

```{r echo=T, collapse=TRUE}
table_circuit <- read_xlsx("Exercise3_22.xlsx")
model_circuit <- aov(Respone~Type, data=table_circuit)
summary(model_circuit)
```

 ---
 
  (b) Use Tukey‚Äôs test to compare pairs of treatment means. Use ùõº = 0.01.

Let $H_{(1-2)0}:\mu_1-\mu_2=0,H_{(1-2)1}:\mu_1-\mu_2\neq0$; $H_{(1-3)0}:\mu_1-\mu_3=0,H_{(1-3)1}:\mu_1-\mu_3\neq0$;$H_{(2-3)0}:\mu_2-\mu_3=0,H_{(2-3)1}:\mu_2-\mu_3\neq0$;

The P-values of F-test are $P_{(1-2)}=0.0.002366, P_{(1-3)}=0.636704, P_{(2-3)}=0.000504$. We can reject $H_{(1-2)0}$ and $H_{(2-3)0}$, but fail to reject $H_{(1-3)0}$. We can conclude treatment means of type 1 and type 2, treatment means of type 2 and type 3 has a significant differce on 0.01 significent level. There is not significant differce between type 1 and type 3 on 0.01 significent level.

```{r echo=T, collapse=TRUE}
TukeyHSD(aov(model_circuit), ordered = TRUE,conf.level=0.99)
pairw.anova(table_circuit$Respone,as.factor(table_circuit$Type), conf.level = 0.99, method = "tukey", MSE = NULL, df.err = NULL, control = NULL)
```

 ---
 
  (d) Construct a set of orthogonal contrasts, assuming that at the outset of the experiment you suspected the response time of circuit type 2 to be different from the other two.

Construct a set of contrasts $\sum_{i=1}^3c_i=1-2+1=0,\sum_{i=1}^3d_i=1+0-1=0$

$$\begin{cases}\mu_1-2\mu_2+\mu_3=0\\\mu_1-\mu_3=0\end{cases}$$

For $\sum_{i=1}^3c_id_i=1*1+(-2)*0+1*(-1)=0$, they are orthogonal contrasts.

$$C=\sum_{i=1}^ac_i\bar y_{i.}=\bar y_{1.}-2\bar y_{2.}+\bar y_{3.}=10.8-2(22.2)+8.4=-25.2$$

$$H_{C0}:\mu_1-2\mu_2+\mu_3=0\quad H_{C1}:\mu_1-2\mu_2+\mu_3\neq0$$

$$F_{1,12}=\frac{SS_C}{MS_E}=\frac{C^2}{\frac{MS_E}n\sum_{i=1}^ac_i^2}=\frac{(-25.2)^2}{\frac{16.9}56}=31.31$$

For the first contrast, the P-value of F-test ($0.0001169669$) is small enough to reject $H_{C0}$, We can conclude treatment means of type 2 has a significant differenence with treatment means of type 1 and type 3 on 0.01 significent level.

$$D=\sum_{i=1}^ac_i\bar y_{i.}=\bar y_{1.}-\bar y_{3.}=10.8-8.4=2.4$$

$$H_{D0}:\mu_1-\mu_3=0\quad H_{D1}:\mu_1-\mu_3\neq0$$

$$F_{1,12}=\frac{SS_C}{MS_E}=\frac{C^2}{\frac{MS_E}n\sum_{i=1}^ac_i^2}=\frac{2.4^2}{\frac{16.9}52}=0.852071$$

For the second contrast, the P-value of F-test ($0.374155$) is large and fail to reject $H_{D0}$, We can conclude there is not a significant differce between treatment means of type 1 and type 3 on 0.01 significent level.

```{r, collapse=TRUE}
HSD.test(model_circuit,"Type", group=TRUE,console=TRUE)

(25.2^2*5)/(16.9*6)
(2.4^2*5)/(16.9*2)

qf(0.95, 1, 12, lower.tail = TRUE, log.p = FALSE)
1-pf(31.31, 1, 12, lower.tail = TRUE, log.p = FALSE)
1-pf(0.852071, 1, 12, lower.tail = TRUE, log.p = FALSE)
```

 ---

  (e) If you were the design engineer and you wished to minimize the response time, which circuit type would you select?

Type 1 and type 3 have lower response time ($\mu_1=10.8,\mu_3=8.4$). We had proved the mean of response time for type 1 and type 3 have not significant different on response time on 0.01 significant level. The engineer could choose either one.

 ---
 
  (f) Analyze the residuals from this experiment. Are the basic analysis of variance assumptions satisfied?

 - The histogram doesn't show a quite normality of of residuals.
 - The QQ plot of residuals shows some  violaion of normality of residuals. The respnse variable may need to be transformed.
 - The residuals versus fitted values and studentized residuals show no violation of zero mean and constant variance assumptions.
 - There are some outlier and leverage data points. 
 - Compared with the Bartlett‚Äôs test , the Levene test is less sensitive to departures from normality. The result shows variances across samples may be equal. 
 
 Therefore, we should check the outlier points and try to do some transfoation. 

```{r  echo=T, collapse=TRUE}
ols_plot_diagnostics(lm(Respone~Type, data=table_circuit))
bartlett.test(Respone~Type, data=table_circuit)
leveneTest(model_circuit)
```
  

### Problem 6: (3.41/9 and 3.49/57)

  (3.41/9) Refer to Problem 5. If we wish to detect a maximum difference in mean response times of 10 milliseconds with a probability of at least 0.90, what sample size should be used? How would you obtain a preliminary estimate of $œÉ^2$?

For $a=3, D=10$, the preliminary estimate $\hat\sigma^2=MS_E=16.9$.

$$Œ¶^2=\frac{nD^2}{2aœÉ^2}=\frac{n(10)^2}{2(3)(16.9)}=0.986n$$

Let $Œ±=0.05$, accept probability $P=0.1,\nu_1=a‚àí1=2$

The results show that when $n=5$, the power is 0.86476, when $n=6$, the power is 0.9345867. Thus, the sample size should be $n\ge6, N\ge18$.

```{r echo=T, collapse=TRUE}
library(pwr2)
pwr.1way(k=3, n=5, alpha=0.05, f=NULL, delta=10, sigma=sqrt(16.9))
pwr.1way(k=3, n=6, alpha=0.05, f=NULL, delta=10, sigma=sqrt(16.9))
```

 ---

  (3.49/57) Consider the data shown in Problem 5.

  (a) Write out the least squares normal equations for this problem and solve them for $\hat\mu$ and $\hat\tau_i$, using the usual constraint ($\sum^3_{i=1} \hat\tau_i=0$). Estimate $\tau_1-\tau_2$

For $\sum^3_{i=1} \hat\tau_i=0$,

$$\begin{cases}\hat\tau_1+\hat\tau_2+\hat\tau_3=0\\\hat\mu+\hat\tau_1=\bar y_{1.}=10.8\\\hat\mu+\hat\tau_2=\bar y_{2.}=22.2\\\hat\mu+\hat\tau_3=\bar y_{3.}=8.4\end{cases}\implies
\begin{cases}\hat\mu=13.80\\\hat\tau_1=-3.00\\\hat\tau_2=8.40\\\hat\tau_3=-5.40\end{cases}\implies
\hat\tau_1-\hat\tau_2=-11.40$$

 ---

  (b) Solve the equations in (a) using the constraint $\hat\tau_3=0$. Are the estimator $\hat\tau_i$ and $\hat\mu$ the same as you found in (a)? Why? Now estimate $\tau_1-\tau_2$ and compare your answer with that for (a). What statement can you make about estimating contrasts in the $\tau_i$

$$\begin{cases}\hat\tau_3=0\\\hat\mu+\hat\tau_1=\bar y_{1.}=10.8\\\hat\mu+\hat\tau_2=\bar y_{2.}=22.2\\\hat\mu+\hat\tau_3=\bar y_{3.}=8.4\end{cases}\implies
\begin{cases}\hat\mu=8.40\\\hat\tau_1=2.40\\\hat\tau_2=13.8\\\hat\tau_3=0\end{cases}\implies
\hat\tau_1-\hat\tau_2=-11.40$$

The result shows the value of $\hatœÑ_1-\hatœÑ_2$ is irrelevant with the value of $\hatœÑ_3$. The contrasts are estimable.

 ---
 
  (c) Estimate $\mu+\tau_1$, $2\tau_1-\tau_2-\tau_3$, and $\mu+\tau_1+\tau_2$ using the two solutions to the normal equations. Compare the results obtained in each case.
  
|     Contrast          |Estimated from Part (a)|Estimated from Part (b)|
|          ----         |           ---         |            ---        |
|$\mu+\tau_1$           | 10.80   |10.80  |
|$2\tau_1-\tau_2-\tau_3$| -9.00   |-9.00  |
|$\mu+\tau_1+\tau_2$    | 19.20   |24.60  |

The result shows that $\tau_1$ is independent with $\tau_3$, $\tau_1$ is independent with $\tau_2$ and $\tau_3$, while $\tau_1$ and $\tau_2$ is dependent with $\tau_3$.


  
```{r eval=FALSE, include=FALSE}
lsdCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
bonfCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
tukeyCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
scheffeCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
dunnettCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, control = "Brand2")
scheffe.cont(table_battery$Life,as.factor(table_battery$Brand), lvl = c("Brand2", "Brand3"), conf.level = 0.99, MSE = NULL, df.err = NULL)

https://www.statmethods.net/stats/power.html

(LSD.test(model_circuit, trt="Type",alpha = 0.05))

power.anova.test(groups = 3, n =NULL,between.var =var(c(10.8,22.2,8.4)), within.var = 16.9, sig.level = 0.01, power = 0.9)

power.anova.test(groups = 3, n =NULL,between.var =var(table_circuit$Respone)/2within.var = 16.9, sig.level = 0.05,1power = 0.9)

power.anova.test(groups = 3, n =NULL,between.var =(var(table_circuit$Respone)-16.9)/2within.var = 16.9, sig.level = 0.05,1power = 0.9)

# pwr  type = c("two.sample", "one.sample", "paired"),alternative = c("two.sided", "less", "greater"))
pwr.t.test(n =5,d =10, sig.level =0.05,1power =, 0.9type ="one.sample", alternative ="two.sided")
# biotools
# samplesize(x, fun, sizes = NULL, lcl = NULL, ucl = NULL,nboot = 200, conf.level = 0.95, nrep = 500, graph = TRUE, ...)


ggplot(table_battery, aes(x = Brand, y = Life)) + geom_boxplot()

prop.table(xtabs(~Life+Brand,data=table_battery,subset=!is.na(Brand), drop.unused.levels=T), 2)

chisq.test(xtabs(~Brand+Life,data=table_battery,subset=!is.na(Life)))

GKgamma(xtabs(~Respone + Type,data=table_circuit,subset=!is.na(Type), drop.unused.levels=T))
#running Leven's Equality of Variance
leveneTest(Life ~ Brand, data = table_battery,center = "mean")
```






## HW2

### Problem 1 (4.9-29)

Three different washing solutions are being compared to study their effectiveness in retarding bacteria growth in 5-gallon milk containers. The analysis is done in a laboratory,
and only three trials can be run on any day. Because days could represent a potential source of variability, the experimenter decides to use a randomized block design. Observations
are taken for four days, and the data are shown here. Analyze the data from this experiment (use $\alpha = 0.05$) and draw conclusions.

```{r include=FALSE}
table_wash <- read_xlsx("Exercise4_4.xlsx")
glimpse(table_wash)
table_wash %>% ggplot(aes(Solution,Response))+geom_line(aes(Solution,Response,lty=as.factor(Days)))+theme(legend.position = c(0.9, 0.85))
```

```{r echo=T, collapse=TRUE}


model_wash <- aov(Response~Solution+Days, data=table_wash)
summary(model_wash)
```

 ---

4.29 Consider the randomized complete block design in Problem 4.9. Assume that the days are random. Estimate the block variance component.


### Problem 2 (4.13)

A consumer products company relies on direct mail marketing pieces as a major component of its advertising campaigns. The company has three different designs for a new brochure and wants to evaluate their effectiveness, as there are substantial differences in costs between the three designs.
The company decides to test the three designs by mailing 5000 samples of each to potential customers in four different regions of the country. Since there are known regional differences in the customer base, regions are considered as blocks. The number of responses to each mailing is as follows.

```{r include=FALSE}
table_mail <- read_xlsx("Exercise4_8.xlsx")
glimpse(table_mail)
table_mail %>% ggplot(aes(Design,Response))+geom_line(aes(Design,Response,lty=as.factor(Region)))+theme(legend.position = c(0.9, 0.85))
```


(a) Analyze the data from this experiment.

```{r echo=T, collapse=TRUE}
model_mail <- aov(Response~Design+Region, data=table_mail)
summary(model_mail)
```

 ---

(b) Use the Fisher LSD method to make comparisons among the three designs to determine specifically which designs differ in the mean response rate.

```{r echo=T,collapse=TRUE}
(LSD.test(model_mail, trt="Design",alpha = 0.05))
pairw.anova(table_mail$Response,as.factor(table_mail$Design), conf.level = 0.95, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
``` 

 ---
 
(c) Analyze the residuals from this experiment.

 - The histogram shows a normality of of residuals.
 - The QQ plot of residuals doesn't show obvious problem of normality of residuals.
 - The residuals versus fitted values and studentized residuals show no violation of zero mean and constant variance assumptions.
 - There is not outlier and leverage data points. 
 - The residuals versus observation number shows the residuals are independent with each other.
 - The Bartlett‚Äôs test shows variances across samples are not different. 

```{r echo=T,collapse=TRUE, out.width='45%'}
plot(model_mail)
ols_plot_diagnostics(lm(Response~Design+Region, data=table_mail))
# bartlett.test(Response~Design+Region, data=table_mail)
```

### Problem 3

$y_{ijkl}=Œº+œÑ_i+Œ±_{j(l)}+Œ≤_k+Œ¥_l+Œµ_{ijkl}$ for $i,j,k =1,‚Ä¶,p$ ; $l=1,‚Ä¶,n$; $Œµ_{ijkl}\sim iid N(0,œÉ^2 )$
Derive the parameter estimates from the least squares method using the constrains
$\sum_i\hat\tau_i=0;\ \sum_j\hat\alpha_{j(l)}=0;\ \sum_l\hat\alpha_{j(l)}=0;\ \sum_k\hat\beta_k=0;\ \sum_l\hat\delta_l=0$

$$SSE=\sum_i^p\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-Œº-œÑ_i-Œ±_{j(l)}-Œ≤_k-Œ¥_l)^2$$

Derive

$\left.\frac{\partial SSE}{\partial Œº}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=-2\sum_i^p\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial œÑ_i}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=-2\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ±_{j(l)}}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=-2\sum_i^p\sum_k^p(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ≤_k}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=-2\sum_i^p\sum_j^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ¥_l}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=-2\sum_i^p\sum_j^p\sum_k^p(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$$\left\{\begin{array}{l} 
y_{....}=np^2\hatŒº+np\sum_i^p\hatœÑ_i+p^2\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\sum_k^p\hatŒ≤_k+p^2\sum_l^n\hatŒ¥_l\\
y_{i...}=np\hatŒº+np\hatœÑ_i+p\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\sum_k^p\hatŒ≤_k+p^2\sum_l^n\hatŒ¥_l \\
y_{.j.l}=p^2\hatŒº+p\sum_i^p\hatœÑ_i+p^2\hatŒ±_{j(l)}+p\sum_k^p\hatŒ≤_k+p^2\hatŒ¥_l \\
y_{..k.}=np\hatŒº+n\sum_i^p\hatœÑ_i+\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\hatŒ≤_k+p\sum_l^n\hatŒ¥_l \\
y_{...l}=p^2\hatŒº+p^2\sum_i^p\hatœÑ_i+p^2\sum_j^p\hatŒ±_{j(l)}+p^2\sum_k^p\hatŒ≤_k+p^2\hatŒ¥_l\\
\end{array}\right.$$

For $\sum_i\hat\tau_i=0;\ \sum_j\hat\alpha_{j(l)}=0;\ \sum_l\hat\alpha_{j(l)}=0;\ \sum_k\hat\beta_k=0;\ \sum_l\hat\delta_l=0$
For $\hatŒº$ is constant, $\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l$ are constants for summations on other parameters.
For $y_{....}=np^2 \bar y_{....}=np\bar y_{i...}=np\bar y_{.j..}=np\bar y_{..k.}=p^2\bar y_{...l}$
For $y_{i...}=np\bar y_{i...}$, $y_{.j..}=np\bar y_{.j..}$, $y_{..k.}=np\bar y_{..k.}$, $y_{...l}=p^2\bar y_{...l}$,

$$\left\{\begin{array}{l}\ np^2\bar y_{....}=np^2\hatŒº+0+0+0+0\\
np\bar y_{i...}=np\hatŒº+np\hatœÑ_i+0+0+0 \\
p^2\bar y_{.j.k}=p^2\hatŒº+0+p^2\hatŒ±_{j(l)}+0+p^2\hatŒ¥_l \\
np\bar y_{..k.}=np\hatŒº+0+0+np\hatŒ≤_k+0 \\
p^2\bar y_{...l}=p^2\hatŒº+0+0+0+p^2\hatŒ¥_l \end{array}\right.
\implies
\left\{\begin{array}{l}\ \hatŒº=\bar y_{....}\\
\hatœÑ_i=\bar y_{i...}-\bar y_{....}\\
\hatŒ±_{j(l)}=\bar y_{.j.l}-\bar y_{...l} \\
\hatŒ≤_k=\bar y_{..k.}-\bar y_{....} \\
\hatŒ¥_l=\bar y_{...l}-\bar y_{....} \end{array}\right.$$


