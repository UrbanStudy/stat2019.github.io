---
title: 'STAT565 Homework'
author: ""
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW0

### Problem 1 (2.1)

Compute the values of the missing quantities.

|Variable| N | Mean| SE Mean|Std. Dev.|Variance|Minimum| Maximum
|  ----  |---| ----|  ----  |   ----  |  ----  |  ---- | ----
|Y       | 9 |19.96|   ?    |3.12     |       ?|  15.94| 27.16

$$\text{SE mean}=\frac{Std.Dev.}{\sqrt{n}}=\frac{3.12}{\sqrt9}=1.04$$

$$Var(Y)=S^2=3.12^2=9.7344$$

### Problem 2 (2.17/19)

The viscosity of a liquid detergent is supposed to average 800 centistokes at 25‚àòC. A random sample of 16 batches of detergent is collected, and the average viscosity is 812. Suppose we know that the standard deviation of viscosity is œÉ = 25 centistokes.

 (a) State the hypotheses that should be tested.
 Let $\mu=800, \bar y=812, n=16, œÉ=25$
 
 $$H_0: \mu=800\qquad H_A: \mu\neq800$$

 ---
 
 (b) Test these hypotheses using ùõº = 0.05. What are your conclusions?
 
Assuming the population is normal, the sample size is less than 30 but the variance is known. The hypothesis may be tested using a direct application of the normal distribution.
 
 $$se(\hat\mu)=\frac{œÉ}{\sqrt{n}}=\frac{25}{\sqrt{16}}=6.25$$
 
 $$Z_0=\frac{\bar y-\mu}{se(\hat\mu)}=\frac{812-800}{6.25}=1.92 $$
 
 Since $Z_{\frac{Œ±}2} =Z_{0.025}=1.96$, do not reject.
 
 ---
 
 (c) What is the P-value for the test?
 
 The P-value is below:
 
```{r}

2*(1-pnorm(812, mean = 800, sd = 25/4, lower.tail = TRUE, log.p = FALSE))

2*pnorm(-1.92)

```

 ---
 
 (d) Find a 95 percent confidence interval on the mean.

$$\bar y-Z_{\frac{Œ±}2}se(\mu)\le\mu\le\bar y+Z_{\frac{Œ±}2}se(\mu)\implies 812-1.96\times6.25\le\mu\le812+1.96\times6.25$$

$$\implies 799.75\le\mu\le824.25$$


### Problem 3

The simple linear regression model: $y_i=Œ≤_0+Œ≤_1 x_i+Œµ$, $E(Œµ_i)=0$, $Var(Œµ_i)=\sigma^2$, for $i=1,2,‚Ä¶,n$; $Cov(Œµ_i,Œµ_j)=0$ for $i‚â†j$, $x_i=\begin{cases}0& \text{for Male for }i=1,2,‚Ä¶,\frac{n}2\\1& \text{for Female for}\frac{n}2+1,\frac{n}2+2,..n\end{cases}$

Use the least squares method to find the estimated value of $Œ≤_1$ and simplify your answer as much as possible. Show all the steps of your work.

For $Œµ_i\sim iid N(0, \sigma^2), i=1,2,‚Ä¶,n$, 

$S(\beta_0,\beta_1)=\sum_{i=1}^n\varepsilon_i=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2$

$$\left.\frac{\partial S}{\partial\beta_0}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-1)=0$$
$$\left.\frac{\partial S}{\partial\beta_1}\right|_{\hat\beta_0,\hat\beta_1}=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-x_i)=0$$

- Normal Equations

$$\sum_{i=1}^ny_i=n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i$$
$$\sum_{i=1}^nx_iy_i=\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2$$


For a indicator variable $x_i$, 

$\sum_{i=1}^nx_i=\sum_{i=1}^nx_i^2=\sum_{i=1}^{\frac{n}2}0+\sum_{i=\frac{n}2+1}^n1=\frac{n}2$,

$\sum_{i=1}^nx_iy_i=\sum_{i=1}^{\frac{n}2}0y_i+\sum_{i=\frac{n}2+1}^n1y_i=\sum_{i=\frac{n}2+1}^ny_i$, and 

$E[\bar y]=E[\frac{\sum_{i=1}^ny_i}n]=\mu$, $n\bar y=\sum_{i=1}^ny_i$, then the normal equations are


$$\bar y= \hat\beta_0+\frac{\hat\beta_1}2;\quad \sum_{i=\frac{n}2+1}^ny_i=\frac{n}2(\hat\beta_0+\hat\beta_1)$$

$$\therefore\frac2n\sum_{i=\frac{n}2+1}^ny_i=(\bar y-\frac{\hat\beta_1}2)+\hat{\beta_1}\implies\hat{\beta_1}=\frac4n\sum_{i=\frac{n}2+1}^ny_i-2\bar y$$


### Problem 4 (2.30/32)

Front housings for cell phones are manufactured in an injection molding process. The time the part is allowed to cool in the mold before removal is thought to influence the occurrence of a particularly troublesome cosmetic defect, flow lines, in the finished housing. After manufacturing, the housings are inspected visually and assigned a score between 1 and 10 based on their appearance, with 10 corresponding to a perfect part and 1 corresponding to a completely defective part. An experiment was conducted using two cool-down times, 10 and 20 seconds, and 20 housings were evaluated at each level of cool-down time. All 40 observations in this experiment were run in random order. The data are as follows.

```{r include=FALSE}

library(readxl)
library(tidyverse)

table_cellphone <- read_xlsx("Exercsie2_30.xlsx")

```
 (a) Is there evidence to support the claim that the longer cool-down time results in fewer appearance defects? Use ùõº = 0.05.

We use the paired t-test because the observations from the factor levels are ‚Äúpaired‚Äù on each experimental unit.

In order to test the claim that the longer cool-down time results in fewer appearance defects on average, the hypotheses are $H_0:\mu_{20-10}\le0;\quad H_1:\mu_{20-10}>0$.

```{r echo=T}

t.test(table_cellphone$TwentySeconds, table_cellphone$TenSeconds, paired = T,var.equal=T, alternative="greater", mu = 0, conf.level= 0.95)

```


P-value is less than significance level (5%) and null hypothesis ($H_0$) is rejected. Therefore, we can be 95% sure the housings with 20 seconds of cool-down times have a higher means of scores than 10 seconds at least 2.176087.

 ---

 (b) What is the P-value for the test conducted in part (a)?

p-value = $1.077\times10^{-05}$

 - -

 (c) Find a 95 percent confidence interval on the difference in means. Provide a practical interpretation of this interval.
 
In order to test the 95 percent confidence interval on the difference in means, the hypotheses are $H_0:\mu_{20-10}=0;\quad H_1:\mu_{20-10}\neq0$.

```{r echo=T}

t.test(score~cool_down_time, data=gather(table_cellphone,key=cool_down_time,value=score), paired = T,var.equal=T, alternative="two.sided", mu = 0, conf.level= 0.95)

```

P-value is less than significance level (5%) and the 95%  confidence bound is negative (-4.32887 -1.97113). Therefore, the null hypothesis ($H_0$) is rejected. We can be 95% confidence that the difference in average scores of housing with 10 seconds and 20 seconds of cool-down times is between -4.32887 and -1.97113. In other words, we can conclude that the scores of housings with 10-second and 20-second cool-down times are significantly different at 5% significance level.








## HW1

### Problem 1 

In a greenhouse experiment, 5 types of light were used: natural, white, green, blue, and yellow. Eight tobacco plants, each 10 mm tall, were subjected to each type of light with the following means and variances of stem growths (in mm).

|  Color | Natural | White | Yellow | Blue | Green |
|   ---- |    ---  |  ---  |  ----  | ---- | ----  |
|  Mean  | 4.09    |4.30   |3.55    |3.01  |3.19   |
|Variance| 0.027   |0.083  |0.066   |0.030 |0.073  |

A partial ANOVA table was found to be

|Source of Variance|Sum of Squares(SS)|Degrees of Freedom(df)|Mean Squares(MS)|F test statistic|
| ---  | ---  | --- | --- | --- |
|Treatment(Between or Model)|9.95904 |4 |2.48976|44.688|
|Error(Within)              |1.950   |35|0.05571429|   
|Total                      |11.90904      |39|

  (a) Complete the ANOVA table assuming the one-way fixed effects ANOVA model. Show main steps of your work. Keep 3 decimal places in $SS_{Trt}$

For $a=5,n=8$, $df_T=N-1=5\times8-1=39$, $df_{Trt}=5-1=4$, $df_E=N-a=35$

$\bar y_{..}=\frac{1}a\sum^a_{i=1}\bar y_{i.}=\frac{1}5(4.09+4.30+3.55+3.01+3.19)=3.628$

$SS_{Trt}=n\sum^a_{i=1}(\bar y_{i.}‚àí\bar y_{..})^2=9.95904$; $SS_{T}=SS_{Trt}+SS_{E}=11.90904$;

$MS_{Trt}=\frac{SS_{Trt}}{a-1}=\frac{9.95904}4=2.48976$; $MS_{E}=\frac{SS_{E}}{N-a}=\frac{1.950}{35}=0.05571429$;

$F_{0}=\frac{MS_{Trt}}{MS_{E}}=\frac{2.48976}{0.05571429}=44.688$

```{r eval=FALSE, collapse=TRUE, include=FALSE}
bar_y_all <- (4.09+4.30+3.55+3.01+3.19)/5
bar_y_all
ss_trt <- 8*((4.09-bar_y_all)^2+(4.30-bar_y_all)^2+(3.55-bar_y_all)^2+(3.01-bar_y_all)^2+(3.19-bar_y_all)^2)
ss_trt
(ss_trt/4)/(1.950/35)
```

 --- 

  (b) Compute the standard error of the difference of two observed treatment means. (that is, compute $se(\bar y _{i.}-\bar y_{j.})$) Show main steps of your work.

$$se(\bar y _{i.}-\bar y_{j.})=\sqrt{\frac{2MSE}n}=\sqrt{\frac{2\times0.05571429}8}=0.1180194$$

```{r eval=FALSE, collapse=TRUE, include=FALSE}

sqrt(2*1.950/{8*35})

```
 ---

### Problem 2 (3.37/45)

Exercise 3.37. Assume n_i is the number of observations (replications) for the $i^th$ treatment in the one-way ANVOA. Show main steps of work.
Show that the variance of the linear combination $\sum^{a}_{i=1}c_iy_{i.}$ is $œÉ^2\sum^{a}_{i=1}n_ic_i^2$

By the assumption for linear combination,$c_i$ is contrast constants. This is an unbalanced CRD, $\mu_i$ is not random. For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$ and are uncorrelated, $Var(Œµ_{ij})=\sigma^2$. Thus,

$$Var\left[\sum^{a}_{i=1}c_iy_{i.}\right]=\sum^{a}_{i=1}\left[c_i^2Var(\sum^{n_i}_{j=1}y_{ij})\right]=\sum^{a}_{i=1}\left[c_i^2Var(\sum^{n_i}_{i=1}(\mu_i+Œµ_{ij}))\right]=\sum^{a}_{i=1}\left[c_i^2\sum^{n_i}_{j=1}Var(Œµ_{ij})\right]=\sum^{a}_{i=1}\left[c_i^2n_i\sigma^2\right]=œÉ^2\sum^{a}_{i=1}n_ic_i^2$$

 ---

### Problem 3

Let $y_{ij}=\mu+\tau_{i}+\varepsilon_{ij}, Œµ_{ij}\sim iid\ N(0,œÉ^2)$,for $i=1,2,‚Ä¶,a;\ j=1,2,‚Ä¶,n$;$\sum_{i=1}^a\tau_i=0$, $\bar y_{i.}=\frac1n \sum_{j=1}^ny_{ij}$, $\bar y_{i.}\sim iid\ N(\mu+\tau_i,\frac{\sigma^2}n)$

  (a) Using ONLY the above given information and properties of expected value of a function of random variables, derive (DO NOT use any other distribution properties)

For a fixed effects model, $\mu$ is a constant and $\tau_i$ is a constant of the $i^{th}$ treatment, then

$y_{ij}-\bar y_{i.}=\mu+\tau_{i}+\varepsilon_{ij}-\frac1n \sum_{j=1}^n(\mu+\tau_{i}+\varepsilon_{ij})=\mu+\tau_{i}+\varepsilon_{ij}-\mu-\tau_{i}-\frac1n \sum_{j=1}^n\varepsilon_{ij}=\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}$

For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$, $E[\varepsilon_{ij}]=0$

$$E(y_{ij}-\bar y_{i.})=E[\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}]=E[\varepsilon_{ij}]-\frac1n \sum_{j=1}^nE[\varepsilon_{ij}]=0$$

$Var(y_{ij}-\bar y_{i.})=Var[\varepsilon_{ij}-\frac1n \sum_{j=1}^n\varepsilon_{ij}]=Var[\varepsilon_{ij}]+\frac1{n^2}\sum_{j=1}^nVar[\varepsilon_{ij}]-\frac2nCov[\varepsilon_{ij},\sum_{j=1}^n\varepsilon_{ij}]$

For $Œµ_{ij}\sim iid\ N(0,œÉ^2)$, $Var[\varepsilon_{ij}]=Cov[\varepsilon_{ij},\varepsilon_{ij}]=\sigma^2$, $Cov[\varepsilon_{ij},\varepsilon_{ik}]=0,k\neq j$ then

$-\frac2nCov[\varepsilon_{ij},\sum_{j=1}^n\varepsilon_{ij}]=-\frac2nCov[\varepsilon_{ij},(\varepsilon_{i1}+..+\varepsilon_{ij}..+\varepsilon_{in})]=-\frac2n(Cov[\varepsilon_{ij},\varepsilon_{i1}]+..+Cov[\varepsilon_{ij},\varepsilon_{ij}]..+Cov[\varepsilon_{ij},\varepsilon_{in}])=-\frac2n\sigma^2$

$$Var(y_{ij}-\bar y_{i.})=\sigma^2+\frac{\sigma^2}n-\frac2n\sigma^2=\frac{n-1}n\sigma^2$$
 
 ---

  (b) Using your answers in part (a) and shortcut formula for variance ($Var(X)=E(X^2)-E(X)^2$), find $E[(y_{ij}-\bar y_{i.})^2]$

For $Var[y_{ij}-\bar y_{i.}]=E[(y_{ij}-\bar y_{i.})^2]-E[y_{ij}-\bar y_{i.}]^2$, then

$$E[(y_{ij}-\bar y_{i.})^2]=Var[y_{ij}-\bar y_{i.}]+E[y_{ij}-\bar y_{i.}]^2=\frac{n-1}n\sigma^2$$

 ---

  (c) Let $a=3$ treatments in the above model and the following linear functions of $\mu_i=\mu+\tau_i$ are tested. $\mu_1=\mu_2$, $\frac{\mu_1+\mu_2}{2}=\mu_3$.

   (i) Prove that each of these is a contrast and they are orthogonal contrasts.

For $a=3,\mu_1=\mu_2$, let $c_1=1,c_2=-1,c_3=0$. For $\frac{\mu_1+\mu_2}2=\mu_3$, let $d_1=1,d_2=1,d_3=-2$

$$\sum_{i=1}^3c_i=1-1+0=0;\quad\sum_{i=1}^3d_i=1+1-2=0$$

$$\sum_{i=1}^3c_id_i=1*1+(-1)*1+(-2)*0=0$$

Therefore, they are orthogonal contrasts.

 ---

   (ii) Write Sum of Squares for each contrast $SS_c$ in terms of $y_{1.},y_{2.},y_{3.}$.
    
$$SS_c=\frac{C^2}{\frac1n\sum_{i=1}^ac_i^2}=\frac{(\sum_{i=1}^ac_i\bar y_{i.})^2}{\frac1n\sum_{i=1}^ac_i^2}=\begin{cases}\frac{(\bar y_{1.}-\bar y_{2.})^2}{\frac{1+1}n}=\frac{n(\bar y_{1.}-\bar y_{2.})^2}{2}
&for\ \mu_1=\mu_2\\
\frac{(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{\frac{1+1+4}n}=\frac{n(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{6}
&for\ \frac{\mu_1+\mu_2}{2}=\mu_3
\end{cases}$$
    
  ---
    
   (iii) Add the two $SS_c$ you found in part (ii) and simplify (expand and bring similar terms together).

$SS_c=\frac{n(\bar y_{1.}-\bar y_{2.})^2}{2}+\frac{n(\bar y_{1.}+\bar y_{2.}-2\bar y_{3.})^2}{6}=\frac{n}6(4\bar y_{1.}^2+4\bar y_{2.}^2+4\bar y_{3.}^2-4\bar y_{1.}\bar y_{2.}-4\bar y_{2.}\bar y_{3.}-4\bar y_{1.}\bar y_{3.})$

Then expand and simplify the $SS_{Trt}$ given below to prove that it is equivalent to the sum of two $SS_c$ you found earlier.

$SS_{Trt}=n\sum_{i=1}^3(\bar y_{i.}-\bar y_{..})^2=\frac{n}9\sum_{i=1}^3(3\bar y_{i.}-\bar y_{1.}-\bar y_{2.}-\bar y_{3.})^2=\frac{n}9[(2\bar y_{1.}-\bar y_{2.}-\bar y_{3.})^2+(2\bar y_{2.}-\bar y_{1.}-\bar y_{3.})^2+(2\bar y_{3.}-\bar y_{1.}-\bar y_{2.})^2]$
$=\frac{n}9[6\bar y_{1.}^2+6\bar y_{2.}^2+6\bar y_{3.}^2-6\bar y_{1.}\bar y_{2.}-6\bar y_{2.}\bar y_{3.}-6\bar y_{1.}\bar y_{3.}]$

$$\therefore SS_c=SS_{Trt}=\frac{2n}3(\bar y_{1.}^2+\bar y_{2.}^2+\bar y_{3.}^2-\bar y_{1.}\bar y_{2.}-\bar y_{2.}\bar y_{3.}-\bar y_{1.}\bar y_{3.})$$

### Problem 4 (3.26/30)

Use software to do all the computations, and attach code and output at the end of the question.
If your answers rely on graphs, please include them along with the answer appropriately
Report p-value along with your conclusions
Data are given in Exercise3_26 Excel file .

Three brands of batteries are under study. It is suspected that the lives (in weeks) of the three brands are different. Five randomly selected batteries of each brand are tested with the following results:

```{r include=FALSE}
library(readxl)
library(tidyverse)
require(ggplot2)
library(car)
library(olsrr)

library(vcd)
library(vcdExtra)
library(multcompView)
library(agricolae) #LSD.test
library(asbio) #pairw.anova
library(pwr) 
```

  (a) Are the lives of these brands of batteries different?
  
The relevant overall hypotheses are $H_0: \mu_1=\mu_2=\mu_3, H_1:$ at lesat two of $\mu_i$ are different.

The ANOVA result shows that p-value of F-test ($6.141\times10^{-6}$) is small enough to reject $H_0$. We can conclude there is a significant differce in treatment means or these brands have an effect on the life of battery on 0.05 significent level.

```{r echo=T, collapse=TRUE}
table_battery <- read_xlsx("Exercise3_26.xlsx")
model_battery <- aov(Life~Brand, data=table_battery)
summary(model_battery)
```

 ---
 
  (b) Analyze the residuals from this experiment.

 - The histogram shows a normality of of residuals.
 - The QQ plot of residuals doesn't show obvious problem of normality of residuals.
 - The residuals versus fitted values and studentized residuals show no violation of zero mean and constant variance assumptions.
 - There is not outlier and leverage data points. 
 - The residuals versus observation number shows the residuals are independent with each other.
 - The Bartlett‚Äôs test shows variances across samples are not different. 

```{r echo=T,collapse=TRUE}
ols_plot_diagnostics(lm(Life~Brand, data=table_battery))
bartlett.test(Life~Brand, data=table_battery)
```
  
 ---
 
  (c) Construct a 95 percent confidence interval estimate on the mean life of battery brand 2. Construct a 99 percent confidence interval estimate on the mean difference between the lives of battery brands 2 and 3.

The 95% confidence interval on the mean life of battery brand 2 is:

$$\bar y_{i.}\pm t_{\frac{\alpha}2,N-a}\sqrt\frac{MS_E}n=79.4\pm t_{\frac{0.05}2,18-3}\sqrt\frac{15.6}5$$
  
The 99% confidence interval on the mean difference between the lives of battery brands 2 and 3 is:

$$\bar y_{i.}-\bar y_{j.}\pm t_{\frac{\alpha}2,N-a}\sqrt\frac{2MS_E}n=79.4-100.4\pm t_{\frac{0.01}2,18-3}\sqrt\frac{2*15.6}5$$

Therefore, $75.551\le\mu_2\le83.249$, $-28.631\le\mu_2-\mu_3\le-13.369$
  
```{r echo=T,collapse=TRUE}
(LSD.test(model_battery, trt="Brand",alpha = 0.05))
pairw.anova(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
bonf.cont(table_battery$Life,as.factor(table_battery$Brand), lvl = c("Brand2", "Brand3"), conf.level = 0.99, MSE = NULL, df.err = NULL, comps = 1)
``` 

 ---
 
  (d) Which brand would you select for use? If the manufacturer will replace without charge any battery that fails in less than 85 weeks, what percentage would the company expect to replace?

Chose brand 3 for longest life. Mean life of this brand in 100.4 weeks, and the variance of life is estimated by 15.60 (MSE). Assuming normality, then the probability of failure before 85 weeks is:

$$Œ¶\left(\frac{85-100.4}{\sqrt{15.60}}\right)=Œ¶(-0.390)=0.00005$$

That is, about 5 out of 100,000 batteries will fail before 85 week.

### Problem 5 (3.22/26)

The response time in milliseconds was determined for three different types of circuits that could be used in an automatic valve shutoff mechanism. The results from a completely randomized experiment are shown in the following table:

  (a) Test the hypothesis that the three circuit types have the same response time. Use ùõº = 0.01.
  
The relevant overall hypotheses are $H_0: \mu_1=\mu_2=\mu_3, H_1:$ at lesat two of $\mu_i$ are different.

The ANOVA result shows that p-value of F-test ($0.000402$) is small enough to reject $H_0$. We can conclude there is a significant differce in treatment means or these circuits types have an effect on the response time on 0.01 significent level.

```{r echo=T, collapse=TRUE}
table_circuit <- read_xlsx("Exercise3_22.xlsx")
model_circuit <- aov(Respone~Type, data=table_circuit)
summary(model_circuit)
```

 ---
 
  (b) Use Tukey‚Äôs test to compare pairs of treatment means. Use ùõº = 0.01.

Let $H_{(1-2)0}:\mu_1-\mu_2=0,H_{(1-2)1}:\mu_1-\mu_2\neq0$; $H_{(1-3)0}:\mu_1-\mu_3=0,H_{(1-3)1}:\mu_1-\mu_3\neq0$;$H_{(2-3)0}:\mu_2-\mu_3=0,H_{(2-3)1}:\mu_2-\mu_3\neq0$;

The P-values of F-test are $P_{(1-2)}=0.0.002366, P_{(1-3)}=0.636704, P_{(2-3)}=0.000504$. We can reject $H_{(1-2)0}$ and $H_{(2-3)0}$, but fail to reject $H_{(1-3)0}$. We can conclude treatment means of type 1 and type 2, treatment means of type 2 and type 3 has a significant differce on 0.01 significent level. There is not significant differce between type 1 and type 3 on 0.01 significent level.

```{r echo=T, collapse=TRUE}
TukeyHSD(aov(model_circuit), ordered = TRUE,conf.level=0.99)
pairw.anova(table_circuit$Respone,as.factor(table_circuit$Type), conf.level = 0.99, method = "tukey", MSE = NULL, df.err = NULL, control = NULL)
```

 ---
 
  (d) Construct a set of orthogonal contrasts, assuming that at the outset of the experiment you suspected the response time of circuit type 2 to be different from the other two.

Construct a set of contrasts $\sum_{i=1}^3c_i=1-2+1=0,\sum_{i=1}^3d_i=1+0-1=0$

$$\begin{cases}\mu_1-2\mu_2+\mu_3=0\\\mu_1-\mu_3=0\end{cases}$$

For $\sum_{i=1}^3c_id_i=1*1+(-2)*0+1*(-1)=0$, they are orthogonal contrasts.

$$C=\sum_{i=1}^ac_i\bar y_{i.}=\bar y_{1.}-2\bar y_{2.}+\bar y_{3.}=10.8-2(22.2)+8.4=-25.2$$

$$H_{C0}:\mu_1-2\mu_2+\mu_3=0\quad H_{C1}:\mu_1-2\mu_2+\mu_3\neq0$$

$$F_{1,12}=\frac{SS_C}{MS_E}=\frac{C^2}{\frac{MS_E}n\sum_{i=1}^ac_i^2}=\frac{(-25.2)^2}{\frac{16.9}56}=31.31$$

For the first contrast, the P-value of F-test ($0.0001169669$) is small enough to reject $H_{C0}$, We can conclude treatment means of type 2 has a significant differenence with treatment means of type 1 and type 3 on 0.01 significent level.

$$D=\sum_{i=1}^ac_i\bar y_{i.}=\bar y_{1.}-\bar y_{3.}=10.8-8.4=2.4$$

$$H_{D0}:\mu_1-\mu_3=0\quad H_{D1}:\mu_1-\mu_3\neq0$$

$$F_{1,12}=\frac{SS_C}{MS_E}=\frac{C^2}{\frac{MS_E}n\sum_{i=1}^ac_i^2}=\frac{2.4^2}{\frac{16.9}52}=0.852071$$

For the second contrast, the P-value of F-test ($0.374155$) is large and fail to reject $H_{D0}$, We can conclude there is not a significant differce between treatment means of type 1 and type 3 on 0.01 significent level.

```{r, collapse=TRUE}
HSD.test(model_circuit,"Type", group=TRUE,console=TRUE)

(25.2^2*5)/(16.9*6)
(2.4^2*5)/(16.9*2)

qf(0.95, 1, 12, lower.tail = TRUE, log.p = FALSE)
1-pf(31.31, 1, 12, lower.tail = TRUE, log.p = FALSE)
1-pf(0.852071, 1, 12, lower.tail = TRUE, log.p = FALSE)
```

 ---

  (e) If you were the design engineer and you wished to minimize the response time, which circuit type would you select?

Type 1 and type 3 have lower response time ($\mu_1=10.8,\mu_3=8.4$). We had proved the mean of response time for type 1 and type 3 have not significant different on response time on 0.01 significant level. The engineer could choose either one.

 ---
 
  (f) Analyze the residuals from this experiment. Are the basic analysis of variance assumptions satisfied?

 - The histogram doesn't show a quite normality of of residuals.
 - The QQ plot of residuals shows some  violaion of normality of residuals. The respnse variable may need to be transformed.
 - The residuals versus fitted values and studentized residuals show no violation of zero mean and constant variance assumptions.
 - There are some outlier and leverage data points. 
 - Compared with the Bartlett‚Äôs test , the Levene test is less sensitive to departures from normality. The result shows variances across samples may be equal. 
 
 Therefore, we should check the outlier points and try to do some transfoation. 

```{r  echo=T, collapse=TRUE}
ols_plot_diagnostics(lm(Respone~Type, data=table_circuit))
bartlett.test(Respone~Type, data=table_circuit)
leveneTest(model_circuit)
```
  

### Problem 6: (3.41/9 and 3.49/57)

  (3.41/9) Refer to Problem 5. If we wish to detect a maximum difference in mean response times of 10 milliseconds with a probability of at least 0.90, what sample size should be used? How would you obtain a preliminary estimate of $œÉ^2$?

For $a=3, D=10$, the preliminary estimate $\hat\sigma^2=MS_E=16.9$.

$$Œ¶^2=\frac{nD^2}{2aœÉ^2}=\frac{n(10)^2}{2(3)(16.9)}=0.986n$$

Let $Œ±=0.05$, accept probability $P=0.1,\nu_1=a‚àí1=2$

The results show that when $n=5$, the power is 0.86476, when $n=6$, the power is 0.9345867. Thus, the sample size should be $n\ge6, N\ge18$.

```{r echo=T, collapse=TRUE}
library(pwr2)
pwr.1way(k=3, n=5, alpha=0.05, f=NULL, delta=10, sigma=sqrt(16.9))
pwr.1way(k=3, n=6, alpha=0.05, f=NULL, delta=10, sigma=sqrt(16.9))
```

 ---

  (3.49/57) Consider the data shown in Problem 5.

  (a) Write out the least squares normal equations for this problem and solve them for $\hat\mu$ and $\hat\tau_i$, using the usual constraint ($\sum^3_{i=1} \hat\tau_i=0$). Estimate $\tau_1-\tau_2$

For $\sum^3_{i=1} \hat\tau_i=0$,

$$\begin{cases}\hat\tau_1+\hat\tau_2+\hat\tau_3=0\\\hat\mu+\hat\tau_1=\bar y_{1.}=10.8\\\hat\mu+\hat\tau_2=\bar y_{2.}=22.2\\\hat\mu+\hat\tau_3=\bar y_{3.}=8.4\end{cases}\implies
\begin{cases}\hat\mu=13.80\\\hat\tau_1=-3.00\\\hat\tau_2=8.40\\\hat\tau_3=-5.40\end{cases}\implies
\hat\tau_1-\hat\tau_2=-11.40$$

 ---

  (b) Solve the equations in (a) using the constraint $\hat\tau_3=0$. Are the estimator $\hat\tau_i$ and $\hat\mu$ the same as you found in (a)? Why? Now estimate $\tau_1-\tau_2$ and compare your answer with that for (a). What statement can you make about estimating contrasts in the $\tau_i$

$$\begin{cases}\hat\tau_3=0\\\hat\mu+\hat\tau_1=\bar y_{1.}=10.8\\\hat\mu+\hat\tau_2=\bar y_{2.}=22.2\\\hat\mu+\hat\tau_3=\bar y_{3.}=8.4\end{cases}\implies
\begin{cases}\hat\mu=8.40\\\hat\tau_1=2.40\\\hat\tau_2=13.8\\\hat\tau_3=0\end{cases}\implies
\hat\tau_1-\hat\tau_2=-11.40$$

The result shows the value of $\hatœÑ_1-\hatœÑ_2$ is irrelevant with the value of $\hatœÑ_3$. The contrasts are estimable.

 ---
 
  (c) Estimate $\mu+\tau_1$, $2\tau_1-\tau_2-\tau_3$, and $\mu+\tau_1+\tau_2$ using the two solutions to the normal equations. Compare the results obtained in each case.
  
|     Contrast          |Estimated from Part (a)|Estimated from Part (b)|
|          ----         |           ---         |            ---        |
|$\mu+\tau_1$           | 10.80   |10.80  |
|$2\tau_1-\tau_2-\tau_3$| -9.00   |-9.00  |
|$\mu+\tau_1+\tau_2$    | 19.20   |24.60  |

The result shows that $\tau_1$ is independent with $\tau_3$, $\tau_1$ is independent with $\tau_2$ and $\tau_3$, while $\tau_1$ and $\tau_2$ is dependent with $\tau_3$.


  
```{r eval=FALSE, include=FALSE}
lsdCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
bonfCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
tukeyCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
scheffeCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, MSE = NULL, df.err = NULL)
dunnettCI(table_battery$Life,as.factor(table_battery$Brand), conf.level = 0.99, control = "Brand2")
scheffe.cont(table_battery$Life,as.factor(table_battery$Brand), lvl = c("Brand2", "Brand3"), conf.level = 0.99, MSE = NULL, df.err = NULL)

https://www.statmethods.net/stats/power.html

(LSD.test(model_circuit, trt="Type",alpha = 0.05))

power.anova.test(groups = 3, n =NULL,between.var =var(c(10.8,22.2,8.4)), within.var = 16.9, sig.level = 0.01, power = 0.9)

power.anova.test(groups = 3, n =NULL,between.var =var(table_circuit$Respone)/2within.var = 16.9, sig.level = 0.05,1power = 0.9)

power.anova.test(groups = 3, n =NULL,between.var =(var(table_circuit$Respone)-16.9)/2within.var = 16.9, sig.level = 0.05,1power = 0.9)

# pwr  type = c("two.sample", "one.sample", "paired"),alternative = c("two.sided", "less", "greater"))
pwr.t.test(n =5,d =10, sig.level =0.05,1power =, 0.9type ="one.sample", alternative ="two.sided")
# biotools
# samplesize(x, fun, sizes = NULL, lcl = NULL, ucl = NULL,nboot = 200, conf.level = 0.95, nrep = 500, graph = TRUE, ...)


ggplot(table_battery, aes(x = Brand, y = Life)) + geom_boxplot()

prop.table(xtabs(~Life+Brand,data=table_battery,subset=!is.na(Brand), drop.unused.levels=T), 2)

chisq.test(xtabs(~Brand+Life,data=table_battery,subset=!is.na(Life)))

GKgamma(xtabs(~Respone + Type,data=table_circuit,subset=!is.na(Type), drop.unused.levels=T))
#running Leven's Equality of Variance
leveneTest(Life ~ Brand, data = table_battery,center = "mean")
```






## HW2

### Problem 1 (4.9-29)

Three different washing solutions are being compared to study their effectiveness in retarding bacteria growth in 5-gallon milk containers. The analysis is done in a laboratory, and only three trials can be run on any day. Because days could represent a potential source of variability, the experimenter decides to use a randomized block design. Observations are taken for four days, and the data are shown here. Analyze the data from this experiment (use $\alpha = 0.05$) and draw conclusions.

```{r include=FALSE}
library(readxl)
library(tidyverse)
require(ggplot2)
library(agricolae) #LSD.test
library(asbio) #pairw.anova
```

```{r echo=FALSE, out.width='45%'}
table_wash <- read_xlsx("Exercise4_4.xlsx")
table_wash$Days <- as.factor(table_wash$Days)
ggplot(table_wash,aes(Solution,Response,lty=Days))+geom_line()+theme(legend.position = c(0.9, 0.85))
table_wash$Solution <- as.factor(table_wash$Solution)
glimpse(table_wash)
```

 - The overall results

The ANOVA for RCBD shows that both treatments of solutions and blocks of days have significant effects on the average values of bacteria growth at 0.05 significant level. The P-value for treatment is 0.000323 while it for days is 0.000192.

The results of estimating regression coefficients show the P-value of Solution1 and Days1(0.000291), Solution3(0.000359), Days4(0.0000627) are small enough, while the P-value of Solution2(0.320563), Days2(0.067976), and Days3(0.790493) are larger than 0.05.  The coefficients of Solution1, Days1, Solution3, Days4 are significant at 0.05 significant level. The rest are not.

 - Comparing the treatment effects
 
The plot of LSD test shows the average response value for Solution3 has a different interval with that for other solutions. The Tukey test also shows the average response values for Solution3 are significant different with Solution1(P-value=0.0008758) and Solution2(P-value=0.0004067) at 0.05 significant level, while there is not significant difference between the average response values for Solution1 and Solution2(P-value=0.5577862) at 0.05 significant level.

However, the results by another code "pairw.anova" didn't reveal significant difference among the three solutions in both LSD and Tukey methods. I'm still looking for the reason.
 
 - Comparing the block effects
 
The plot of LSD test shows the average response value for Days4 has a different interval with that for other Days. The Tukey test also shows the average response values for Days4 are significant different with Days1(P-value=0.0002622,), Days2(P-value=0.0010843) and Days3(P-value=0.0003081) at 0.05 significant level, while there is not significant difference between the average response values for Days1, Days2, and Days3(P-value: 1-2=0.2193500, 1-3=0.9917442, 2-3=0.3037891) at 0.05 significant level.

Using the code "pairw.anova", The LSD method gave the same result with "TukeyHSD" but the Tukey method didn't find significant difference among the four days at 0.05 significant level. I'm still looking for the reason.

 - Model adequacy Checking

There is a little bit of violation of normality in histogram plot. The QQ plot has a little bit of heavier left tail. The residuals versus fitted values shows a slight curved plot. Studentized residuals show some violation of zero mean and constant variance assumptions. This could mean that other regressor variables are needed in the model. Transformations on the regressor or the response variable may also be helpful. There is not serious outlier and leverage data points. The residuals versus observation number shows the residuals are independent with each other.

 - The initial conculsion

Based on the experimental data, we found Solution3 has a stronger effectiveness in retarding bacteria growth in 5-gallon milk containers. Meanwhile, the data in Days4 are significant different with others. Next step, We suggest to investigate the factors affecting the data, refine the design, and repeat the experiment.

```{r echo=T, collapse=F, out.width='45%'}
model_wash <- aov(Response~Solution+Days, data=table_wash)
summary(model_wash)
summary.lm(model_wash)
```
```{r echo=T, collapse=T, out.width='30%',fig.show='hold'}
# Plot for solutions and days
plot(LSD.test(model_wash, trt="Solution",alpha = 0.05,p.adj="bonferroni"),main="Ranges of response grouped by solutions")
plot(LSD.test(aov(Response~Days+Solution, data=table_wash), trt="Days",alpha = 0.05,p.adj="bonferroni"),main="Ranges of response grouped by days")
```
```{r echo=T, collapse=F, out.width='45%'}
# Comparison of solutions
TukeyHSD(model_wash,conf.level = 0.95)
pairw.anova(table_wash$Response,table_wash$Solution, conf.level = 0.95, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
pairw.anova(table_wash$Response,table_wash$Solution, conf.level = 0.95, method = "tukey", MSE = NULL, df.err = NULL, control = NULL)
# Comparison of days
pairw.anova(table_wash$Response,table_wash$Days, conf.level = 0.95, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
pairw.anova(table_wash$Response,table_wash$Days, conf.level = 0.95, method = "tukey", MSE = NULL, df.err = NULL, control = NULL)
```

```{r echo=T,collapse=TRUE, out.width='20%',fig.show='hold'}
ols_test_normality(lm(Response~Solution+Days, data=table_wash))
ols_plot_resid_hist(lm(Response~Solution+Days, data=table_wash))
plot(model_wash)
```
 ---

4.29 Consider the randomized complete block design in Problem 4.9. Assume that the days are random. Estimate the block variance component.

$$\hat\sigma_{Blk}^2=\frac{MS_{Blk}-MS_{E}}{a}=\frac{369-8.6}{3}=120.1333$$

```{r echo=T, collapse=TRUE}
summary(aov(Response~Solution+Error(Days), data=table_wash))
```

### Problem 2 (4.13)

A consumer products company relies on direct mail marketing pieces as a major component of its advertising campaigns. The company has three different designs for a new brochure and wants to evaluate their effectiveness, as there are substantial differences in costs between the three designs. The company decides to test the three designs by mailing 5000 samples of each to potential customers in four different regions of the country. Since there are known regional differences in the customer base, regions are considered as blocks. The number of responses to each mailing is as follows.

```{r echo=FALSE, out.width='45%'}
table_mail <- read_xlsx("Exercise4_8.xlsx")
table_mail$Region <- as.factor(table_mail$Region)
table_mail %>% ggplot(aes(Design,Response))+geom_line(aes(Design,Response,lty=Region))+theme(legend.position = c(0.85, 0.85))
table_mail$Design <- as.factor(table_mail$Design)
glimpse(table_mail)
```

(a) Analyze the data from this experiment.

The ANOVA for RCBD shows that both treatments of Designs and blocks of Regions have significant effects on the average values of bacteria growth at 0.05 significant level. The P-value for treatment is 0.00018 while it for blocks is 0.00208.

The results of estimating regression coefficients show the P-value of Design1 and RegionNE(0.0000201), Design2(0.000173), RegionNW(0.007661), RegionSW(0.003636) are small enough, while the P-value of Design3(0.320563) and RegionSE(0.166470) are larger than 0.05. The coefficients of Design1, RegionNE, Design2, RegionNW, RegionSW are significant at 0.05 significant level. The rest are not.

```{r echo=T, collapse=F}
model_mail <- aov(Response~Design+Region, data=table_mail)
summary(model_mail)
summary.lm(model_mail)
```

 ---

(b) Use the Fisher LSD method to make comparisons among the three designs to determine specifically which designs differ in the mean response rate.

The plot of LSD test shows the average response value for Design2 has a different interval with that for other designs. The LSD test using "pairw.anova" also shows the average response values for Design2 are significant different with Design1(P-value=0.01108) and Design3(P-value=0.00673) at 0.05 significant level, while there is not significant difference between the average response value for Design1 and Design3(P-value=0.76098) at 0.05 significant level.

```{r echo=T,collapse=T, out.width='24%'}
plot(LSD.test(model_mail, trt="Design",alpha = 0.05,p.adj="bonferroni"))
pairw.anova(table_mail$Response,table_mail$Design, conf.level = 0.95, method = "lsd", MSE = NULL, df.err = NULL, control = NULL)
``` 

 ---
 
(c) Analyze the residuals from this experiment.

 - Model adequacy Checking
 
The normality test and histogram of residuals show a violation of normality. The QQ plot of residuals shows a sharp upward and downward curve at both extremes, indicating that the tails of this distribution are too light for it to be considered normal. The variance of $y$ seems be proportional to $x$ which implies $y$ may be a Poisson random variable in a simple linear regression model and its variance is equal to the mean. We could regress $y‚Ä≤=\sqrt y$ against $x$ since the variance of the square root of a Poisson random variable is independent of the mean. 

 - After transformation

The violation of normality in QQ plot and histogram are not obvious as before. The residuals versus fitted values and studentized residuals don't show strong violation of zero mean and constant variance assumptions. There is not serious outlier and leverage data points. The residuals versus observation number shows the residuals are independent with each other.

The ANOVA for RCBD shows simular results. Moreover, MSE after transformation(0.502) is much smaller than before(905). The P-value is smaller too (0.000106 for treatment, 0.000986 for blocks).

The results of estimating regression coefficients are simular. The P-value of Design1 and RegionNE(0.000006), Design2(0.000105), RegionNW(0.004490), RegionSW(0.002479) are smaller, while the P-value of Design3(0.376944) and RegionSE(0.076721) are still larger than 0.05.  The coefficients of Design1, RegionNE, Design2, RegionNW, RegionSW are significant at 0.05 significant level. The rest are not.

The results of paried test are same. The plot of LSD test shows the average response value for Design2 has a different interval with that for other designs. The LSD test using "pairw.anova" also shows the average response values for Design2 are significant different with Design1(P-value=0.01333) and Design3(P-value=0.00792) at 0.05 significant level, while there is not significant difference between Design1 and Design3(P-value=0.7525) at 0.05 significant level.

```{r echo=T,collapse=TRUE, out.width='20%',fig.show='hold'}
ols_test_normality(lm(Response~Design+Region, data=table_mail))
ols_test_normality(lm(sqrt(Response)~Design+Region, data=table_mail))
ols_plot_resid_hist(lm(Response~Design+Region, data=table_mail))
plot(model_mail)
ols_plot_resid_hist(lm(sqrt(Response)~Design+Region, data=table_mail))
plot(lm(sqrt(Response)~Design+Region, data=table_mail))
```

```{r echo=T, collapse=F}
model_mail_sqrt <- aov(sqrt(Response)~Design+Region, data=table_mail)
summary(model_mail_sqrt)
summary.lm(model_mail_sqrt)
```

```{r echo=T,collapse=T, out.width='24%'}
# plot(LSD.test(model_mail_sqrt, trt="Design",alpha = 0.05,p.adj="bonferroni"))
pairw.anova(sqrt(table_mail$Response),table_mail$Design, conf.level = 0.95, method = "lsd")
``` 

### Problem 3

$y_{ijkl}=Œº+œÑ_i+Œ±_{j(l)}+Œ≤_k+Œ¥_l+Œµ_{ijkl}$ for $i,j,k =1,‚Ä¶,p$ ; $l=1,‚Ä¶,n$; $Œµ_{ijkl}\sim iid N(0,œÉ^2 )$
Derive the parameter estimates from the least squares method using the constrains
$\sum_i\hat\tau_i=0;\ \sum_j\hat\alpha_{j(l)}=0;\ \sum_l\hat\alpha_{j(l)}=0;\ \sum_k\hat\beta_k=0;\ \sum_l\hat\delta_l=0$

$$SSE=\sum_i^p\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-Œº-œÑ_i-Œ±_{j(l)}-Œ≤_k-Œ¥_l)^2$$

Derive

$\left.\frac{\partial SSE}{\partial Œº}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=2\sum_i^p\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial œÑ_i}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=2\sum_j^p\sum_k^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ±_{j(l)}}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=2\sum_i^p\sum_k^p(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ≤_k}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=2\sum_i^p\sum_j^p\sum_l^n(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$\left.\frac{\partial SSE}{\partial Œ¥_l}\right|_{\hatŒº,\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l}=2\sum_i^p\sum_j^p\sum_k^p(y_{ijkl}-\hatŒº-\hatœÑ_i-\hatŒ±_{j(l)}-\hatŒ≤_k-\hatŒ¥_l)(-1)=0$

$$\left\{\begin{array}{l} 
y_{....}=np^2\hatŒº+np\sum_i^p\hatœÑ_i+p^2\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\sum_k^p\hatŒ≤_k+p^2\sum_l^n\hatŒ¥_l\\
y_{i...}=np\hatŒº+np\hatœÑ_i+p\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\sum_k^p\hatŒ≤_k+p^2\sum_l^n\hatŒ¥_l \\
y_{.j.l}=p\hatŒº+p\sum_i^p\hatœÑ_i+p\hatŒ±_{j(l)}+p\sum_k^p\hatŒ≤_k+p\hatŒ¥_l \\
y_{..k.}=np\hatŒº+n\sum_i^p\hatœÑ_i+\sum_j^p\sum_l^n\hatŒ±_{j(l)}+np\hatŒ≤_k+p\sum_l^n\hatŒ¥_l \\
y_{...l}=p^2\hatŒº+p^2\sum_i^p\hatœÑ_i+p^2\sum_j^p\hatŒ±_{j(l)}+p^2\sum_k^p\hatŒ≤_k+p^2\hatŒ¥_l\\
\end{array}\right.$$

For $\sum_i\hat\tau_i=0;\ \sum_j\hat\alpha_{j(l)}=0;\ \sum_l\hat\alpha_{j(l)}=0;\ \sum_k\hat\beta_k=0;\ \sum_l\hat\delta_l=0$,

and for $\hatŒº$ is constant, $\hatœÑ_i,\hatŒ±_{j(l)},\hatŒ≤_k,\hatŒ¥_l$ are constants for summations on other parameters,

and for $y_{....}=np^2 \bar y_{....}=np\bar y_{i...}=np\bar y_{.j..}=np\bar y_{..k.}=p^2\bar y_{...l}$,

and for $y_{i...}=np\bar y_{i...}$, $y_{.j.l}=p\bar y_{.j.l}$, $y_{..k.}=np\bar y_{..k.}$, $y_{...l}=p^2\bar y_{...l}$, then

$$\left\{\begin{array}{l}\ np^2\bar y_{....}=np^2\hatŒº+0+0+0+0\\
np\bar y_{i...}=np\hatŒº+np\hatœÑ_i+0+0+0 \\
p\bar y_{.j.l}=p\hatŒº+0+p\hatŒ±_{j(l)}+0+p\hatŒ¥_l \\
np\bar y_{..k.}=np\hatŒº+0+0+np\hatŒ≤_k+0 \\
p^2\bar y_{...l}=p^2\hatŒº+0+0+0+p^2\hatŒ¥_l \end{array}\right.
\implies
\left\{\begin{array}{l}\ \hatŒº=\bar y_{....}\\
\hatœÑ_i=\bar y_{i...}-\bar y_{....}\\
\hatŒ±_{j(l)}=\bar y_{.j.l}-\bar y_{...l} \\
\hatŒ≤_k=\bar y_{..k.}-\bar y_{....} \\
\hatŒ¥_l=\bar y_{...l}-\bar y_{....} \end{array}\right.$$





## HW3
### Problem 1: RCBD with one missing observation

Winter road treatments to clear snow and ice can lead to cracking in the pavement. An experiment was conducted comparing 4 treatments: sodium chloride (A), calcium chloride (B), a proprietary organic compound (C), and sand (D).
Traffic level was used as a blocking factor and a randomized complete block experiment was conducted.
One observation is missing, because the spreader in that district was no operating properly.
The response is new cracks per mile of treated roadway.

```{r include=FALSE}
library(readxl)
library(tidyverse)
require(ggplot2)
library(agricolae) #LSD.test
library(asbio) #pairw.anova
library(ggrepel)
library(olsrr)
```

```{r echo=FALSE}
table_miss0 <- as.table(matrix(c(0,32,27,36,38,40,43,33,40,63,14,27),ncol=4,byrow=TRUE))
colnames(table_miss0) <- c("A","B","C","D")
rownames(table_miss0) <- c("Block1","Block2","Block3")
```

 (a) Compute the mean response for each treatment and report here. You may copy and paste output table from your software here.

```{r echo=FALSE}
margin.table(table_miss0,2)/c(2,3,3,3)
```

 (b) Compute the mean response for each block and report here. You may copy and paste output table from your software here.

```{r echo=FALSE}
margin.table(table_miss0,1)/c(3,4,4)
```

 (c) Compute the overall mean response and report here.

```{r echo=FALSE}
margin.table(table_miss0)/11
```

 (d) Use the approximation method learned to estimate one missing value in Chapter 4 and estimate the value of the missing observation. Report it here. Show any computation here.
 
$$\hat x=\frac{ay'_{1.}+by'_{.1}-y'_{..}}{(a-1)(b-1)}=\frac{3*95+4*78-393}{(3-1)(4-1)}=34$$

```{r include=FALSE}
addmargins(table_miss0, margin =c(1,2), FUN = mean, quiet =T)
(3*margin.table(table_miss0,1)[1]+4*margin.table(table_miss0,2)[1]-margin.table(table_miss0))/6
```

 (e) Include that estimated missing value in the given data file appropriately and obtain an ANOVA table for this design. Report ONLY the ANOVA table here.

```{r include=FALSE}
table_complete <- tibble(y=c(34,32,27,36,38,40,43,33,40,63,14,27),trt=rep(c("A","B","C","D"),3),blk=c(rep("Block1",4),rep("Block2",4),(rep("Block3",4))))
glimpse(table_complete)
```
```{r echo=FALSE}
summary(aov(y~trt+blk,data=table_complete))
```

 (f) Now, adjust the degrees of freedom for error in the ANOVA table for one missing value. (See the Chapter 4 note). And use adjusted df of error to compute the correct p-value for testing significance of treatments.

$$F_{adj}=\frac{MS_{Trt}}{SSE/dfE_{adj}}=\frac{162.08}{921.5/(6-1)}=0.8794357$$

```{r echo=FALSE}
1-pf(0.8794357, 3, 5, lower.tail = TRUE, log.p = FALSE)
```

Write your conclusion based on your p value and report the p value here.

Both of the ANOVA for RCBD of original data and adjusted data show that 4 treatments have not significant effects on the average values of cracking in the pavement at 0.05 significant level. The P-value for treatment with missing observation is 0.435 while adjusted P-value is 0.5111 and 0.5141 in two methods. We find the P-value in the Apporximation method is smaller than the Exact method because the first method is that minimizing only $SSE$ may increasing $SS_{Trt}$ and hence it is more likely to conclude effects are significant when they are actually not.

 (g) The second method of analyzing data with missing value is extra (partial) sum of squares method. Fit a multiple linear regression model and obtain the extra (partial) sum of squares for treatments. If you forgot how to do this, look at regression analysis labs and notes.
 
Test whether treatment is a significant factor using extra (partial) SS F test. Report the p-value from extra (partial) SS F test.
	
$$F_0=\frac{\frac{SSE_{reduced}-SSE_{full}}{dfE_{reduced}-dfE_{full}}}{MSE_{full}}=\frac{1403.7-921.5}{184.30*(a-1)}=0.8721288$$

```{r echo=FALSE}
1-pf(0.8721288, 3, 5, lower.tail = TRUE, log.p = FALSE)
```

 (h) Report the complete code along with output here.

```{r collapse=T}
# (e)
table_complete <- tibble(y=c(34,32,27,36,38,40,43,33,40,63,14,27),trt=rep(c("A","B","C","D"),3),blk=c(rep("Block1",4),rep("Block2",4),(rep("Block3",4))))
summary(aov(y~trt+blk,data=table_complete))
# (f)
1-pf(0.8794357, 3, 5, lower.tail = TRUE, log.p = FALSE)
# (g)
table_miss1 <- tibble(y=c(NA,32,27,36,38,40,43,33,40,63,14,27),trt=rep(c("A","B","C","D"),3),blk=c(rep("Block1",4),rep("Block2",4),(rep("Block3",4))))
summary(aov(y~trt+blk,data=table_miss1))
summary(aov(y~blk,data=table_miss1))
1-pf(0.8721288, 3, 5, lower.tail = TRUE, log.p = FALSE)
```


### Problem 2 (4.22/27)

The effect of five different ingredients (A, B, C, D, E) on the reaction time of a chemical process is being studied.
Each batch of new material is only large enough to permit five runs to be made. Furthermore, each run requires approximately 3/2 hours, so only five runs can be made in one day. The experimenter decides to run the experiment as a Latin square so that day and batch effects may be systematically controlled. She obtains the data that follow. Analyze the data from this experiment (use ùõº = 0.05) and draw conclusions.

 (a) Create plot of data and report it here.

```{r echo=FALSE, warning=FALSE, out.width='30%'}
table_ingredients <- read_xlsx("Exercise4_22.xlsx")
table_ingredients$Day <- as.factor(table_ingredients$Day)
table_ingredients$Batch <- as.factor(table_ingredients$Batch)
table_ingredients$Trt <- as.factor(table_ingredients$Trt)
glimpse(table_ingredients)
ggplot(table_ingredients,aes(Trt,y))+geom_point(aes(size=Day,shape=Batch))+theme_light()
```

 (b) Obtain the ANOVA table for this study and report it here.

```{r echo=FALSE}
model_ingredients <- aov(y~Trt+Day+Batch,data =table_ingredients)
summary(model_ingredients)
```

 (c) Test whether ingredients have a significant effect on the reaction time at Œ±=0.05. Report your conclusion and p value here.

The ANOVA for RCBD shows that ingredients have significant effects on the average values of reaction time at 0.05 significant level. The P-value of 0.000488 for treatment is small enough.

 (d) Test the significance of different batches of material at Œ±=0.05. Report your conclusion and p value here.

The reaults of TukeyLSD test show that there is not significant differents on the average values of reaction time among the batches at 0.05 significant level. The P-value of all the paired comparing among batches is large enough. All of the confident intervals include zero.

|   |diff      |lwr     |upr    |p adj
|---|---       |---     |---    |---
2-1|1.0|-2.564608|4.564608|0.8936609
3-1|0.6|-2.964608|4.164608|0.9816047
4-1|2.0|-1.564608|5.564608|0.4225127
5-1|-0.2|-3.764608|3.364608|0.9997349
3-2|-0.4|-3.964608|3.164608|0.9960012
4-2|1.0|-2.564608|4.564608|0.8936609
5-2|-1.2|-4.764608|2.364608|0.8166339
4-3|1.4|-2.164608|4.964608|0.7232162
5-3|-0.8|-4.364608|2.764608|0.9489243
5-4|-2.2|-5.764608|1.364608|0.3365811

 (e) Provide all the necessary residual plots here and use them to explain whether assumptions about the model for this design are valid or not.

The plot of studentized residual versus predicted (fitted) value shows that except few outliers, the residuals are evenly distributed about zero at each prededict value (zero mean) and vertical deviations of residuals from zero are about same for each predicted value (constant variance).

The plots of studentized residual versus factor levels didn't show obvious violation of zero mean and constant variance.

The QQ plot shows that some data points are not on the line and flattening at the extremes, which is a pattern typical of samples from a distribution with heavier tails than the normal.

```{r echo=FALSE, warning=FALSE, out.width='20%'}
table_ingredients$student_resid <- rstudent(model_ingredients)
ols_plot_resid_stud_fit(lm(y~Trt+Day+Batch,data =table_ingredients))
ggplot(table_ingredients,aes(Trt,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ggplot(table_ingredients,aes(Day,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ggplot(table_ingredients,aes(Batch,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ols_plot_resid_qq(lm(y~Trt+Day+Batch,data =table_ingredients))
```

 (f) Report the complete code along with output here.

```{r,collapse=T, out.width='20%'}
# (a)
table_ingredients <- read_xlsx("Exercise4_22.xlsx")
table_ingredients$Day <- as.factor(table_ingredients$Day)
table_ingredients$Batch <- as.factor(table_ingredients$Batch)
table_ingredients$Trt <- as.factor(table_ingredients$Trt)
glimpse(table_ingredients)
ggplot(table_ingredients,aes(Trt,y))+geom_point(aes(size=Day,shape=Batch))+theme_light()
```

```{r,collapse=T}
# (b)
model_ingredients <- aov(y~Trt+Day+Batch,data =table_ingredients)
summary(model_ingredients)
```

```{r,collapse=T}
# (d)
TukeyHSD(model_ingredients,conf.level = 0.95)
```

```{r,collapse=T, out.width='20%',fig.show='hold'}
# (e)
table_ingredients$student_resid <- rstudent(model_ingredients)
ols_plot_resid_stud_fit(lm(y~Trt+Day+Batch,data =table_ingredients))
ggplot(table_ingredients,aes(Trt,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ggplot(table_ingredients,aes(Day,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ggplot(table_ingredients,aes(Batch,student_resid))+geom_dotplot(binaxis = "y", stackdir = "center",binwidth=0.1,alpha=0.6)+theme_light()
ols_plot_resid_qq(lm(y~Trt+Day+Batch,data =table_ingredients))
```

### Problem 3: 

The data below are from a replicated Latin Square with 4 treatments: row blocks were reused, but column blocks were not.

```{r echo=FALSE, warning=FALSE, out.height='40%'}
table_LSD2 <- read_xlsx("Problem3.xlsx")
table_LSD2$Replicate <- as.factor(table_LSD2$Replicate)
table_LSD2$Row <- as.factor(table_LSD2$Row)
table_LSD2$Col <- as.factor(table_LSD2$Col)
table_LSD2$Trt <- as.factor(table_LSD2$Trt)
glimpse(table_LSD2)
ggplot(table_LSD2,aes(Col,y))+geom_point(aes(shape=Row,col=Replicate))+scale_colour_grey()+facet_grid(.~Trt)+theme_light()
```

```{r eval=FALSE, include=FALSE}
ggplot(table_LSD2,aes(Trt,y))+geom_point(data = transform(table_LSD2, Replicate= NULL),aes(shape=Row), alpha=0.2)+geom_point(aes(shape=Row)) +geom_text_repel(aes(label=Col),alpha=.6)+facet_wrap(~Replicate)+theme_light()
ggplot(table_LSD2,aes(Col,y))+geom_point(data = transform(table_LSD2, Replicate= NULL),aes(shape=Row), alpha=0.2)+geom_point(aes(shape=Row)) +geom_text_repel(aes(label=Trt),alpha=.6)+facet_wrap(~Replicate)+theme_light()

ggplot(table_LSD2,aes(Col,y))+geom_point(data = transform(table_LSD2, Replicate= NULL),aes(shape=Row), )+geom_point(data=filter(table_LSD2,Replicate==2), aes(shape=Row)) +facet_grid(.~Trt)+theme_light()

ggplot(table_LSD2,aes(Col,y))+geom_point(data = transform(filter(table_LSD2,Replicate==2), Replicate=NULL),aes(shape=Row), alpha=0.5,col="red")+geom_point(data = transform(table_LSD2, Replicate=NULL),aes(shape=Row),col="blue", alpha=1)+facet_grid(.~Trt)+theme_light()
```

 (a) Obtain the ANOVA table for this study and report it here.
 
```{r echo=FALSE}
model_LSD2 <- aov(y~Trt+Replicate/Row+Col,data =table_LSD2)
summary(model_LSD2)
```

 (b) Test whether treatments are statistically significant at Œ±=0.01. Report your conclusion and p value here.

The P-value of 0.000107 for treatment is samller than 0.01. The ANOVA for RCBD shows that the treatments have significant effects on the average values of resopnse variable at 0.01 significant level. 

### Problem 4:

The BIBD model with fixed effect is:$y_{ij}=Œº+œÑ_i+Œ≤_j+Œµ_{ij}$,for i=1,2,‚Ä¶,a$;$j=1,2,‚Ä¶,b; $N=kb=ra‚â†ab$.
$Œµ_{ij} ~ iidN(0,œÉ^2)$;$\sum_{i=1}^aœÑ_i=0$ and $\sum_{j=1}^bŒ≤_j =0$; $y_{i.}=\sum_jy_{ij}$,$\bar y_{.j}=\frac1k\sum_iy_{ij}=\frac{y_{.j}}k$.
Let $Q_i=(y_{i.}-\sum_{j=1}^bn_{ij}\bar y_{.j})$ where $n_{ij}=\begin{cases}1& \text{if } i^{th} \text{ treatment appears in } j^{th}\text{ block}\\0&\text{Otherwise}\end{cases}$
Prove  that $\sum_{i=1}^aQ_i=0$

$$\sum_{i=1}^aQ_i=\sum_{i=1}^a[y_{i.}-\sum_{j=1}^bn_{ij}\bar y_{.j}]=\sum_{i=1}^a\sum_{j=1}^b y_{ij}-\sum_{j=1}^b(\bar y_{.j}\sum_{i=1}^an_{ij})=y_{..}-\sum_{j=1}^b(\frac{y_{.j}}kk)=y_{..}-y_{..}=0$$
 
### Problem 5:

Consider the two-factor factorial model with fixed effects:
$y_{ijk}=Œº+œÑ_i+Œ≤_j+(œÑŒ≤)_{ij}+Œµ_{ijk}$, for $i=1,2,‚Ä¶,a$; $j=1,2,‚Ä¶,b$; $k=1,2,‚Ä¶,n$
$Œµ_{ijk}\sim iid N(0,œÉ^2)$;$\sum_{i=1}^aœÑ_i=0$ and $\sum_{j=1}^bŒ≤_j=0$; $\sum_{i=1}^a(œÑŒ≤)_{ij}=0$   and $\sum_{j=1}^b(œÑŒ≤)_{ij}=0$
$\bar y_{i..}=\frac1{bn}\sum_{j=1}^b\sum_{k=1}^ny_{ijk}$; $y_{...}=\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^ny_{ijk}$; $\bar y_{...}=\frac{y_{...}}N$. Compute the $E(SS_A)$ where  $SS_A=bn\sum_{i=1}^a(\bar y_{i..}-\bar y_{...})^2$ 

For $i=1,2,‚Ä¶,a$; $j=1,2,‚Ä¶,b$; $k=1,2,‚Ä¶,n$,

for $\sum_{i=1}^aœÑ_i=0$, $\sum_{j=1}^bŒ≤_j=0$; $\sum_{i=1}^a(œÑŒ≤)_{ij}=0$, and $\sum_{j=1}^b(œÑŒ≤)_{ij}=0$,

$\bar y_{i..}-\bar y_{...}=\frac1{bn}\sum\limits_{j=1}^b\sum\limits_{k=1}^n(Œº+œÑ_i+Œ≤_j+(œÑŒ≤)_{ij}+Œµ_{ijk})-\frac1{abn}\sum\limits_{i=1}^a\sum\limits_{j=1}^b\sum\limits_{k=1}^n(Œº+œÑ_i+Œ≤_j+(œÑŒ≤)_{ij}+Œµ_{ijk})$

$=Œº+œÑ_i+\frac1{b}\sum\limits_{j=1}^bŒ≤_j+\frac1{b}\sum\limits_{j=1}^b(œÑŒ≤)_{ij}+\frac1{bn}\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}-Œº-\frac1{a}\sum\limits_{i=1}^aœÑ_i-\frac1{b}\sum\limits_{j=1}^bŒ≤_j-\frac1{ab}\sum\limits_{i=1}^a\sum\limits_{j=1}^b(œÑŒ≤)_{ij}-\frac1{abn}\sum\limits_{i=1}^a\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}$ 

$=œÑ_i+\frac1{bn}\sum_{j=1}^b\sum_{k=1}^nŒµ_{ijk}-\frac1{abn}\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^nŒµ_{ijk}=œÑ_i+\bar Œµ_{i..}-\bar Œµ_{ijk}$

 ---

For $œÑ_i$ is a constant of the $i^th$ treatment, $E{œÑ_i}=œÑ_i,V{œÑ_i}=0$,

$E[\bar y_{i..}-\bar y_{...}]=E[œÑ_i+\bar Œµ_{i..}-\bar Œµ_{ijk}]=œÑ_i+\frac1{bn}\sum_{j=1}^b\sum_{k=1}^nE[Œµ_{ijk}]-\frac1{abn}\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^nE[Œµ_{ijk}]=\tau_i$

 ---
 
For $œÑ_i$ and $Œµ_{ijk}$ are independent. $Cov\left[œÑ_i,\bar Œµ_{i..}\right]=Cov[œÑ_i,\bar Œµ_{...}]=0$,

for $Œµ_{ijk}\sim iidN(0,œÉ^2)$, $Cov\left[Œµ_{ijk},\sum_{i=1}^aŒµ_{ijk}\right]=\sigma^2$, $V[\bar y_{i..}-\bar y_{...}]=$

$$V[œÑ_i+\bar Œµ_{i..}-\bar Œµ_{ijk}]=V[\tau_i]+V[\frac1{bn}{\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}}]+V[\frac1{abn}{\sum\limits_{i=1}^a\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}}]+2Cov[œÑ_i,\bar Œµ_{i..}]-2Cov[œÑ_i,\bar Œµ_{...}]-2Cov[\bar Œµ_{i..},\bar Œµ_{...}]$$

$=0+\frac{œÉ^2}{bn}+\frac{œÉ^2}{abn}+0-0-2Cov\left[\frac1{bn}{\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}},\frac1{abn}{\sum\limits_{i=1}^a\sum\limits_{j=1}^b\sum\limits_{k=1}^nŒµ_{ijk}}\right]=\frac{œÉ^2}{bn}+\frac{œÉ^2}{abn}-\frac{2}{ab^2n^2}\sum\limits_{j=1}^b\sum\limits_{k=1}^nCov[Œµ_{ijk},\sum\limits_{i=1}^aŒµ_{ijk}]$

$=\frac{œÉ^2}{bn}+\frac{œÉ^2}{abn}-\frac{2}{ab^2n^2}\sum\limits_{j=1}^b\sum\limits_{k=1}^nœÉ^2=\frac{a-1}{abn}œÉ^2$

 ---

For $E[X^2]=E[X]^2+V[X]$, therefore,

$$E[SS_A]=bn\sum_{i=1}^aE[(\bar y_{i..}-\bar y_{...})^2]=bn\sum_{i=1}^a(E[\bar y_{i..}-\bar y_{...}]^2+V[\bar y_{i..}-\bar y_{...}])=bn\sum_{i=1}^a(\tau_i^2+\frac{a-1}{abn}œÉ^2)=bn\sum_{i=1}^aœÑ_i^2+(a-1)œÉ^2$$ 

