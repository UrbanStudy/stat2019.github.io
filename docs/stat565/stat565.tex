\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.2in,left=.2in,right=.2in,bottom=.2in} }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

%\begin{center}
%     \Large{\textbf{STAT564}} \\
%\end{center}

\section{simple linear regression}
\subsection{model}
response variable=parameters(coefficients).predictor+random error;
5 Assumption: linear relationship between x,y; $E(\varepsilon)=0,Var(\varepsilon)=\sigma^2$;homoscedasticity;$Cov(\varepsilon_i,\varepsilon_j)=0 for i\ne{j}$
$y_i=\beta_0+\beta_1x_i+\varepsilon_i$; $\varepsilon_i\sim^{iid}N(0,\sigma^2)\ i=1,2..n$;

\subsubsection{least square}
$SSE,S(\hat\beta_0,\hat\beta_1),\sum_{i=1}^n\hat\varepsilon_i^2=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=(Y-X\beta)'(Y-X\beta)=YY'-2Y'X\beta+\beta'X'X\beta$;

\begin{tabular}{ l|l }
$\frac{\partial SSE}{\partial\beta_0}=0=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-1)$   & $\frac{\partial{a'w}}{\partial{w}}=a$ \\
$\frac{\partial SSE}{\partial\beta_1}=0=2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)(-x_i)$ & $\frac{\partial{w'Aw}}{\partial{w}}=2Aw$ \\
\end{tabular}

\subsubsection{normal equation}
$\sum_{i=1}^ny_i=n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i\Rightarrow \bar y=\hat\beta_0+\hat\beta_1\bar x$;
$\sum_{i=1}^nx_iy_i=\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2; \mathbf{X'Y=X'X\beta}$

\subsubsection{corrected sum of suqares}
$S_{xx}=\sum_{i=1}^n(x_i-\bar x)^2=\sum_{i=1}^nx_i^2-n\bar x^2=\sum_{i=1}^nx_i(x_i-\bar x)=\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2/n$;
$S_{xy}=\sum_{i=1}^n(x_i-\bar x)y_i=\sum_{i=1}^nx_i(y_i-\bar y)=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)=\sum_{i=1}^nx_iy_i-(\sum_{i=1}^nx_i\sum_{i=1}^ny_i)/n$;
$S_{yy}=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ny_i^2-n\bar y^2$

\subsubsection{sum}
$\sum_{i=1}^n(y_i)=n\bar y$;
$\sum_{i=1}^n(y_i-\hat y)=\sum_{i=1}^ne_i=0$;
$\sum_{i=1}^ny_i=\sum_{i=1}^n\hat y$;
$\sum_{i=1}^n\hat ye_i=\sum_{i=1}^nx_ie_i=0$

\subsubsection{Ci}
$c_i=\frac{x_i-\bar x}{S_{xx}}$;
$\sum_{i=1}^nc_i^2=\frac{\sum_{i=1}^n(x_i-\bar x)^2}{S_{xx}^2}=\frac1{S_{xx}}$;
$\sum_{i=1}^nc_ix_i=\frac1{S_{xx}}\sum_{i=1}^nx_i(x_i-\bar x)=\frac{S_{xx}}{S_{xx}}=1$;
$\sum_{i=1}^nc_i=0=\sum_{i=1}^n\frac{x_i-\bar x}{S_{xx}}=\frac1{S_{xx}}\sum_{i=1}^n(x_i-\bar x)=\frac1{S_{xx}}(n\bar x-n\bar x)$

\subsection{estimation, regression coefficient}
$\hat{y_i}=\hat\beta_0+\hat\beta_1x_i=X\hat\beta=X(X'X)^{-1}X'Y=HY$
$\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=r \frac{SD_x}{SD_y}=\sum_{i=1}^nc_iy_i=(X'X)^{-1}X'Y$;
$\hat{\beta_0}=\bar y-\hat{\beta_1}\bar x=\frac1n{\sum_{i=1}^ny_i}-\bar x(\sum_{i=1}^n c_iy_i)=\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i$
$\hat{w}=rZ=\frac{\hat{y-\hat{y}}}{S_y}=r\frac{\hat{x-\hat{x}}}{S_x}$


\subsubsection{T, expected value}
a statistic T is an unbiased estimator of parameter if $E(\hat{\beta_1})=\beta_1=E[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_iE[\beta_0+\beta_1x_i+\varepsilon_i]=\beta_0\frac{\sum_{i-1}^n(x_i-\bar x)}{S_{xx}}+{\beta_1}\frac{\sum_{i-1}^n(x_i-\bar x)x_i}{S_{xx}}+\sum_{i=1}^nc_iE(\varepsilon_i)$;
$E(\hat{\beta_0})=\beta_0=E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\bar xE[\hat\beta_1]=E[\frac{\sum_{i-1}^ny_i}n]-\bar x\hat\beta_1=\frac1n\sum_{i=1}^nE[\beta_0+\beta_1x_i+\varepsilon_i]-\bar x\hat\beta_1=\frac{n\beta_0}n+\frac{\beta_1}n\sum_{i=1}^nx_i+\frac1n\sum_{i=1}^nE(\varepsilon_i)-\bar x\hat\beta_1$


\subsubsection{variance covariance}
Sample variance $S_x^2=\frac{S_{xx}}{n-1}$; $S_y^2=\frac{S_{yy}}{n-1}$;

As $n\rightarrow \infty,Var[\hat\beta_{0/1}]\rightarrow 0$;
$\hat\beta_{0/1}$ are consistant estimators for $\beta_{0/1}$

Because $Var(x+y)=Var(x)+Var(y)+2Cov(x,y)$, 

$Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ Therefore 
$Var(\hat{\beta_1})=\frac{\sigma^2}{S_{xx}}=Var[\sum_{i=1}^nc_iy_i]=\sum_{i=1}^nc_i^2Var[y_i]$;
$Var(\hat{\beta_0})=\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})=Var[\bar y-\hat\beta_1\bar x]=Var[\sum_{i=1}^n(\frac1n-\bar{x}c_i)y_i]=\sum_{i=1}^n(\frac1n-\bar{x}c_i)^2$;
$Var[ax,by]=a^2Var(x)+b^2Var(y)+2Cov(x,y)$;

$Cov(x,y)=\sigma_{xy}=E[(x-\bar x)(y-\bar y)]=\frac1n\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)$
$Cov(x,a)=0; Cov(x,x)=Var(x)\equiv\sigma^2(x)\equiv\sigma_x^2$
$Cov(x,y)=Cov(y,x); Cov(ax,by)=abCov(x,y)$
$Cov(x+a,y+b)=Cov(x,y)$
$Cov(a_1x_1+a_2x_2, b_1y_1+b_2y_2)=a_1b_1Cov(x_1,y_1)+a_1b_2Cov(x_1,y_2)+a_2b_1Cov(x_2,y_1)+a_2b_2Cov(x_2,y_2)$
$Cov(\hat\beta_0, \hatβ_1)=−\overline x\sigma^2 S_{xx}$
$Cov(\bar y,\hat\beta_1) = 0$

\subsubsection{correlation coefficient}
$|r|=\sqrt{R^2}=Cor(x,y)=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{Cov(x,y)}{\sqrt{Var{x}Var{y}}}$;
$R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$ indicates the  variability around $\bar y$ coefficient of determination: proportion of variation in response is explained by the fitted model.

\subsubsection{error variance}
$S_{yy}=SSE+SSR$
$\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n(y_i-\hat y)^2+\sum_{i=1}^n(\hat y-\bar y)^2$

$\mathbf{SSE}=\sum_{i=1}^ne_i^2=\sum_{i=1}^n(y_i-\hat y)^2=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}^n(y_i-(\bar y-\hat{\beta_1}\bar x)-\hat\beta_1x_i)^2=\sum_{i=1}^n[(y_i-\bar y)-\hat{\beta_1}(x_i-\bar x)]^2=\sum_{i=1}^n(y_i-\bar y)^2-2\hat{\beta_1}\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)+\sum_{i=1}^n(x_i-\bar x)^2=S_{yy}-2\hat\beta_1^2S_{xy}+\hat\beta_1^2S_{xx}=\mathbf{S_{yy}-\hat\beta_1^2S_{xx}}$;

$\mathbf{SSR}=\hat\beta_1^2S_{xx}=\hat\beta_1S_{xy}$;

$\mathbf{S_{yy}}=\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n[(\beta_0+\beta_1x_i+\varepsilon_i)-\frac1n\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon_i)]^2=\sum_{i=1}^n[\beta_1(x_i-\bar x)+(\varepsilon_i-\bar\varepsilon_i)]^2=\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2+2\beta_1\sum_{i=1}^n(x_i-\bar x)(\varepsilon_i-\bar\varepsilon_i)+\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)^2$;

Because $\sum_{i=1}^n(\varepsilon_i-\bar\varepsilon_i)=S_{\varepsilon_i\varepsilon_i}=(n-1)S_\varepsilon^2$;
because $E(S_\varepsilon^2)=\sigma^2$;
$\mathbf{E[S_{yy}]}=\beta_1^2S_{xx}+2\beta_1\sum_{i=1}^n(x_i-\bar x)(E[\varepsilon_i]-E[\bar\varepsilon_i])+E[(n-1)S_\varepsilon^2]=\mathbf{\beta_1^2S_{xx}+(n-1)\sigma^2}$;

$E[\hat\beta_1^2]=Var(\hat\beta_1)-E[\hat\beta_1]^2=\frac{\sigma^2}{S_xx}+\beta_1^2$;

$\mathbf{E[SSE]}=E[\mathbf{S_{yy}}]-S_{xx}E[\hat\beta_1^2]=\mathbf{(n-2)\sigma^2}$;

$\mathbf{E[SSR]=E[MSR]=\sigma^2+\hat\beta_1^2S_{xx}}$

$E[MSE]={E[SSE]}/(n-2)=\sigma^2=Var(y_i)=Var(\varepsilon)$ MSE is an unbiased estimator for error variance

\subsubsection{distribution, assumption}
$\hat{\beta_0}\sim N\left(\beta_0,\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})\right)$;
$\hat{\beta_1}\sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)$;
$\mathbf{For}\frac{(n-2)\hat\sigma^2}{\sigma^2}\sim\chi_{(n-2)}^2$;
$\hat\sigma^2=MSE=SSE/(n-2)=S_{xx}Var(\hat\beta_1)=S_{xx}se(\hat\beta_1)^2$;
$=\frac{S_{xx}}{S_{xx}+n\bar x^2}Var(\hat\beta_0)=\frac{S_{xx}}{S_{xx}+n\bar x^2}se(\hat\beta_0)^2$;

$\frac{\hat\beta_1-\beta_1}{se(\hat\beta_1)}\sim t_{(n-2)}$;
$\frac{\hat\beta_0-\beta_0}{se(\hat\beta_0)}\sim t_{(n-2)}$;
$\frac{\hat y_0-\bar y_0}{se(\hat y_0)}= \sim t_{(n-2)}$;
$MSR/MSE=t^2\sim F_{(1,(n-2))}$


\subsubsection{standard error(deviation)}
$se(\hat\beta_1)=\sqrt{Var(\hat\beta_1)}=\sqrt \frac{\hat\sigma^2}{S{xx}}$;
$se(\hat{\beta_0})=\sqrt{Var(\hat\beta_0)}=\sqrt{\hat\sigma^2(\frac1n+\frac{\bar x^2}{S_{xx}})}$;


\subsubsection{CI}
$\hat{\beta_1}\pm t_{\alpha/2}se(\beta_1)$;
$\hat{\beta_0}\pm t_{\alpha/2}se(\beta_0)$;
Since zero is (not) inside this interval, true intercept can (not) be zero with $1-\alpha$ confidence.

$(\frac{(n-2)\hat\sigma^2}{\sigma^2}\sim\chi_{(\alpha/2)}^2,\frac{(n-2)\hat\sigma^2}{\sigma^2}\sim\chi_{(1-\alpha/2)}^2)$;

\subsubsection{prediction}
estimated mean (point estimate) of response \textbf{(UNITS)} $\hat y_0=\hat{\beta_0}+\hat{\beta_1}x_0$ True mean $\bar y_0$
k number of new values of response at a new value of predictor denoted by $x_0$ \textbf{without extapolation}.
$se(\hat y_0)=\sqrt{MSE\left[(1)+\frac1n+\frac{(x_0-\bar x)^2}{S_{xx}}\right]}$;
100(1−$\alpha$) \% confidence interval for the MEAN (a single value of) response at a given new value $x_0$ of predictor is
$(\hat y_0) \hat{\beta_0}+\hat{\beta_1}x_0\pm t_{\alpha/2,n-2}se(\hat y_0)$;
$E[\hat y_0-\bar y_0]=E[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=0$; 
$\hat y_0$ is unbiased estimator of $\bar y_0$;

$Var[\hat y_0-\bar y_0]=Var[\hat{\beta_0}+\hat{\beta_1}x_0-\beta_0-\beta_1x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=Var[\bar y-\hat\beta_1\bar x+\hat{\beta_1}x_0-\frac1k\sum_{i=1}^k\varepsilon_i]=Var[\frac{\sum_{i=1}^ny_i}n-\sum_{i=1}^nc_iy_i(\bar x-x_0)-\frac1k\sum_{i=1}^k\varepsilon_i]=Var\Big[\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]y_i\Big]+Var[\sum_{i=1}^k\frac1k\varepsilon_i]-2Cov[y_i,\varepsilon_i]$
Because $y_i$ and $\varepsilon_i$ are independent for new observations.
$=\sum_{i=1}^n[\frac1n-(\bar x-x_0)c_i]^2Var(y_i)+\sum_{i=1}^k\frac1{k^2}Var(\varepsilon_i)=\sigma^2[\sum_{i=1}^n{\frac1{n^2}}-\frac{2\bar x}{n}\sum_{i=1}^nc_i+\bar x^2\sum_{i=1}^nc_i^2]=\sigma^2[\frac1n+\frac{(\bar x-x_0)^2}{S_{xx}}+\frac1k]$


\subsubsection{intercept known}
$y=\beta_0+\hat\beta_1x+\varepsilon$;
$Let SSE=\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)^2$;
$\left.\frac{\partial SSE}{\partial\beta_1}\right|_{\hat\beta_1}=-2\sum_{i=1}^n(y_i-\beta_0-\hat\beta_1x_i)x_i=0$;
$\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^n(y_i-\beta_0)x_i$;
$\hat\beta_1=\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}$;
$E(\hat\beta_1)=E\left[\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right]=\frac1{\sum_{i=1}^nx_i^2}E\left[\sum_{i=1}^nx_i(\beta_1x_i+\varepsilon)\right]=\frac{\beta_1\sum_{i=1}^nx_i^2}{\sum_{i=1}^nx_i^2}-\frac{\sum_{i=1}^nx_i}{\sum_{i=1}^nx_i^2}E[\varepsilon]=\beta_1$ is unbiased;
Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ Therefore 
$Var(\hat\beta_1)=Var\left(\frac{\sum_{i=1}^n(y_i-\beta_0)x_i}{\sum_{i=1}^nx_i^2}\right)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i-\sum_{i=1}^n\beta_0x_i)=\frac1{(\sum_{i=1}^nx_i^2)^2}Var(\sum_{i=1}^ny_ix_i)=\frac{\sum_{i=1}^nx_i^2}{(\sum_{i=1}^nx_i^2)^2}Var(y_i)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$;
$se(\hat{\beta_1})=\sqrt{Var(\hat\beta_1)}=\sqrt{\frac{\sigma^2}{\sum_{i=1}^nx_i^2}}$

\subsubsection{slope known}
$y=\hat\beta_0+\beta_1x+\varepsilon$;
$Let SSE=\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)^2$;
$\left.\frac{\partial SSE}{\partial\beta_0}\right|_{\hat\beta_0}=-2\sum_{i=1}^n(y_i-\hat\beta_0-\beta_1x_i)=0$;
$n\hat\beta_0=\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i$;
$\hat\beta_0=\bar y-\beta_1\bar x$;
$E(\hat\beta_0)=E\left[\frac1n\sum_{i=1}^ny_i-\frac1n\beta_1\sum_{i=1}^nx_i\right]=\frac1nE\left[\sum_{i=1}^n(\beta_0+\beta_1x_i+\varepsilon)-\beta_1\sum_{i=1}^nx_i\right]=\frac1n\sum_{i=1}^nE(\beta_0+\beta_1x_i)+E(\varepsilon)-\beta_1\sum_{i=1}^nx_i=\beta_0$  is unbiased;
Because $Cov(y_i,y_j)=Cov(\varepsilon_i,\varepsilon_j)=0,for\ {i}\ne{j}$ and
$Var(y_i)=Var(\beta_0+\beta_1x_i+\varepsilon_i)=Var(\varepsilon_i)=\sigma^2$ Therefore 
$Var(\hat\beta_0)=\frac1{n^2}Var\left(\sum_{i=1}^ny_i-\beta_1\sum_{i=1}^nx_i\right)=\frac1{n^2}\sum_{i=1}^nVar(y_i)=\frac{\sigma^2}n$
$se(\hat{\beta_0})=\sqrt{Var(\hat\beta_0)}=\sqrt{\frac{\sigma^2}n}=\sqrt{\frac{MSE}n}$

\subsubsection{through origin}
$y=\beta_1x_i+\varepsilon_i,\ \varepsilon_i\sim^{iid}N(0,\sigma^2)\ i=1,2..n$;
$\hat\beta_1=\frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}$;
$E(\hat\beta_1)==\beta_1$; is unbiased;
$Var(\hat\beta_1)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}$;
$se(\hat{\beta_1})=\sqrt{\frac{\hat\sigma^2}{\sum_{i=1}^nx_i^2}}$;
$\hat{\beta_1}\pm t_{\frac\alpha2,n-1}se(\hat{\beta_1})$

\subsubsection{ANOVA}
\begin{tabular}{@{}ll|@{}ll|@{}ll|@{}ll|@{}ll|@{}}
    & df  & Sum Sq             & MS  & F    & P \\
SSR & k predictor & $\hat\beta_1S{xy},\hat\beta_1^2S{xx}$ & SSR/dfR & MSR/ &   \\
SSE & n-1-k & $SST-SSR$          & SSE/dfE & MSE     \\
SST & n observation -1 & $S_{yy}$ &  & $t^2$\\
\end{tabular}
There is strong evidence that the fitted model is statistically significant at 5\% significance level because p value is much smaller than 0.05

\subsubsection{regression analysis}

\begin{tabular}{@{}ll|@{}ll|@{}ll|@{}ll|@{}}
 & estimate & se & t & P \\
intercept          & $\hat\beta_0$            & 
$\sqrt{MSE(\frac1n+\frac{\bar x^2}{S_{xx}})}$ & 
$\frac{\hat\beta_0}{se(\hat\beta_0)}$         & \\
slope          & $\hat\beta_1$            & 
$\sqrt{\frac{MSE}{S_{xx}}}$                   & 
$\frac{\hat\beta_1}{se(\hat\beta_1)}$         & \\
\end{tabular}

residual standard error($\hat\sigma=\sqrt{MSE}$) on dfE;

Multiple $R^2$, adjusted $R^2$;

F statistic on dfR and dfE.

There seems to be a decreasing (negative) medium approximately linear relationship.

The average y increases by $\beta_1$ units as the x increases by 1 unit.

There is strong (not enoygh ) evidence that the true slope of fitted model is significantly different from zero at 5\% significance level because p value is much smaller than 0.05;

The 95\% confidence interval (does not) contain zero. Therefore, we are 95\% confidence that the true intercept is different from (can be) zero;

hypothesis $H_0:\beta_1=c; H_1:\beta_1\ne c$;

Test Statistic (computed assuming $H_0$ is true:

$\pm? t_0=\frac{\hat\beta_1-c}{se(\hat\beta_1)}$;

Decision: Since the p-value$>(<)$significance level ($\alpha$=0.05), there is no enough evidence to conclude that the true slope is different from c (reject $H_0$ and conclude $H_1$ is true).

\subsection{multiple regression}
$y_i=\beta_0+\beta_1x_i1+\beta_2x_i2+..+\beta_kx_ik+\varepsilon_i=\beta_0+\sum_{\beta_j}x_ij+\varepsilon_i, i=1,2,..n, \varepsilon_i\sim^{iid} N(0,\sigma^2) $

$\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\varepsilon}$; $\mathbf{\varepsilon}\sim N(0,\sigma^2\mathbf{I})$ where 0 is a $n\times1$ vector of zero and $\mathbf{I}$ is a $n\times n$ identity matrix.




\end{multicols}
\end{document}
