---
title: "STAT561 Fall 2018 HW6"
author: 'Shen Qu'
date: "11/15/2018"
output: html_document
---

**3.18** There is an interesting relationship between negative binomial and gamma random variables, which may sometimes provide a useful approximation. Let Y be a negative binomial random variable with parameters r and p, where p is the success probability.
Show that as $p\rightarrow0$, the mgf of the random variable pY converges to that of a gamma distribution with parameters r and 1.

$$For\quad Y\sim Negative\ Binomial\ (r,p),\quad M_{Y}(t)=(\frac{p}{1-(1-p)e^t})^r, t<-log(1-p)$$

According to the Theorem 2.3.15, $M_{aX+b}(t)=e^{bt}M_X(at)$
$$M_{pY}(t)=(\frac{p}{1-(1-p)e^{pt}})^r$$
 
According to the L’Hˆopital’s rule, for $\lim_{p\rightarrow0}{p}=\lim_{p\rightarrow0}{(1-(1-p)e^{pt})}=0,\quad \lim_{x\rightarrow c}\frac{f(x)}{g(x)}=\lim_{x\rightarrow c}\frac{f'(x)}{g'(x)}$

$$\lim_{p\rightarrow0}\frac{p}{1-(1-p)e^{pt}}=\lim_{p\rightarrow0}\frac{1}{-(1-p)'e^{pt}-(1-p)te^{pt}}=\lim_{p\rightarrow0}\frac{1}{e^{pt}-(1-p)te^{pt}}=\frac1{1-t}$$

$$\implies M_{pY}(t)=(\frac1{1-t})^r,\ r>0, 0<t<1$$

For Gamma (r,1),

$$M_X(t)=(\frac1{1-\beta t})^\alpha=(\frac1{1-t})^r,\ r>0, 0<t<1$$

Therefore, the mgf of the random variable pY ($Y\sim NB(r,p), p\rightarrow0$) converges to that of Gamma (r,1).

****
**3.24** Many "named" distributions are special cases of the more common distributions already discussed. For each of the following named distributions derive the form of the pdf, verify that it is a pdf, and calculate the mean and variance.

(a) If $X\sim$ exponential($\beta$), then $Y = X^{1/\gamma}$ has the Weibull($\gamma,\beta$) distribution, where $\gamma>0$ is a constant.

$$X\sim expo(\beta),\quad \therefore f_X(x)=\frac1\beta e^{-\frac{x}{\beta}}, x\ge0,\beta>0$$

$$Let\ Y=g(X)= X^{1/\gamma}, X=g^{-1}(Y)=Y^\gamma\quad\therefore g^{-1}(y)=y^\gamma\quad \text{is monotone}$$

$$\therefore f_Y(y)=f_X(y^\gamma)|\frac{dy^\gamma}{dx}|=\frac1\beta e^{-\frac{y^\gamma}{\beta}}\gamma y^{\gamma-1}=\frac{\gamma}\beta y^{\gamma-1}e^{-\frac{y^\gamma}{\beta}}, x\ge0,\beta>0, \gamma>0$$

This is a Weibull distribution.\\

For $W\sim Gamma(\frac{n}\gamma+1,\beta)$, CDF

$$\int_0^{\infty}f_W(w)dw=\int_0^{\infty}\frac{w^{(\frac{n}\gamma+1)-1}}{\Gamma (\frac{n}\gamma+1)\beta^{\frac{n}\gamma+1}}e^{-\frac{w}\beta}dw=1,\ n=0,1,2,..$$

$$EY^n=\int_0^{\infty}y^n\frac\gamma\beta y^{\gamma-1}e^{-\frac{y^\gamma}{\beta}}dy=\int_0^{\infty}\frac{y^n}{\beta} e^{-\frac{y^\gamma}{\beta}}dy^\gamma=\beta^{\frac{n}\gamma}\Gamma(\frac{n}\gamma+1)\left[\int_0^{\infty}\frac{(y^{\gamma})^{(\frac{n}\gamma+1)-1}}{\Gamma(\frac{n}\gamma+1)\beta^{\frac{n}\gamma+1}} e^{-\frac{(y^\gamma)}{\beta}}d(y^\gamma)\right]=\beta^{\frac{n}\gamma}\Gamma(\frac{n}\gamma+1)$$

$$\therefore EY=\beta^{\frac1\gamma}\Gamma(\frac1\gamma+1),\quad EY^2=\beta^{\frac2\gamma}\Gamma(\frac2\gamma+1),\quad VarY=\beta^{\frac2\gamma}[\Gamma(1+\frac2\gamma)-\Gamma^2(1+\frac1\gamma)]$$

Or Let $u=\frac{y^\gamma}{\beta},y=(\beta u)^{\frac1\gamma}, dy=\frac{1}{\gamma}(\beta u)^{\frac1\gamma-1}du$ has same result.

****
(b) If $X\sim$ exponential($\beta$), then $Y =(2X/\beta)^{1/2}$ has the _Rayleigh_ _distribution_.

$$X\sim expo(\beta),\quad  f_X(x)=\frac1\beta e^{-\frac{x}{\beta}}, x\ge0,\beta>0$$

$$Let\ Y=g(X)= (\frac{2X}\beta)^{\frac12}, X=g^{-1}(Y)=\frac{\beta Y^2}2\quad\therefore g^{-1}(y)=\frac{\beta y^2}2,\ y>0,\ \beta>0\quad \text{is monotone}$$

$$f_Y(y)=f_X(\frac{\beta y^2}2)|\frac{d\frac{\beta y^2}2}{dx}|=\frac1\beta e^{-\frac{\beta y^2}{2\beta}}\beta y=ye^{\frac{-y^2}2}, y\ge0$$

This is a Rayleigh distribution with $\beta=\sqrt2$.

For $Z\sim N(0,1)$, 
$$EZ^2=\int_{-\infty}^{\infty}\frac{z^2}{\sqrt{2\pi}}e^{\frac{-z^2}2}dz=1\quad \implies \int_0^{\infty}\frac{z^2}{\sqrt{2\pi}}e^{\frac{-z^2}2}dz=\frac12$$
$$ EY=\int_0^{\infty}yye^{\frac{-y^2}2}dy={\sqrt{2\pi}}\left[\int_0^{\infty}\frac{y^2}{\sqrt{2\pi}}e^{\frac{-y^2}2}dy\right]=\frac{\sqrt{2\pi}}{2}$$

For $W\sim Gamma(2,2)$
$$\int_0^{\infty}f_W(w)dw=\int_0^{\infty}\frac{w^{2-1}}{\Gamma (2)2^2}e^{-\frac{w}2}dw=1$$

$$\therefore EY^2=\int_0^{\infty}y^2ye^{\frac{-y^2}2}dy=2\left[\int_0^{\infty}\frac{(y^2)^{2-1}}{\Gamma (2)2^2}e^{\frac{-y^2}2}dy^2\right]=2,\quad VarY=2-\frac{\pi}2$$

Or for $V\sim Expo(1)$

$$EV=\int_0^{\infty}vf_V(v)dv=\int_0^{\infty}ve^{-v}dv=1$$

$$\therefore EY^2=\int_0^{\infty}y^2ye^{\frac{-y^2}2}dy=2\left[\int_0^{\infty}(\frac{y^2}2)e^{\frac{-y^2}2}d(\frac{y^2}2)\right]=2,\quad VarY=2-\frac{\pi}2$$

Or let $u=y^2, v=-e^{\frac{-y^2}2}, du=2ydy, dv=ye^{\frac{-y^2}2}dy$ has same result.


****
(c) If $X\sim$ gamma(a, b), then $Y=1/X$ has the inverted gamma IG(a, b) distribution. (This distribution is useful in Bayesian estimation of variances; see Exercise 7.23.)


$$X\sim Gamma(a,b),\quad f_X(x)=\frac{x^{a-1}}{\Gamma (a)b^a}e^{-\frac{x}b}, x\ge0, a>0, b>0$$

$$Let\ Y=g(X)=\frac1X, X=g^{-1}(Y)=\frac1Y\quad\therefore g^{-1}(y)=\frac1y,\ y>0\quad \text{is monotone}$$

$$\therefore f_Y(y)=f_X(\frac1y)|\frac{d\frac1y}{dx}|=\frac{y^{1-a}}{\Gamma (a)b^a}e^{-\frac1{by}}|-\frac1{y^2}|=\frac{y^{-a-1}}{\Gamma (a)b^a}e^{-\frac1{by}},\ y>0$$

This is an Inverted gamma IG(a, b)

For $W\sim Gamma(a-n,b)$
$$\int_0^{\infty}f_W(w)dw=\int_0^{\infty}\frac{w^{(a-n)-1}}{\Gamma (a-n)b^{a-n}}e^{-\frac{w}b}dw=1, n=0,1,2,..\quad \text{and}\ \Gamma(a)=(a-1)\Gamma(a-1)$$

$$\therefore EY=\int_0^{\infty}\frac{y(\frac1y)^{a+1}}{\Gamma (a)b^a}e^{-\frac1b\frac1y}dy=\frac{1}{(a-1)b}\left[\int_0^{\infty}\frac{(\frac1y)^{(a-1)-1}}{\Gamma (a-1)b^{a-1}}e^{-\frac1b\frac1y}d(\frac1y)\right]=\frac{1}{(a-1)b},\ a>1$$

$$EY^2=\int_0^{\infty}\frac{y^2(\frac1y)^{a+1}}{\Gamma (a)b^a}e^{-\frac1b\frac1y}dy=\frac{1}{(a-1)(a-2)b^2}\left[\int_0^{\infty}\frac{(\frac1y)^{(a-2)-1}}{\Gamma (a-2)b^{a-2}}e^{-\frac1b\frac1y}d(\frac1y)\right]=\frac{1}{(a-1)(a-2)b^2},\ a>2$$

$$VarY=\frac{1}{(a-1)(a-2)b^2}-\frac{1}{(a-1)^2b^2}=\frac{a-1-(a-2)}{(a-1)^2(a-2)b^2}=\frac{1}{(a-1)^2(a-2)b^2}, a>2$$

****
(d) If $X\sim gamma(3/2, \beta)$, then $Y =(X/\beta)^{1/2}$ has the Maxwell distribution.

$$X\sim Gamma(\frac32,\beta),\quad \therefore f_X(x)=\frac{x^{\frac32-1}}{\Gamma (\frac32)\beta^{\frac32}}e^{-\frac{x}\beta}=\frac{x^{\frac12}}{\Gamma (\frac32)\beta^{\frac32}}e^{-\frac{x}\beta},\ \beta>0$$

$$Let\ Y=g(X)= (\frac{X}\beta)^{\frac12}, X=g^{-1}(Y)={\beta Y^2}\quad\therefore g^{-1}(y)=\beta y^2,\ y>0\quad  \text{is monotone}$$

Because $\Gamma (\frac32)=\Gamma(\frac12+1)=\frac12\Gamma(\frac12)=\frac{\sqrt\pi}2$

$$\therefore f_Y(y)=f_X(\beta y^2)|\frac{d\beta y^2}{dx}|=\frac{(\beta y^2)^{\frac12}}{\Gamma (\frac32)\beta^{\frac32}}e^{-\frac{\beta y^2}\beta}2\beta y=\frac{4y^2}{\sqrt\pi}e^{-y^2},\ y>0$$

For $V\sim Expo(1)$

$$EV=\int_0^{\infty}vf_V(v)dv=\int_0^{\infty}ve^{-v}dv=1$$
$$EY=\int_0^{\infty}\frac{y4y^2}{\sqrt\pi}e^{-y^2}dy=\frac2{\sqrt\pi}\left[\int_0^{\infty}y^2e^{-y^2}dy^2\right]=\frac2{\sqrt\pi}$$

For $Z\sim N(0,1)$, 
$$EZ^4=\int_{-\infty}^{\infty}\frac{z^4}{\sqrt{2\pi}}e^{\frac{-z^2}2}dz=3\quad \implies \int_0^{\infty}\frac{z^4}{\sqrt{2\pi}}e^{\frac{-z^2}2}dz=\frac32$$

$$EY^2=\int_0^{\infty}\frac{y^24y^2}{\sqrt\pi}e^{-y^2}dy=\int_0^{\infty}\frac{(\sqrt2y)^4}{\sqrt{2\pi}}e^{-\frac{(\sqrt2y)}2^2}d(\sqrt2y)=\frac32$$

$$VarY=\frac32-\frac4{\pi}$$

****
(e) If $X\sim$ exponential(1), then $Y =\alpha-\gamma \log X$ has the Gumbel($\alpha,\gamma$) distribution, where $-\infty<\alpha<\infty$ and $\gamma>0$. (The Gumbel distribution is also known as the extreme value distribution.)

$$X\sim expo(1),\quad \therefore f_X(x)=e^{-x}, x\ge0$$

$$Let\ Y=g(X)= \alpha-\gamma\log X, X=g^{-1}(Y)=e^{\frac{\alpha-Y}{\gamma}}\quad\therefore g^{-1}(y)=e^{\frac{\alpha-y}{\gamma}}$$

$$\therefore f_Y(y)=f_X(e^{\frac{\alpha-y}{\gamma}})|\frac{de^{\frac{\alpha-y}{\gamma}}}{dx}|=e^{-e^{\frac{\alpha-y}{\gamma}}}e^{\frac{\alpha-y}{\gamma}}|-\frac1{\gamma}|=\frac1{\gamma}e^{\frac{\alpha-y}{\gamma}-e^{\frac{\alpha-y}{\gamma}}}, \gamma>0$$

This is a Gumbel distribution with $\alpha,\beta$.

Because $E(ax+b)=aE(x)+b$, let $I_1=E(\ln x)=\int_0^{\infty}\ln xe^{-x}dx,\ I_2=E(\ln x)^2=\int_0^{\infty}(\ln x)^2e^{-x}dx$, $ I_1=0.5772157$ is Euler's constant.

$$EY=E(\alpha+\gamma\ln x)=\alpha+\gamma E(\ln x)=\alpha+\gamma I_1$$

$$EY^2=E(\alpha+\gamma\ln x)^2=\alpha^2+2\alpha\gamma I_1+\gamma^2 I_2$$

$$VarY=EY^2-(EY)^2=\gamma^2 [I_2-I_1^2]=\frac{\pi^2\gamma^2}{6},\ \gamma>0$$

****
**3.39** Consider the Cauchy family defined in Section 3.3. This family can be extended to a location-scale family yielding pdfs of the form.

$$f(x|\mu,\sigma)=\frac{1}{\sigma\pi\left(1+(\frac{x-\mu}{\sigma})^2\right)},\ -\infty<x<\infty$$

The mean and variance do not exist for the Cauchy distribution. So the parameters $\mu$ and $\sigma^2$ are not the mean and variance. But they do have important meaning. Show that if X is a random variable with a Cauchy distribution with parameters $\mu$ and $\sigma^2$, then:

(a) $\mu$ is the median of the distribution of X, that is, $P(X\ge\mu)=P(X\le\mu)=1/2$.

According to Theorem 3.5.6, f(x) is pdf, $\mu,\sigma\in\mathbf{R}$, $\sigma>0$. X is a random variable with pdf ($1/\sigma)f((x-\mu)/\sigma$) when and only when Z is a random variable with pdf f(z) and $X=\sigma Z+\mu$

$$\because P(X>x_{\alpha})=P(\sigma Z+\mu>\sigma z_{\alpha}+\mu)= P(Z>z_{\alpha})=\int_{z_{\alpha}}^{\infty}f(z)dz=\alpha $$

$$\therefore P(X\ge\mu)=P(\sigma Z+\mu\ge\mu)=P(Z\ge0)=\int_0^{\infty}\frac{1}{\sigma\pi(1+z^2)}d(\sigma z+\mu)$$
$$=\int_0^{\infty}\frac{1}{\pi(1+z^2)}dz=\frac{1}{\pi}\left.\tan^{-1}z\right|_0^{\infty}=\frac{1}{\pi}(\frac\pi2-0)=\frac{1}{2}$$

In same way, $P(X\le\mu)=\frac12$. Therefore, $\mu$ is the median of the distribution of X.

****
(b) $\mu+\sigma$ and $\mu-\sigma$ are the quartiles of the distribution of X, that is, $P(X\ge\mu+\sigma)=P(X\le\mu-\sigma)=1/4$. (Hint: Prove this first for $\mu=0$ and $\sigma=1$ and then use Exercise 3.38.)

In same way, let $X=\sigma Z+\mu$
$$P(X\ge\mu+\sigma)=P(\sigma Z+\mu\ge\mu+\sigma)=P(Z\ge1)=\int_1^{\infty}\frac{1}{\sigma\pi(1+z^2)}d(\sigma z+\mu)$$

$$\int_1^{\infty}\frac{1}{\pi(1+z^2)}dz=\frac{1}{\pi}\left.\tan^{-1}z\right|_1^{\infty}=\frac{1}{\pi}(\frac\pi2-\frac\pi4)=\frac{1}{4}$$

In same way, $P(X\le\mu-\sigma)=\frac14$. Therefore, $\mu+\sigma$ and $\mu-\sigma$ are the quartiles of the distribution of X.



