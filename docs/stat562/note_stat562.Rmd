---
title: 'STAT562 Notes'
author: ""
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## 4. Multiple Random Variables

### 4.4 Hierarchical Models and Mixture Distributions

**Theorem 4.4.3** if X and Y are any two random variables, then

$$EX=E(E(X|Y)),$$

provided that the expectation exist.

**Theorem 4.4.7 (Conditional variance identity)** For any two random varibles X and Y,

$$VarX=E(Var(X|Y))+Var(E(X|Y))$$

provided that the expectation exist.

### 4.5 Covariance and Correlation

**Definition 4.5.1** The covariance of X and Y is the number defined by

$$Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))$$



**Definition 4.5.2** The correlation of X and Y is the number defined by 

$$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y};\quad Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$

**Theorem 4.5.3** For any two random varibles X and Y,

$$Cov(X,Y)=EXY-\mu_X\mu_Y$$
**Theorem 4.5.5** If X and Y are independent random variables, then $Cov(X,Y)=0$ and $\rho_{XY}=0$


**Theorem 4.5.6** If X and Y are any two random variables and a and b are any two constants, then

$$Var(aX+bY)=a^2VarX+b^2VarY+2abCov(X,Y)$$
If X and Y are indepent random variables, then

$$Var(aX+bY)=a^2VarX+b^2VarY$$

## 5. Properties of a Random Sample

### 5.2 Sums of Random Variables from a Random Sample

**Definition 5.2.1** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

**Definition 5.2.2** The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by

$$\bar X=\frac{X_1+..+X_n}{n}=\frac1n\sum_{i=1}^nX_i$$

**Definition 5.2.3** The sample variance is the statistic defined by

$$S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$$
The sample standard deviation is the statistic defined by $S=\sqrt{S^2}$

**Theorem 5.2.4** $\sum_{i=1}^n(X_i-a)^2$ is minimized when $a=\bar x$


**Lemma 5.2.5** 

$$E\left(\sum_{i=1}^ng(X_i) \right)=nE(g(X_1))$$
$$Var\left(\sum_{i=1}^ng(X_i) \right)=nVar(g(X_1))$$

**Theorem 5.2.6** 

$$E\bar X=\mu$$
$$Var\bar X=\frac{\sigma^2}n$$
$$ES^2=\sigma^2$$

### 5.3 Sampling from the Normal Distribution

#### 5.3.1 Properties of the Sample Mean and Variance

**Theorem 5.3.1**

#### 5.3.2 The Derived Distributions: Student's t and Snedecor's F

**Definition 5.3.4**


**Theorem 5.3.8**

### 5.4 Order Statistics

### 5.5 Convergence Concepts

#### 5.5.1 Convergence in Probability

#### 5.5.2 Almost Sure Convergence

#### 5.5.3 Convergence in Distribution

**Theorem 5.5.14 (Central Limit Theorem)** Let $X_1,X_2,..$ be a sequence of iid random variables whose mgfs exist in a neighborhood of 0 (that is, $M_{X_i}(t)$ exists for $|t|<h$, for some positive h). Let $EX_i =\mu$ and $VarX_i=\sigma^2>0$. (Both $\mu$ and $\sigma^2$ are finite since the mgf exists.) Define $\bar X_n=(\frac1n)\sum_{i=1}^nX_i$. Let $G_n(x)$ denote the cdf of $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$. Then, for any x, $-\infty< x <\infty$,

$$\lim_{n\to\infty}G_n(x)=\int_{-\infty}^x\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}dy$$

that is, $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$ has a limiting standard normal distribution.

**Theorem 5.5.17 (Slutsky's Theorem)** If $X_n\to X$ in distribution and $Y_n\to a$, a constant, in probability, then

 a. $Y_nX_n\to aX$ in distribution

 b. $X_n+Y_n\to X+a$ in distribution

#### 5.5.4 The Delta Method

## 6. Principles of Data Reduction

### 6.2 The Sufficiency Principle

#### 6.2.1 Sufficient Statistics

**Definition 6.2.1** A satistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$.


**Example 6.2.3 (Binomial sufficient statistic)**

**Example 6.2.4 (Normal sufficient statistic)**

**Example 6.2.5 (Sufficient order statistics)**

**Theorem 6.2.6 (Factorization Theorem)**

**Example 6.2.8 (Uniform sufficient statistic)**

**Example 6.2.9 (Normal sufficient statistic, both parameters unknown)**

#### 6.2.2 Minimal Sufficient Statistics

**Definition 6.2.11** A sufficient statistic $T(\mathbf{X})$ is called a minimal sufficient statistic if, for any other sufficient statistic $T'(\mathbf{X})$, $T(\mathbf{x})$ is a function of $T'(\mathbf{X})$.

**Definition 6.2.13** Let $f(x|\theta)$, be the pmf or pdf of a sample $\mathbf{X}$. Suppose there exists a function $T'(\mathbf{x})$ such that, for every two sample points x and y, the ratio $\frac{f(x|\theta)}{f(y|\theta)}$ is constant as a function of $\theta$ if and only if $T(\mathbf{x})=T(\mathbf{y})$, then $T(\mathbf{x})$ is a minimal sufficient statistic for $\theta$.


#### 6.2.3 Ancillary Statistics

**Definition 6.2.16** A satistic $S(\mathbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic.

#### 6.2.4 Sufficient, Ancillary, and Complete Statistics

**Theorem 6.2.24 (Basu's Theorem)** If $T(\mathbf{X})$ is a complete and minimal sufficient statistic, then $T(\mathbf{X})$ is independent of every ancillary statistic.


### 6.3 The Likelihood Principle

### 6.4 The Equivariance Principle

>Equivariance Principle: if $Y=g(\mathbf{X}$ is a change of measurement scale such that the model for $\mathbf{Y}$ has the same formal structure as the model for $\mathbf{X}$, then an inference procedure should be both measurement equivariant and formally equivariant.

## 7. Point Estimation

#### 7.2.1 Method of Moments

#### 7.2.2 Maximum Likelihood Estimators

**Theorem 7.2.10 (Invariance property of MLEs)** If $\hat\theta$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\tau(\theta)$ is $\tau(\hat\theta)$.


#### 7.2.3 Bayes Estimators


**Example 7.2.16 (Normal Bayes estimators)**

$$E(\theta|x)=\frac{\tau^2}{\tau^2+\sigma^2}x+\frac{\sigma^2}{\sigma^2+\tau^2},$$

$$Var(\theta|x)=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}$$

#### 7.2.4 The EM Algorithm

(Expectation-Maximization)

**Theorem 7.2.20 (Monotonic EM sequence)** The sequence {$\hat\theta_{(r)}$} defined by 7.2.20 satisfies

$$L\left(\hat\theta^{(r+1)}|y\right)\ge L\left(\hat\theta^{(r)}|y\right)$$

### 7.3 Methods of Evaluating Estimators

#### 7.3.1 Mean Squared Error

#### 7.3.2 Best Unbiased Estimators

**Corollary 7.3.10 (Cramer-Rao Inequality, iid case)**

#### 7.3.3 Sufficiency and Unbiasedness

#### 7.3.4 Losst Function Optimality

