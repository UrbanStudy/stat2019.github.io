---
title: "STAT562 Final Exam"
author: ""
date: 'Winter 2019'
output:
  pdf_document: 
geometry: margin=0.5in
fontsize: 12pt
spacing: double
---



1. \textcolor[rgb]{0.5,0.5,0.5}{$X_1,X_2,..X_n$ is a random sample from a distribution having a p.d.f of the form.
$f(x)=\begin{cases}\lambda x^{\lambda-1}&0<x<1\\0&\text{otherwise}\end{cases}$
Find a complete sufficient statistic for $\lambda$. Justify your answer}

 - Step1: Proof sufficient 

From *Fisherâ€“Neyman factorization theorem* (`2019-2-14p5`)

$$f(x|\lambda)=L(\lambda)=\lambda^n(\prod x_i)^{\lambda-1}=\lambda^ne^{(\lambda-1)\sum^n_{i=1} \ln x_i}\cdot1=k(t|\lambda)h(\vec x)$$

$h(\vec x)=1$ is free of $\lambda$. So $T=\sum^n_{i=1} \ln x_i$  a sufficient statistic.

 - Step2: Proof complete 

$f(x|\lambda)$ is a member of the exponential family (`2019-2-19p12`),

$$f(x|\vec\lambda)=\lambda^ne^{(\lambda-1)\sum^n_{i=1} \ln x_i}=h(x)c(\vec \lambda)e^{\sum^k_{j=1}W_j(\vec \lambda)t_j(x)}$$

For $\lambda>0$, $\{W_1(\vec \lambda),..,W_k(\vec \lambda)\}$ contains an open interval in $\Re$, so $T(\vec x)=\sum^n_{i=1} \ln x_i)$ is a complete sufficient statistic.

 ---

2. \textcolor[rgb]{0.5,0.5,0.5}{Let $Y_n$ be the $n^th$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.}

 - Step1:  $\theta$ is a location parameter

Let $X=Y_n-\bar Y$. For $N(\theta,\sigma^2)$ is a location family of densities (`2018.11.20p7`),

$$g(y|\theta)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{y-\theta}{2\sigma^2}}=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(y-\theta)^2}{2\sigma^2}}=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{x^2}{2\sigma^2}}=f(x)=f(y-\theta)$$

Thus, $\theta$ is a location parameter.

 - Step2: $Y_n-\bar Y$ is location invariant

`2019-2-21p4-6`

 - step3: $Y_n-\bar Y$ is ancillary statistic for $\theta$
 
`2019-2-19p6`

 - step4: $\bar Y$ is sufficient statistic for $\theta$

 - step5: $\bar Y$ is complete statistic for $\theta$

 - step6:  By Basu's theorem, $Y_n-\bar Y$ and $\bar Y$ are independent.
 
`2019-2-19p10`

 ----
 
3. \textcolor[rgb]{0.5,0.5,0.5}{Suppose that $X_1,X_2,..X_n\sim$ idd. $f(x|\theta)=\theta e^{-\theta x}, x>0$. Assume that the prior distribution of $\theta$ is $\pi(\theta)=\lambda e^{-\lambda\theta},\theta>0$}

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}

`2019-2-26p8-9,p11-p13`

For $L(\theta)=\hat\theta e^{-\theta\sum x_i},\quad \pi(\theta)=\lambda e^{-\lambda\theta}$, and the kernel of a function is the main part of the function, the part that remains when constants are disregarded (`2019-2-28p8 Exapmle 2.3.8`). that is

$$\pi(\theta|\vec x)\propto L(\theta)\pi(\theta)=\theta^n e^{-\theta\sum x_i}\lambda e^{-\lambda\theta}\propto\theta^{n+1-1}e^{-\theta(\lambda+\sum x_i)}$$
which is $Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})$

 ----

 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}

`2019-2-28p1`

Posterior mean $=\alpha\beta=\frac{n+1}{\lambda+\sum x_i}$
 
 ----
 
 c. \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}
 
$$\frac1{\hat\theta}=\frac{\lambda+\sum x_i}{n+1}=\frac{\lambda+n\bar x}{n+1}=\frac{\frac1{\frac1\lambda}+\frac{n}{\frac1{\bar x}}}{n+1}$$

$$\hat\theta=\frac1{\frac{\frac1\lambda+\frac{n}{\frac1{\bar x}}}{n+1}}=\frac1{\frac1{n+1}(\frac1{1/\lambda}+\frac{n}{1/\bar x})}$$
 which is the weighted hamonic mean of $\frac1{\lambda}$, which is the prior mean, and $\frac1{\bar x}$, which is the MLE of $\theta$.
 
 ----
 
 d. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}
 
 `2019-2-28p4,9`
 
 Postmedian
 $$\hat\theta=F^{-1}(\frac12)$$
 where $F(x)$ is the $Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})$ pdf, for which there is no closed form.
 
 ----
 
 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}
 
 `2019-2-28p5-6`
 
 Posterior mode
 $$\hat\theta=(\alpha-1)\beta=\frac{n}{\lambda+\sum x_i}$$

 ----
 
4. \textcolor[rgb]{0.5,0.5,0.5}{Redo all of problem 3, using the non-informative prior $\pi(\theta)=1,\theta>0$. Note that this is not a valid density function since its intergral is infinite, but proceed with it anyway}

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}

`2019-2-26p8-9`

For $\pi(\theta)=1,\theta>0$, $L(\theta)=\theta^n e^{-\theta\sum x_i}$

$$\pi(\theta|\vec x)\propto L(\theta)\pi(\theta)=\theta^n e^{-\theta\sum x_i}\sim Gamma(\alpha=n+1,\beta=\frac1{\sum x_i}=\frac{n}{\bar x})$$

 ----
 
 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}

$$\hat\theta=\alpha\beta=\frac{n+1}{\sum x_i}$$

 ----
 
 c. \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}

$$\frac1{\hat\theta}=\frac{1\times0+n\bar x}{n+1}=\lim_{c\to\infty}\frac{1\times\frac1c+\frac{n}{\frac1{\bar x}}}{n+1}$$
So $\hat\theta$ is  the limit as $c\to\infty$ of the hamonic mean of $\frac1{\bar x}$ (the MLE) and $c$.(the prior mean is infinite)

 ----
 
 d. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}

Posterior median $\hat\theta$, which has no closed form.

 ----
 
 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}

Posterior mode  $$\hat\theta=(\alpha-1)\beta=\frac{n}{\sum x_i}=\frac{1}{\bar x}$$

 ----
 
5. \textcolor[rgb]{0.5,0.5,0.5}{Let $X_1,X_2,..X_n\sim$ idd. $f(x|\theta)=\theta x^{-\theta-1}, x_i>1, \theta>2$.}

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MLE}$, the maximum likelihood estimator of $\theta$.}
 
`2019-2-2p12`

$$f(\vec x|\theta)=L(\theta)=\theta^n(\prod x_i)^{\theta-1}=\theta^n e^{(-\theta-1\sum^n_{i=1} \ln x_i)}$$
$$l(\theta)=n\ln\theta-(\theta+1)\sum^n_{i=1} \ln x_i$$
$$l'(\theta)=\frac{n}\theta-\sum^n_{i=1} \ln x_i\overset{\text{set}}{=}0$$

$$\hat\theta=\frac{n}{\sum\ln x_i}$$

 ----
 
 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the expected value of $\hat\theta_{MLE}$.}

Let $Y_i=\ln x_i$, then $X=e^y$, $\frac{dx}{dy}e^y$

$$g(Y)=\theta(e^y)^{-\theta-1}e^y=\theta e^{-y\theta}, y>0$$

So $Y_i=\ln x_i\sim Gamma(\alpha=n,\beta=\frac1{\theta})$

$$E[\hat\theta]=nE[Y^{-1}]=n\frac{\beta^{-1}\Gamma(-1+\alpha)}{\Gamma(\alpha)}=\frac{n\theta\Gamma(n-1)}{\Gamma(n)}=\frac{n\theta(n-2)!}{(n-1)!}=\frac{n\theta}{n-1}$$

 ----
 
 c. \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{MLE}$.}
 
$$E[\hat\theta^2]=n^2E[Y^{-2}]=n^2\frac{\beta^{-2}\Gamma(-2+\alpha)}{\Gamma(\alpha)}=\frac{n^2\theta^2\Gamma(n-2)}{\Gamma(n)}=\frac{n^2\theta^2(n-3)!}{(n-1)!}=\frac{n^2\theta^2}{(n-1)(n-2)}$$
$$Var[\hat\theta^2]=\frac{n^2\theta^2}{(n-1)(n-2)}-\frac{n^2\theta^2}{(n-1)^2}=\frac{n^2\theta^2}{(n-1)}[\frac1{n-2}-\frac1{n-1}]=\frac{n^2\theta^2}{(n-1)^2(n-2)}$$

 ----
 
 d. \textcolor[rgb]{0.5,0.5,0.5}{Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.}

$$\hat\theta_{U}=\frac{n-1}{n}\hat\theta_{MLE}$$

 ----
 
 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{U}$.}

$$Var[\hat\theta_{U}]=(\frac{n-1}{n})^2\frac{n^2\theta^2}{(n-1)^2(n-2)}=\frac{\theta^2}{n-2}$$

6. \textcolor[rgb]{0.5,0.5,0.5}{Refer to problem 5.}

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MOM}$, the method of moments estimator of $\theta$.}

`2019-2-21p8 7.2.1`

$$\mu=\int_1^{\infty}x\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+1}}{-\theta+1}\right|_1^{\infty}=\frac\theta{\theta-1}$$

Set $\bar X=\frac\theta{\theta-1}$, $\theta\bar x-\bar x=\theta$, $\theta(\bar x-1)=\bar x$, 

$\hat\theta_{MOM}=\frac{\bar x}{\bar x-1}$

 ----
 
 b. \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the expected value of $\hat\theta_{MOM}$.}

`2019-3-5p1`

$$g(x)=\frac{x}{x-1},\quad g'(x)=\frac{(x-1)\times1-x\times1}{(x-1)^2}=\frac{-1}{(x-1)^2},\quad g''(x)=\frac{2}{(x-1)^3}$$

$$g(x)=g(x_0)+g'(x_0)(x-x_0)+g''(x_0)\frac{(x-x_0)^2}2+R$$

Choose $x_0=EX=\mu$

$$g(x)\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(x-\mu)+\frac{2}{(\mu-1)^3}\frac{(x-\mu)^2}2$$

$$\hat\theta_{MOM}=\frac{\bar x}{\bar x-1}\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(\bar x-\mu)+\frac{1}{(\mu-1)^3}(\bar x-\mu)^2$$

For $EX=\mu=\frac\theta{\theta-1}$

$$E[X^2]=\int_1^{\infty}x^2\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+2}}{-\theta+2}\right|_1^{\infty}=\frac\theta{\theta-2}$$

$$Var[X]=\sigma^2=E[X^2]-E[X]^2=\frac\theta{\theta-2}-(\frac\theta{\theta-1})^2$$

$$=\frac{\theta(\theta-1)^2-\theta^2(\theta-2)}{(\theta-1)^2(\theta-2)}=\frac{\theta^3-2\theta^2+\theta-\theta^3+2\theta^2}{(\theta-1)^2(\theta-2)}=\frac{\theta}{(\theta-1)^2(\theta-2)}$$

$$E[\hat\theta_{MOM}]\approx \frac{\mu}{\mu-1}+0+\frac{1}{(\mu-1)^3}\frac{\sigma^2}n=\frac{\frac\theta{\theta-1}}{\frac\theta{\theta-1}-1}+\frac{1}{(\frac\theta{\theta-1}-1)^3}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\theta+\frac{\theta(\theta-1)}{n(\theta-2)}$$

 ----
 
 c. \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the variance of $\hat\theta_{MOM}$.}

`2019-3-5p3`

$$Var[\hat\theta_{MOM}]\approx Var[g(x_0)+g'(x_0)(x-x_0)]=Var[\frac{\mu}{\mu-1}+\frac{1}{(\mu-1)^2}(\bar x-\mu)]$$
$$=\frac{1}{(\mu-1)^4}\frac{\sigma^2}n=\frac{1}{(\frac\theta{\theta-1}-1)^4}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\frac{\theta(\theta-1)^2}{n(\theta-2)}$$


```{r, eval =F}
End
```