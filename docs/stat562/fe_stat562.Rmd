---
title: "STAT562 Final Exam"
author: "Shen Qu 918881147"
date: 'Winter 2019'
output:
  pdf_document: 
geometry: margin=0.5in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=FALSE, size="tiny", results="markup", tidy=T)

# theme: default, cerulean, journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti

# highlight: default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, and textmate

#    includes:
#      in_header: preamble.tex
#      before_body: doc-prefix.tex
#      after_body: doc-suffix.tex

#    df_print: default, kable, tibble

#    citation_package: natbib, biblatex

# size = c("normalsize", "tiny", "scriptsize", "footnotesize", "small", "large", "Large", "LARGE", "huge", "Huge")
```

1. \textcolor[rgb]{0.5,0.5,0.5}{$X_1,X_2,..X_n$ is a random sample from a distribution having a p.d.f of the form.
$f(x)=\begin{cases}\lambda x^{\lambda-1}&0<x<1\\0&\text{otherwise}\end{cases}$
Find a complete sufficient statistic for $\lambda$. Justify your answer}

$$L(\lambda)=\lambda^n(\prod x_i)^{\lambda-1}=\lambda^ne^{(\lambda-1\sum^n_{i=1} \ln x_i)}$$

For $\lambda>0$, certainly contains an open interval, so $\sum^n_{i=1} \ln x_i)$ is a complete sufficient statistic.

2. \textcolor[rgb]{0.5,0.5,0.5}{Let $Y_n$ be the $n^th$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.}

$\theta$ is a location parameter.

$Y_n-\bar Y$ is location invariant, and so it is ancillary for $\theta$

$\bar Y$ is sufficient for $\theta$, and is complete.

By Basu's theorem, $Y_n-\bar Y$ and $\bar Y$ are independent.
 
3. \textcolor[rgb]{0.5,0.5,0.5}{Suppose that $X_1,X_2,..X_n\sim$ idd.}

$$L(\theta)=\hat\theta e^{-\theta\sum x_i},\quad \pi(\theta)=\lambda e^{-\lambda\theta}$$

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}

$$\pi(\theta|\vec x)\propto \theta^n e^{-\theta\sum x_i}\lambda e^{-\lambda\theta}\sim Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})$$

 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}
 
Posterior mean $=\alpha\beta=\frac{n+1}{\lambda+\sum x_i}$
 
 c. \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}
 
$$\frac1{\hat\theta}=\frac{\lambda+\sum x_i}{n+1}=\frac{\lambda+n\bar x}{n+1}=\frac{\frac1\lambda+\frac{n}{\frac1{\bar x}}}{n+1}$$

$$\hat\theta=\frac1{\frac{\frac1\lambda+\frac{n}{\frac1{\bar x}}}{n+1}}$$
 which is the weighted hamonic mean of $\frac1{\lambda}$, which is the prior mean, and $\frac1{\bar x}$, which is the MLE of $\theta$.
 
 
 d. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}
 
 Postmedian
 $$\hat\theta=F^{-1}(.s)$$
 where $F(x)$ is the $Gamma(\alpha=n+1,\beta=\frac1{\lambda+\sum x_i})$ pdf, for which there is no closed form.
 
 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}
 
 Posterior mode
 $$\hat\theta=(\alpha-1)\beta=\frac{n}{\lambda+\sum x_i}$$

4. \textcolor[rgb]{0.5,0.5,0.5}{Redo all of problem 3, using the non-informative prior}

$\pi(\theta)=1,\theta>0$, so $$\pi(\theta|\vec x)\propto L(\theta)=\theta^n e^{-\theta\sum x_i}$$

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find the posterior distribution $\pi(\theta|\vec x)$.}

It must be
$$\pi(\theta|\vec x)\propto \theta^n e^{-\theta\sum x_i}\sim Gamma(\alpha=n+1,\beta=\frac1{\sum x_i}=\frac{n}{\bar x})$$

 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming square-error loss}

$$\hat\theta=\alpha\beta=\frac{n+1}{\sum x_i}$$

 c. \textcolor[rgb]{0.5,0.5,0.5}{writing this estimator as a weighted (arithmetic, geometric, or harmonic) average of the MLE and some prior constant}

$$\frac1{\hat\theta}=\frac{1\times0+n\bar x}{n+1}=\lim_{c\to\infty}\frac{1\times\frac1c+\frac{n}{\frac1{\bar x}}}{n+1}$$
So $\hat\theta$ is  the limit as $c\to\infty$ of the hamonic mean of $\frac1{\bar x}$ (the MLE) and $c$.(the prior mean is infinite)

 d. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming absolute loss}

Posterior median $\hat\theta$, which has no closed form.

 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the Bayes estimator of $\theta$, assuming binary loss}

Posterior mode  $$\hat\theta=(\alpha-1)\beta=\frac{n}{\sum x_i}=\frac{1}{\bar x}$$

5. \textcolor[rgb]{0.5,0.5,0.5}{Let $X_1,X_2,..X_n\sim$ idd. $f(x|\theta)=$}

$$L(\theta)=\theta^n(\prod x_i)^{\theta-1}=\theta^n e^{(-\theta-1\sum^n_{i=1} \ln x_i)}$$
$$l(\theta)=n\ln\theta-(\theta+1)\sum^n_{i=1} \ln x_i$$
$$l'(\theta)=\frac{n}\theta-\sum^n_{i=1} \ln x_i\overset{\text{set}}{=}0$$

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MLE}$, the maximum likelihood estimator of $\theta$.}

$$\hat\theta=\frac{n}{\sum\ln x_i}$$

 b. \textcolor[rgb]{0.5,0.5,0.5}{Find the expected value of $\hat\theta_{MLE}$.}

Let $Y_i=\ln x_i$, then $X=e^y$, $\frac{dx}{dy}e^y$

$$g(Y)=\theta(e^y)^{-\theta-1}e^y=\theta e^{-y\theta}, y>0$$

So $Y_i=\ln x_i\sim Gamma(\alpha=n,\beta=\frac1{\theta})$

$$E[\hat\theta]=nE[Y^{-1}]=n\frac{\beta^{-1}\Gamma(-1+\alpha)}{\Gamma(\alpha)}=\frac{n\theta\Gamma(n-1)}{\Gamma(n)}=\frac{n\theta(n-2)!}{(n-1)!}=\frac{n\theta}{n-1}$$

 c. \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{MLE}$.}
 
$$E[\hat\theta^2]=n^2E[Y^{-2}]=n^2\frac{\beta^{-2}\Gamma(-2+\alpha)}{\Gamma(\alpha)}=\frac{n^2\theta^2\Gamma(n-2)}{\Gamma(n)}=\frac{n^2\theta^2(n-3)!}{(n-1)!}=\frac{n^2\theta^2}{(n-1)(n-2)}$$
$$Var[\hat\theta^2]=\frac{n^2\theta^2}{(n-1)(n-2)}-\frac{n^2\theta^2}{(n-1)^2}=\frac{n^2\theta^2}{(n-1)}[\frac1{n-2}-\frac1{n-1}]=\frac{n^2\theta^2}{(n-1)^2(n-2)}$$

 d. \textcolor[rgb]{0.5,0.5,0.5}{Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.}

$$\hat\theta_{U}=\frac{n-1}{n}\hat\theta_{MLE}$$


 e. \textcolor[rgb]{0.5,0.5,0.5}{Find the variance of $\hat\theta_{U}$.}

$$Var[\hat\theta_{U}]=(\frac{n-1}{n})^2\frac{n^2\theta^2}{(n-1)^2(n-2)}=\frac{\theta^2}{n-2}$$

6. \textcolor[rgb]{0.5,0.5,0.5}{Refer to problem 5.}

$$\mu=\int_1^{\infty}x\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+1}}{-\theta+1}\right|_1^{\infty}=\frac\theta{\theta-1}$$

So $\bar X=\frac\theta{\theta-1}$, $\theta\bar x-\bar x=\theta$, $\theta(\bar x-1)=\bar x$, 

 a. \textcolor[rgb]{0.5,0.5,0.5}{Find $\hat\theta_{MOM}$, the method of moments estimator of $\theta$.}

$\hat\theta_{MOM}=\frac{\bar x}{\bar x-1}$


 b. \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the expected value of $\hat\theta_{MOM}$.}

$g(x)=\frac{x}{x-1}$, $g'(x)=\frac{(x-1)\times1-x\times1}{(x-1)^2}=\frac{-1}{(x-1)^2}$, $g''(x)=\frac{2}{(x-1)^3}$

$$g(x)\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(x-\mu)+\frac{2}{(\mu-1)^3}\frac{(x-\mu)^2}2$$

$$\frac{\bar x}{\bar x-1}\approx \frac{\mu}{\mu-1}+\frac{-1}{(\mu-1)^2}(\bar x-\mu)+\frac{1}{(\mu-1)^3}(\bar x-\mu)^2$$

$$E[\frac{\bar x}{\bar x-1}]\approx \frac{\mu}{\mu-1}+\frac{1}{(\mu-1)^3}\frac{\sigma^2}n$$
where $\mu=\frac\theta{\theta-1}$

$$E[x^2]=\int_1^{\infty}x^2\theta x^{-\theta-1}dx=\left.\theta\frac{x^{-\theta+2}}{-\theta+2}\right|_1^{\infty}=\frac\theta{\theta-2}$$

$$\sigma^2=\frac\theta{\theta-2}-(\frac\theta{\theta-1})^2=\frac{\theta(\theta-1)^2-\theta^2(\theta-2)}{(\theta-1)^2(\theta-2)}=\frac{\theta^3-2\theta^2+\theta-\theta^3+2\theta^2}{(\theta-1)^2(\theta-2)}=\frac{\theta}{(\theta-1)^2(\theta-2)}$$

$$E[\hat\theta_{MOM}]\approx \frac{\frac\theta{\theta-1}}{\frac\theta{\theta-1}-1}+0+\frac{1}{(\frac\theta{\theta-1}-1)^3}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\theta+\frac1n\frac{\theta(\theta-1)}{\theta-2}$$

 c. \textcolor[rgb]{0.5,0.5,0.5}{Using the delta method to approximate the variance of $\hat\theta_{MOM}$.}

$$Var[\hat\theta_{MOM}]\approx \frac{1}{(\mu-1)^4}\frac{\sigma^2}n=\frac{1}{(\frac\theta{\theta-1}-1)^4}\frac1n\frac{\theta}{(\theta-1)^2(\theta-2)}=\frac1n\frac{\theta(\theta-1)^2}{\theta-2}$$


```{r, eval =F}
End
```