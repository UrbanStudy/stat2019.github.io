---
title: 'STAT562 Homework'
author: "Shen Qu"
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## HW1

**4.4**  A pdf is defined by $f(x,y)=\begin{cases}C(x+2y)& 0<y<1,0<x<2 \\0 &otherwise\end{cases}$

 (a). Find the value of C.

$$\because \int_0^1\int_0^2f(x,y)dxdy=\int_0^1\int_0^2C(x+2y)dxdy=C\int_0^1\left[\frac{x^2}2+2yx\right]_0^2dy=C\int_0^1(2+4y)dy=\left.C(2y+2y^2)\right|_0^1=4C=1$$

$$\therefore C=\frac14$$

  ---

 (b). Find the marginal distribution of X.
 
 $$f_X(x)=\begin{cases}\int_0^1f(x,y)dy=\int_0^1\frac14(x+2y)dy=\frac14\left[xy+y^2\right]_0^1=\frac14(x+1)& 0<x<2\\ 0 & otherwise \end{cases}$$

  ---

 (c). Find the joint cdf of X and Y.
 
 let u=X, v=Y
 
 $$F_{XY}(x,y)=P(X\le x,Y\le y)=\begin{cases}0& x\le0,y\le0\\\\\int_0^x\int_0^yf(u,v)dvdu&0<y<1,0<x<2\\\\1& x\ge2,y\ge1 \end{cases}$$
 
$$=\begin{cases}0& y\le0,x\le0\\\int_0^x\int_0^y\frac14(u+2v)dvdu=\frac14\int_0^x[uv+v^2]_{v=0}^{y}du=\frac14\int_0^x(yu+y^2)du=\frac14[\frac{yu^2}2+y^2u]_{u=0}^x=\frac{x^2y}8+\frac{xy^2}4&0<y<1,0<x<2\\\int_0^2\int_0^y\frac14(u+2v)dvdu=\frac14\int_0^2[uv+v^2]_{v=0}^{y}du=\frac14\int_0^2(yu+y^2)du=\frac14[\frac{yu^2}2+y^2u]_{u=0}^2=\frac{y^2}2+\frac{y}2&0<y<1,2\le x\\\int_0^x\int_0^1\frac14(u+2v)dvdu=\frac14\int_0^x[uv+v^2]_{v=0}^{1}du=\frac14\int_0^x(u+1)du=\frac14[\frac{u^2}2+u]_{u=0}^x=\frac{x^2}8+\frac{x}{4}&1\le y,0<x<2 \\1& y\ge1,x\ge2\end{cases}$$

  ---

 (d). Find the pdf of the random variable $Z=9/(X + 1)^2$

For $Z=g(x)=9/(X + 1)^2$ is monotone when $0<x<2$, $X=g^{-1}(z)=3Z^{-\frac12}-1$, $1<z<9$, $d(g^{-1}(z))=-\frac32z^{-\frac32}$

$$f_Z(z)=\begin{cases}f_X(g^{-1}(z))|\frac{d}{dz}g^{-1}(z)|=\frac14(3z^{-\frac12}-1+1)|-\frac32z^{-\frac32}|=\frac98z^{-2}\quad1<z<9 \\0\qquad elsewhere\end{cases}$$

  ---

**4.11** Let U the number of trials needed to get the first head and V the number of trials needed to get two heads in repeated tosses of a fair coin. Are U and V independent random variables?

For $U\sim Geom(p=\frac12),u=1,2..$ and $V\sim N Bin(p=\frac12,r=2),v=2,3..$, the distribution of (U,V) is $\{(u,v):u=1,2,..;v=u+1,u+2,..\}$, which is not a cross-product set. Given $U=u$, $V>u$ is always true and U and V are not independent.


  ---

**4.16** Let X and Y be independent random variables with the same geometric distribution.

 (a). Show that U and V are independent, where U and V are defined by $U=\min(X,Y)$ and $V=X-Y$.

$$f_{U,V}(u,v)=P(U=u,V=v)$$

$$=\left\{\begin{array}{l} P(X=u+v,Y=u)=p(1-p)^{u+v-1}p(1-p)^{u-1}=p^2(1-p)^{2u+v-2} &\text{if } X>Y\\P(X=u,Y=u)=p^2(1-p)^{2(u-1)} &\text{if } X=Y \\P(X=u,Y=u-v)=p(1-p)^{u-1}p(1-p)^{u-v-1}=p^2(1-p)^{2u-v-2} &\text{if } X<Y\end{array}\right\}=(\frac{p}{1-p})^{2}(1-p)^{2u}(1-p)^{|v|},$$

$u=1,2..,v=0,\pm1,\pm2,..$

Since the joint pmf factors into a function of u and v, U and V are independent.

  ---

 (b). Find the distribution of $Z=X/(X+Y)$, where we define $Z=0$ if $X+Y=0$.

Suppose r and s are positve integers, $r<s$, and $\frac{r}s$ is in reduced form. Let $Z=\frac{X}{X+Y}=\frac{ir}{is},i=1,2..$, then $x=ir,y=i(s-r)$

$$P(Z=\frac{ir}{is})=\sum_{i=1}^{\infty}P(X=ir,Y=i(s-r))=\sum_{i=1}^{\infty}p(1-p)^{ir-1}p(1-p)^{i(s-r)-1}=(\frac{p}{1-p})^{2}\sum_{i=1}^{\infty}(1-p)^{is}$$

This is a geometric series with common ratio $0<(1-p)^{s}<1$, therefore

$$=(\frac{p}{1-p})^{2}\frac{(1-p)^{s}}{1-(1-p)^{s}}$$

  ---

 (c). Find the joint pdf of X and X + Y.

Let $U=X+Y$, then $Y=U-X$

$$P(X=x,U=x+y)=P(X=x,Y=u-x)=P(X=x)P(Y=u-x)=p(1-p)^{x-1}p(1-p)^{u-x-1}=(\frac{p}{1-p})^{2}(1-p)^{u}$$

  ---

**4.17** Let X be an exponential(1) random variable, and define Y to be the integer part of X + 1, that is $Y=i+1$ if and only if $i\le X<i+1, i=0,1,2,..$

(a) Find the distribution of Y . What well-known distribution does Y have?

For $Y=X+1$, $X=Y-1$

$$P(Y=i+1)=\int_{i}^{i+1}e^{-x}dx=\left.-e^{-x}\right|_{x=i}^{i+1}=-e^{-i-1}+e^{-i}=(1-\frac1e)(\frac1e)^i$$

Which is a Geometric distribution with $p=1-\frac1e,x=0,1,2,..$

  ---

(b) Find the conditional distribution of $X-4$ given $Y\ge5$.

For $f_X(x)=e^{-x}$, $F_X(x)=P(X\le x)=1-e^{-x}$ and $1-F_X(x)=P(X>x)=e^{-x}$

$$P(X-4\le x|Y\ge5)=P(X\le x-4|X+1\ge5)=1-P(X\ge x+4|X\ge4)$$

According to the 'memoryless' property of Exponential distribution,

$$=1-P(X\ge x|X\ge4)=1-P(X\ge x)=P(X\le x)=1-e^{-x},\quad x\ge4$$

  ---

**4.20** $X_1$ and $X_2$ are independent $n(0, \sigma^2)$ random variables.

 (a). Find the joint distribution of Y1 and Y2 , where $Y_1 = X_1^2 + X_2^2$ and $Y_2 =\frac{X_1}{\sqrt{Y_1}}$
 
Since $Y_2 =\frac{X_1}{\sqrt{Y_1}}=\frac{X_1}{\sqrt{X_1^2+ X_2^2}}=\pm\sqrt{1-\frac{X_2^2}{\sqrt{X_1^2+ X_2^2}}}$. Thus, $|y_2|<1$

To let the transformation between the points $(x_1,x_2)$ and $(y_1,y_2)$ is one-to-one, let

$$\left.\begin{array}\mathcal{A_0}=\{-\infty<x_1<\infty,x_2=0\}\\ \mathcal{A_1}=\{-\infty<x_1<\infty,x_2>0\} \\ \mathcal{A_2}=\{-\infty<x_1<\infty,x_2<0\} \\ \end{array}\right\}\to\mathcal{B}=\{0<y_1<\infty,|y_2|<1\}$$



$$f_{Y_1,Y_2}(y_1,y_2)=\sum_{i=1}^kf_{X_1,X_2}(h_1(y_1,y_2),\ h_2(y_1,y_2))|J_i|$$

$$h_1(y_1,y_2)=y_2\sqrt{y_1}$$
$$h_2(y_1,y_2)=\pm\sqrt{(1-y_2^2)y_1}$$

$$J_{1,2}=\begin{vmatrix}\frac{\partial h_1}{\partial y_1} & \frac{\partial h_2}{\partial y_2} \\ \frac{\partial h_1}{\partial y_1} & \frac{\partial h_2}{\partial y_2} \end{vmatrix}=\frac{\partial h_1}{\partial y_1}\frac{\partial h_2}{\partial y_2}-\frac{\partial h_2}{\partial y_1}\frac{\partial h_1}{\partial y_2}=\pm\frac12y_2y_1^{-\frac12}\cdot y_1^{\frac12}[\frac12(1-y_2^2)^{-\frac12}(-2y_2)]\mp \frac12y_1^{-\frac12}(1-y_2^2)^{\frac12}y_1^{\frac12}$$
$$=\mp\frac{y_2^2}{2(1-y_2^2)^{\frac12}}\mp \frac{1-y_2^2}{2(1-y_2^2)^{\frac12}}=\mp\frac12(1-y_2^2)^{-\frac12}$$

$$\therefore f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(h_1(y_1,y_2))\cdot f_{X_1,X_2}(h_2(y_1,y_2))(|J_1|+|J_2|)=\frac1{2\pi\sigma^2}e^{-\frac{y_2^2y_1+(1-y_2^2)y_1}{2\sigma^2}}(1-y_2^2)^{-\frac12}=\frac1{2\pi\sigma^2}e^{-\frac{y_1}{2\sigma^2}}(1-y_2^2)^{-\frac12}$$

$\quad 0<y_1<\infty,|y_2|<1$

  ---

 (b). Show that $Y_1$ and $Y_2$ are independent, and interpret this result geometrically.

The joint pdf, $f_{Y_1,Y_2}(y_1,y_2)$ factors into a function of $y_1$ and $y_2$. So $Y_1$ and $Y_2$ are independent. 


$Y_1=X_1^2+X_2^2$ is the suqared distance of hypotenuse in Pythagorean equation, which represents the square of the distance from $(X_1,X_2)$ to the origin. 

$Y_2=\frac{X_1}{\sqrt{Y_1}}$ is the ratio of the adjacent leg (the side of the triangle joining the angle to the right angle) to the hypotenuse, which represents the cosine of the angle between the positive $x_1$-axis and the line from $(X_1,X_2)$ to the origin. 

It is ture that the distance from the origin is independent of the orientation - the angle.


## HW2

**5.7** In Example 5.2. 10, a partial fraction decomposition is needed to derive the distribution of the sum of two independent Cauchy random variables. This exercise provides the details that are skipped in that example.

 (a). Find the constants A, B, C, and D that satisfy $\frac1{1+(\frac{w}\sigma)^2} \frac1{1+(\frac{z-w}{\tau})^2}=\frac{Aw}{1+(\frac{w}\sigma)^2}+\frac{B}{1+(\frac{w}\sigma)^2}-\frac{Cw}{1+(\frac{z-w}{\tau})^2}-\frac{D}{1+(\frac{z-w}{\tau})^2}$, where A, B, C, and D may depend on z but not on w.

Transfer the equation to 

$$(1+\frac{(z-w)^2}{\tau^2})(Aw+B)-(1+\frac{w^2}{\sigma^2})(Cw+D)=1$$

Expand this equation

$$\left(\frac{A}{\tau^2}-\frac{C}{\sigma^2}\right)w^3+\left(\frac{B-2Az}{\tau^2}-\frac{D}{\sigma^2}\right)w^2+\left(\frac{A\tau^2+Az^2-2Bz}{\tau^2}-C\right)w+\frac{B\tau^2+Bz^2}{\tau^2}-D-1=0$$

For A, B, C,and D are independent with w, let k is a constant, then

$$\begin{cases}\frac{A}{\tau^2}-\frac{C}{\sigma^2}=0 &(1)  \\
-\frac{2\sigma^2z}{\tau^2}A+\frac{\sigma^2}{\tau^2}B-D=0 &(2)\\
\frac{\tau^2+z^2}{\tau^2}A-\frac{2z}{\tau^2}B-C=0 &(3)\\
\frac{\tau^2+z^2}{\tau^2}B-D-1=0 &(4)
\end{cases}\implies \begin{cases} A=\frac{k}{\sigma^2} \\
C=\frac{k}{\tau^2}\\
B=\frac{(\tau^2+z^2-1)k}{2z}\\
D=\frac{(\tau^2+z^2)(\tau^2+z^2-1)k-2z\tau^2}{2z\tau^2}
\end{cases}$$

It shows k exist. We can get k in question (b).

 (b). Using the facts that evaluate (5.2.4) and hence verify (5.2.5).
$\int\frac{t}{1+t^2}dt=\frac12\log(1+t^2)+$ constant and $\int\frac{1}{1+t^2}dt=\arctan(t)+$ constant
(Note that the integration in part (b) is quite delicate. Since the mean of a Cauchy does not exist, the integrals $\int_{-\infty}^{\infty}\frac{Aw}{1+(\frac{w}\sigma)^2}dw$ and $\int_{-\infty}^{\infty}\frac{Cw}{1+(\frac{z-w}{\tau})^2}dw$ dw do not exist. However, the integral of the difference does exist, which is all that is needed.)

$$f_Z(z)=\int_{-\infty}^{\infty}\frac1{\pi\sigma}\frac1{1+(\frac{w}\sigma)^2}\frac1{\pi\tau}\frac1{1+(\frac{z-w}\tau)^2}dw=\frac1{\pi^2\sigma\tau}\int_{-\infty}^{\infty}\left(\frac{Aw}{1+(\frac{w}\sigma)^2}+\frac{B}{1+(\frac{w}\sigma)^2}-\frac{Cw}{1+(\frac{z-w}{\tau})^2}-\frac{D}{1+(\frac{z-w}{\tau})^2}\right)dw$$

$$\begin{cases}\int_{-\infty}^{\infty}\left(\frac{Aw}{1+(\frac{w}\sigma)^2}+\frac{C(z-w)}{1+(\frac{z-w}{\tau})^2}\right)dw=\left[\frac{\sigma^2A}2\ln(1+(\frac{w}\sigma)^2)-\frac{\tau^2C}2\ln(1+(\frac{z-w}\tau)^2)\right]_{-\infty}^{\infty} &(1)  \\
-\int_{-\infty}^{\infty}\frac{Cz}{1+(\frac{z-w}{\tau})^2}dw=Cz\tau\int_{-\infty}^{\infty}\frac{1}{1+(\frac{z-w}{\tau})^2}d\frac{z-w}{\tau}=Cz\tau[\arctan(\frac{z-w}{\tau})]_{-\infty}^{\infty}=-\pi\tau Cz &(2)\\
\int_{-\infty}^{\infty}\frac{B}{1+(\frac{w}\sigma)^2}dw=B\sigma\int_{-\infty}^{\infty}\frac{1}{1+(\frac{w}\sigma)^2}d\frac{w}\sigma=B\sigma[\arctan(\frac{z-w}{\tau})]_{-\infty}^{\infty}=\pi\sigma B &(3)\\
-\int_{-\infty}^{\infty}\frac{D}{1+(\frac{z-w}{\tau})^2}dw=D\tau\int_{-\infty}^{\infty}\frac{1}{1+(\frac{z-w}{\tau})^2}d\frac{z-w}{\tau}=D\tau[\arctan(\frac{z-w}{\tau})]_{-\infty}^{\infty}=-\pi\tau D &(4)
\end{cases}$$

$$\therefore f_Z(z)=\frac1{\pi^2\sigma\tau}\left((1)+(2)+(3)+(4)\right)=\frac1{\pi(\sigma+\tau)}\frac1{1+(\frac{z}{\sigma+\tau})^2},\quad -\infty<z<\infty$$

$$(1)+(2)+(3)+(4)=\frac{\pi\sigma\tau}{(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}$$

For A, B, C,and D are independent with w, then

$$\begin{cases} (1)=\left[\frac{\sigma^2A}2\ln(1+(\frac{w}\sigma)^2)-\frac{\tau^2C}2\ln(1+(\frac{z-w}\tau)^2)\right]_{-\infty}^{\infty}=0\\
(2)+(3)+(4)=\pi\sigma B-\pi\tau D-\pi\tau Cz=\frac{\pi\sigma\tau}{(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}
\end{cases}\implies \begin{cases} \sigma^2A=\tau^2C=k & k\text{ is a constant}\\
\sigma B=\tau D=r & r\text{ is a constant}\\
-\frac{\pi{z}}\tau k=\frac{\pi\sigma\tau}{(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}
\end{cases}$$

Therefore, when $r=\frac{\sigma\tau}{\sigma+\tau}$ and $k=\frac{-\sigma\tau^2}{z(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}$

$$\begin{cases} A=\frac{k}{\sigma^2}=\frac{-\tau^2}{z\sigma(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}\\
C=\frac{k}{\tau^2}=\frac{-\sigma}{z(\sigma+\tau)[1+(\frac{z}{\sigma+\tau})^2]}\\
B=\frac{r}\sigma=\frac{\tau}{\sigma+\tau} \\
D=\frac{r}\tau=\frac{\sigma}{\sigma+\tau}
\end{cases}$$


 ---

**5.10** Let $X_1,..,X_n$ be a random sample from a $n(\mu,\sigma^2)$ population.

 (c). Calculate $VarS^2$ a completely different (and easier) way: Use the fact that $(n-1)S^2/\sigma^2\sim\chi^2_{n-1}$

For $\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{n-1}$ and $Var\chi^2_{n-1}=2(n-1)$, then

$$Var[\frac{(n-1)S^2}{\sigma^2}]=\frac{(n-1)^2}{\sigma^4}Var[S^2]=2(n-1)\implies Var[S^2]=\frac{2\sigma^4}{n-1}$$

 ---

**5.13** Let $X_1,..,X_n$ be iid $n(\mu,\sigma^2)$. Find a function of $S^2$, the sample variance, say $g(S^2)$, that satisfies $Eg(S^2)=\sigma$. (Hint:Try $g(S^2)=c\sqrt{S^2}$, where c is a constant.)

when $X_1,..,X_n$ be iid $n(\mu,\sigma^2)$, $(n-1)S^2/\sigma^2\sim\chi^2_{n-1}$. Let $u=(n-1)S^2/\sigma^2$, $h(u)=\sqrt u$, and $E[h(u)]=\int_0^{\infty}h(u)f(u)du$, then 

$$E_(\sqrt{\frac{S^2(n-1)}{\sigma^2}})=\int_0^{\infty}\frac{\sqrt u}{\Gamma(\frac{n-1}2)2^{\frac{n-1}2}}u^{\frac{n-1}2-1}e^{-\frac{u}2}du$$

Let $g(S^2)=c\sqrt{S^2}$, where c is a constant. then

$$Eg(S^2)=E[c\sqrt{S^2}]=\frac{c\sigma}{\sqrt{n-1}}E\left[\sqrt{\frac{S^2(n-1)}{\sigma^2}}\right]=\frac{c\sigma}{\sqrt{n-1}}\int_0^{\infty}\frac{u^{\frac12}}{\Gamma(\frac{n-1}2)2^{\frac{n-1}2}}u^{\frac{n-1}2-1}e^{-\frac{u}2}du$$

$$=\frac{c\sigma}{\sqrt{n-1}}\cdot\frac{\Gamma(\frac{n}2)2^{\frac{n}2}}{\Gamma(\frac{n-1}2)2^{\frac{n-1}2}}\cdot\int_0^{\infty}\frac1{\Gamma(\frac{n}2)2^{\frac{n}2}}u^{\frac{n}2-1}e^{-\frac{u}2}du=\frac{c\sigma}{\sqrt{n-1}}\cdot\frac{\Gamma(\frac{n}2)2^{\frac{1}2}}{\Gamma(\frac{n-1}2)}=\sigma$$
 
$$\therefore c=\sqrt{\frac{n-1}{2}}\cdot\frac{\Gamma(\frac{n-1}2)}{\Gamma(\frac{n}2)}$$
 
 ---

**5.15** Establish the following recursion relations for means and variances. Let $\bar{X_n}$, and $S_n^2$ be the mean and variance, respectively, of $X_1,..,X_n$. Then suppose another observation, $X_{n+1}$ , becomes available. Show that

 (a). $\bar X_{n+1}=\frac{X_{n+1}-n\bar X_n}{n+1}$

$$\bar X_{n+1}=\frac{\sum_{i=1}^{n+1}X_{i}}{n+1}=\frac{\sum_{i=1}^{n}X_{i}+X_{n+1}}{n+1}=\frac{n\bar X_{n}+X_{n+1}}{n+1}$$

 (b). $nS^2_{n+1}=(n-1)S_n^2+(\frac{n}{n+1})(X_{n+1}-\bar X_n)^2$

For $S^2=\frac1{n-1}\sum_{i=1}^n(X_i^2-n\bar X^2)$ and the equation in (a),

$$nS^2_{n+1}=\frac{n}{n+1-1}\left({\sum_{i=1}^{n+1}X_{i}^2}-(n+1)\bar X_{n+1}^2\right)=\left({\sum_{i=1}^{n}X_{i}}+X_{n+1}^2\right)-(n+1)\left(\frac{n\bar X_{n}+X_{n+1}}{n+1}\right)^2$$

$$=\left(\sum_{i=1}^{n}X_{i}-n\bar X_n^2\right)+n\bar X_n^2+X_{n+1}^2-\frac{(n\bar X_{n}+X_{n+1})^2}{n+1}=(n-1)S_n^2+\frac{(n^2\bar X_n^2+n\bar X_n^2)+(nX_{n+1}^2+X_{n+1}^2)-(n^2\bar X_{n}^2+2n\bar X_{n}X_{n+1}+X_{n+1}^2)}{n+1}$$
$$=(n-1)S_n^2+\frac{n\bar X_n^2+nX_{n+1}^2-2n\bar X_{n}X_{n+1}}{n+1}=(n-1)S_n^2+\frac{n}{n+1}(X_{n+1}-\bar X_n)^2$$


## HW3

**5.22** Let X and Y be $iid\ n(0,1)$ random variables, and define $Z=\min(X,Y)$. Prove that $Z^2\sim\chi_1^2$

 - Method I

Calculating the cdf of $Z^2$,

$$F_{Z^2}(z)=P([\min(X,Y)]^2\le z)=P(−\sqrt z\le\min(X,Y)\le\sqrt z)=P(\min(X,Y)\le\sqrt z)-P(\min(X,Y)\le-\sqrt z)$$

$$=[1−P(\min(X,Y)>\sqrt z)]−[1−P(\min(X,Y)>−\sqrt z)]=P(\min(X,Y)>−\sqrt z)−P(\min(X,Y)>\sqrt z)$$

For X and Y are independent,

$$=P(X>−\sqrt z)P(Y>−\sqrt z)−P(X>\sqrt z)P(Y>\sqrt z)$$

For X and Y are identically distributed, $P(X>a)=P(Y>a)=1−F_X(a)=F_X(-a)$,

$$=[1−F_X(−\sqrt z)]^2−[1−F_X(\sqrt z)]^2 = 1−2F_X(−\sqrt z),$$

Differentiating and substituting gives

$$f_{Z^2}(z)=\frac{d}{dz}F_{Z^2}(z)=f_{X}(−\sqrt z)\frac{1}{\sqrt z}=\frac{1}{\sqrt{2\pi}}z^{-\frac12}e^{-\frac{z}2}$$

Therefore, $Z^2\sim\chi^2_1$ 

 - Method II

$$P(Z^2\le z)=P([\min(X,Y)]^2\le z)=P(−\sqrt z\le\min(X,Y)\le\sqrt z)=P(-\sqrt z\le X\le\sqrt z,X\le Y)+P(-\sqrt z\le Y\le\sqrt z,Y\le X)$$

X and Y are independent, and $P(Y\le X)=P(X\le Y)=\frac12$

$$=P(-\sqrt z\le X\le\sqrt z|X\le Y)P(X\le Y)+P(-\sqrt z\le Y\le\sqrt z|Y\le X)P(Y\le X)=\frac12P(-\sqrt z\le X\le\sqrt z)+\frac12P(-\sqrt z\le Y\le\sqrt z)$$

For X and Y are identically distributed,

$$P(Z^2\le z)=P(-\sqrt z\le X\le\sqrt z)$$

$$f_{Z^2}(z)=\frac{d}{dz}P(-\sqrt z\le X\le\sqrt z)=\frac{1}{\sqrt{2\pi}}\left(z^{-\frac12}\frac12e^{-\frac{z}2}+z^{-\frac12}\frac12e^{-\frac{z}2}\right)=\frac{1}{\sqrt{2\pi}}z^{-\frac12}e^{-\frac{z}2}$$

Therefore, $Z^2\sim\chi^2_1$ 

 ---

**5.24** Let $X1,..,X_n$ be a random sample from a population with pdf $f_X(x)=\begin{cases}\frac1{\theta}&0<x<1\\0&\text{otherwise}\end{cases}$
otherwise. Let $X_{(1)}<..<X_{(n)}$ be the order statistics. Show that $X_{(1)}/X_{(n)}$ and $X_{(n)}$ are independent random variables.

For $f_X(x)=\frac1{\theta}$, $F_X(x)=\frac{x}{\theta},0<x<\theta$. Let $Y=X_{(n)},Z=X_{(1)}$

From **Theorem 5.4.6**, 

$$f_{X_{(i)},X_{(j)}}(u,v)=\frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1}[F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}$$
$$f_{X_{(1)},..,X_{(n)}}(x_1,..,x_n)=\begin{cases}n!f_X(x_1)\cdot...\cdot f_X(x_n) &-\infty<x_1<..<x_n<\infty\\0&\text{otherwise}\end{cases}$$

$$f_{Z,Y}(z,y)=\frac{n!}{(1-1)!(n-1-1)!(n-n)!}\frac1{\theta}\frac1{\theta}[\frac{z}{\theta}]^{1-1}[\frac{y}{\theta}-\frac{z}{\theta}]^{n-1-1}[1-\frac{y}{\theta}]^{n-n}=\frac{n(n-1)}{\theta^n}(y-z)^{n-2},0<z<y<\theta$$

Now let $W=\frac{Z}{Y}=\frac{X_{(1)}}{X_{(n)}},Q=Y=X_{(n)}$. Then $Y=Q,Z=WQ$. Therefore,

$$J=\begin{vmatrix}\frac{\partial y}{\partial w} & \frac{\partial y}{\partial q} \\ \frac{\partial z}{\partial w} & \frac{\partial z}{\partial q} \end{vmatrix}=\begin{vmatrix}\ 0 & 1 \\ q & w \end{vmatrix}=q$$

$$f_{W,Q}(w,q)=\frac{n(n-1)}{\theta^n}(q-wq)^{n-2}|q|=\frac{n(n-1)}{\theta^n}(1-w)^{n-2}q^{n-1},0<w<1,0<q<\theta$$

The joint pdf factors into functions of $w$ and $q$, and, hence, $W=\frac{X_{(1)}}{X_{(n)}}$ and $Q=X_{(n)}$ are independent.

 ---

**5.25** As a generalization of the previous exercise, let $X_1,..,X_n$ be iid with pdf $f_X(x)=\begin{cases}\frac{a}{\theta^a}x^{a-1}&0<x<1\\0&\text{otherwise}\end{cases}$. Let $X_{(1)}<..<X_{(n)}$ be the order statistics. Show that $X_{(1)}/X_{(2)}, X_{(2)}/X_{(3)},..,X_{(n-1)}/X_{(n)}$ and $X_{(n)}$ are mutually independent random variables. Find the distribution of each of them.

The joint pdf of $X_{(1)}<..<X_{(n)}$ is

$$f(u_1,..,u_n)=\frac{n!a^n}{\theta^{an}}u_1^{a-1}\cdots u_n^{a-1}, 0<u_1<..<u_n<\theta$$

Make the one-to-one transformation to $Y_{1}=\frac{X_{(1)}}{X_{(2)}},..,Y_{n−1}=\frac{X_{(n−1)}}{X_{(n)}},Y_n=X_{(n)}$.


$$J=\begin{vmatrix}\frac{\partial x_1}{\partial y_1} &\frac{\partial x_1}{\partial y_2}&\cdots& \frac{\partial x_1}{\partial y_n} \\
\frac{\partial x_2}{\partial y_1} &\frac{\partial x_2}{\partial y_2}&\cdots& \frac{\partial x_2}{\partial y_n}\\
\vdots &\vdots&\ddots& \vdots\\
\frac{\partial x_n}{\partial y_1} &\frac{\partial x_n}{\partial y_2}&\cdots& \frac{\partial x_n}{\partial y_n}\\
\end{vmatrix}=\begin{vmatrix}
y_2y_3..y_n &y_1y_3..y_n&\cdots& y_1y_2..y_{(n-1)} \\
0 &y_3y_4..y_n&\cdots& y_2y_3..y_{(n-1)}\\

\vdots &\vdots&\ddots& \vdots\\
0 &0&\cdots& 1\\
\end{vmatrix}=y_2y_3^2\cdots y_n^{n-1}$$


So the joint pdf of $Y_1,..,Y_n$ is

$$f(y_1,..,y_n)=\frac{n!a^n}{\theta^{an}}(y_1\cdots y_n)^{a-1}(y_2\cdots y_n)^{a-1}\cdots y_n^{a-1}(y_2y_3^2\cdots y_n^{n-1})$$

$$=\frac{n!a^n}{\theta^{an}}y_1^{a-1}y_2^{2a-1}\cdots y_n^{na-1}, 0<y_i<1,i=1,..,n-1,0<y_n<\theta$$

For $f(y_1,..,y_n)$ can be factored to $Y_1,..,Y_n$, $\frac{X_{(1)}}{X_{(2)}},\quad \frac{X_{(2)}}{X_{(3)}},..,\frac{X_{(n-1)}}{X_{(n)}}$ are mutually independent. 

When $i=1$, let $f_{Y_1}(y_1)=\int_0^1cy_1^{a-1}dy_1=c\int_0^1y_1^{a-1}dy_1=\frac{c}a[y_1^a]_0^1=1$. Then $c=a$. Therefore,

$$f_{Y_1}(y_1)=\int_0^1\cdots\int_0^1f(y_1,..,y_n)dy_2dy_3\cdots dy_n=ay^{a−1}_1,0<y_1<1$$

Similarly, for $f_{Y_2}(y_2)=ay^{2a−1}_2$, $f_{Y_i}(y_i)=iay_i^{ia−1},0<y_i<1,i=1,2,..,n−1$. 

From Theorem 5.4.4, $f_{X_{(j)}}(x)=\frac{n!}{(j-1)!(n-j)!}f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}$, the pdf of $Y_n$ is

$$f_{Y_n}(y_n)=\frac{n!}{(n-1)!(n-n)!}f_X(x)[F_X(x)]^{n-1}[1-F_X(x)]^{n-n}=nf_X(x)[F_X(x)]^{n-1}$$
$$=n\frac{a}{\theta^{a}}y_n^{a-1}\left[\frac1a\frac{a}{\theta^{a}}y_n^{a}\right]^{n-1}=\frac{an}{\theta^{a+an-a}}y_n^{a-1+an-a}=\frac{an}{\theta^{an}}y_n^{an-1}, 0<y_n<\theta$$




## HW4

**5.35** Stirling's Formula (derived in Exercise 1.28), which gives an approximation for factorials, can be easily derived using the CLT.

 (a). Argue that, if Xi '" exponential ( I ) , i = 1 , 2, . . ., all independent, then for every x, 
 
 P (Xn - 1 )
I /Vn
:::; x ...... P (Z :::; x) ,

where Z is a standard normal random variable.

 (b). Show that differentiating both sides of the approximation in part (a) suggests

Vn
(xvn +n) ", - l e -C:z:vn+ n)  _1_ e -:Z:2/2 r(n) y'2;

and that x = 0 gives Stirling's Formula.

 ---

**5.39** This exercise, and the two following, will look at some o f the mathematical details of convergence.

 (a). Prove Theorem 5.5.4. (Hint: Since h is continuous, given e: > 0 we can find a 6
such that Ih(xn) - h(x)1 < e: whenever Ix" - xl < 6. Translate this into probability
statements. )

 (b). In Example 5.5.B, find a subsequence of the XiS that converges almost surely, that is, that converges p ointwise.
 
 ---

**5.41** Prove Theorem 5.5.13; that is, show that

P (IX" - J.LI > e) -t 0 for every e <=> P (X < x) -t {O f x < J.L
n- 1 1f x  J.L.

 (a). Set e: = Ix - J.LI and show that if x > J.L, then P(Xn :0:; x)  P(I Xn - J.LI :0:; e), while
if x < J.L, then P(X" :0:; x) :0:; P ( I X" - J.L I e) . Deduce the => implication.


 (b). Use the fact that {x : Ix J.LI > e:} ::: {x : x J.L < -e:} U {x : x - J.L > e} to deduce the *" implication.
 
(See Billingsley 1995, Section 25, for a detailed treatment of the above results.)
 
## HW5

**6.6** Let Xl , . . . , Xn be a random sample from a gamma(o, t3) population. Find a two dimensional sufficient statistic for (0 . t3) .

 ---

**6.9** For each of the following distributions let Xl , . . . , Xn be a random sample. Find a minimal sufficient statistic for e.

 (a). j(xIO) -00 < x < 00, -00 < 0 < 00  (normal)
 (b). j(xIB) = e-(x-6) , B < x < 00, -00 < 0 < 00  (location exponential)
 (c). j (xIO) =-00 < x < 00, -00 < e < 00 (logistic)
 (d). j (xI O)1f[1+(x-6)2] ,-00 < X < 00, -00 < B < 00 (Cauchy)
 (e). j(xIO)'2I e- lx-81 ,-00 < x < 00, -00 < B < 00 (double exponential)
 
 ---
 
**6.10** Show that the minimal sufficient statistic for the uniform(O, 0 + 1 ) , found in Example 6.2.15, is not complete.

 ---

**6.13** Suppose Xl and X2 are iid observations from the pdf j(xla) =, x > o, a >O. Show that (log XI )/(log X2) is an ancillary statistic.

## HW6

**6.30** Let Xl, . . . , Xn be a random sample from the pdf f(x ifJ.) = e-(x-!-') , where -00 < fJ. <x < 00.

(a). Show that X(l) = min; Xi is a complete sufficient statistic.

(b). Use Basu's Theorem to show that X{!) and 82 are independent.

 ---

**7.6** Let Xl , . . . , Xn be a random sample from the pdf

f(xIO) O -2 X , 0 < 0 􀀬 x < 00 .

 (a). What is a sufficient statistic for 87
 (b). Find the MLE of 8.
 (c). Find the method of moments estimator of O.

 ---

**7.10** The independent random variables X l , . . . , Xn have the common distribution

if x < 0
if O 􀃉 x 􀃉 jJ
if x > jJ,

where the parameters a and jJ are positive.

 (a). Find a tW<rdimensional sufficient statistic for (a, jJ).
 (b). Find the MLEs of a and jJ.
 (c). The length (in millimeters) of cuckoos' eggs found in hedge sparrow nests can be modeled with this distribution. For the data
 
22.0, 23.9, 20.9, 23.8, 25.0, 24.0, 21 .7, 23.8, 22.8, 23. 1, 23. 1 , 23.5, 23.0, 23.0,

find the MLEs of a and jJ.

 ---

**7.11** Let X! , . . . , X", be iid with pdf

f(xI9) = 9x9-t, 0 􀋁 X 􀁱 1 , 0 < (} < 00 .

 (a). Find the MLE of 8, and show that its variance --+ 0 as n --+ 00.

 (b). Find the method of moments estimator of (}.

## HW7

**5.59** Prove that the algorithm of Example 5.6.7 generates a beta( a, b) random variable.

 ---

**5.60** Generalize the algorithm of Example 5.6.7 to apply to any bounded pdf; that is, for an arbitrary bounded pdf f(x) on [a, b), define c = maxa:<;z:9 f (x) . Let X and Y be independent, with X '" uniform(a, b) and Y '" uniform(O, c). Let d be a number greater than b, and define a new random variable

W = {X if Y < f(X)
d if Y  f(X).

 (a). Show that pew 􀁱 w) = J: f(t)dt / [c(b - a)l for a 􀁱 w 􀁱 b.

 (b). Using part (a), explain how a random variable with pdf f(x) can be generated.

(Hint: Use a geometric argument; a picture will help.)


 ---

**7.22** This exercise will prove the assertions in Example 7.2.16, and more. Let Xl, . . . , X n be a random sample from a n(O, a2) population, and suppose that the prior distribution on 0 is n(#, 72). Here we assume that a2, #, and 72 are all known.

 (a). Find the joint pdf of X and 8. 
 
 (b). Show that m(xia2, #, 72), the marginal distribution of X, is n(#, (a2 In) + 72).

 (c). Show that 1T(Oix, a2, #, 72), the posterior distribution of 0 , i s normal with mean and variance given by (7.2.10).

 ---

**7.23** If 82 is the sample variance based on a sample of size n from a normal population, we know that (n - 1 ) 82/u2 has a X!-l distribution. The conjugate prior for u2 is the inverted gamma pdf, 10(a, {3), given by


where a and {3 are positive constants. Show that the posterior distribution of u2 is 10 (a + n;l , [(n_)s2 + 􀀺l-l ) . Find the mean of this distribution, the Bayes estimator of u2•


